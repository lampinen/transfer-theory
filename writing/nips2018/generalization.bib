@article{Pereira2018,
author = {Pereira, Francisco and Lou, Bin and Pritchett, Brianna and Ritter, Samuel and Gershman, Samuel J. and Kanwisher, Nancy and Botvinick, Matthew and Fedorenko, Evelina},
doi = {10.1038/s41467-018-03068-4},
file = {:home/andrew/Documents/grad/Papers/s41467-018-03068-4.pdf:pdf},
issn = {2041-1723},
journal = {Nature Communications},
number = {1},
pages = {963},
pmid = {29511192},
publisher = {Springer US},
title = {{Toward a universal decoder of linguistic meaning from brain activation}},
url = {http://www.nature.com/articles/s41467-018-03068-4},
volume = {9},
year = {2018}
}
@article{Nakamura2018,
author = {Nakamura, Kimihiro and Makuuchi, Michiru and Oga, Tatsuhide and Mizuochi-Endo, Tomomi and Iwabuchi, Toshiki and Nakajima, Yasoichi and Dehaene, Stanislas},
doi = {10.1111/ejn.13890},
file = {:home/andrew/Documents/grad/Papers/Nakamura{\_}et{\_}al-2018-European{\_}Journal{\_}of{\_}Neuroscience.pdf:pdf},
issn = {0953816X},
journal = {European Journal of Neuroscience},
keywords = {N400,event‐related potentials,sentence comprehension,subliminal language processing,visual masking},
pages = {0--2},
pmid = {29512843},
title = {{Neural capacity limits during unconscious semantic processing}},
url = {http://doi.wiley.com/10.1111/ejn.13890},
year = {2018}
}
@article{Vyas2018,
author = {Vyas, Saurabh and Even-Chen, Nir and Stavisky, Sergey D. and Ryu, Stephen I. and Nuyujukian, Paul and Shenoy, Krishna V.},
doi = {10.1016/j.neuron.2018.01.040},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S0896627318300655-main.pdf:pdf},
issn = {08966273},
journal = {Neuron},
pages = {1--10},
pmid = {29456026},
publisher = {Elsevier Inc.},
title = {{Neural Population Dynamics Underlying Motor Learning Transfer}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627318300655},
year = {2018}
}
@article{Haber2018,
abstract = {Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to replicate some of these abilities with a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which the agent can move and interact with objects it sees, the agent learns a world model predicting the dynamic consequences of its actions. Simultaneously, the agent learns to take actions that adversarially challenge the developing world model, pushing the agent to explore novel and informative interactions with its environment. We demonstrate that this policy leads to the self-supervised emergence of a spectrum of complex behaviors, including ego motion prediction, object attention, and object gathering. Moreover, the world model that the agent learns supports improved performance on object dynamics prediction and localization tasks. Our results are a proof-of-principle that computational models of intrinsic motivation might account for key features of developmental visuomotor learning in infants.},
archivePrefix = {arXiv},
arxivId = {1802.07461},
author = {Haber, Nick and Mrowca, Damian and Fei-Fei, Li and Yamins, Daniel L. K.},
eprint = {1802.07461},
file = {:home/andrew/Documents/grad/Papers/1802.07461.pdf:pdf},
journal = {arXiv},
title = {{Emergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation}},
url = {http://arxiv.org/abs/1802.07461},
year = {2018}
}
@article{Smith2017,
abstract = {Visual learning depends on both the algorithms and the training material. This essay considers the natural statistics of infant- and toddler-egocentric vision. These natural training sets for human visual object recognition are very different from the training data fed into machine vision systems. Rather than equal experiences with all kinds of things, toddlers experience extremely skewed distributions with many repeated occurrences of a very few things. And though highly variable when considered as a whole, individual views of things are experienced in a specific order – with slow, smooth visual changes moment-to-moment, and developmentally-ordered transitions in scene content. We propose that the skewed, ordered, biased visual experiences of infants and toddlers are the training data that allow human learners to develop a way to recognize everything, both the pervasively present entities and the rarely encountered ones. The joint consideration of real-world statistics for learning by researchers of human and machine learning seems likely to bring advances in both disciplines.},
author = {Smith, Linda B. and Slone, Lauren K.},
doi = {10.3389/fpsyg.2017.02124},
file = {:home/andrew/Documents/grad/Papers/fpsyg-08-02124.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Active vision,Development,Egocentric vision,Natural environment,Object recognition},
number = {DEC},
pages = {1--10},
pmid = {29259573},
title = {{A developmental approach to machine learning?}},
volume = {8},
year = {2017}
}
@article{Hannagan2017,
abstract = {Number sense, a spontaneous ability to process approximate numbers, has been documented in human adults, infants and newborns, and many other animals. Species as distant as monkeys and crows exhibit very similar neurons tuned to specific numerosities. How number sense can emerge in the absence of learning or fine tuning is currently unknown. We introduce a random-matrix theory of self-organized neural states where numbers are coded by vectors of activation across multiple units, and where the vector codes for successive integers are obtained through multiplication by a fixed but random matrix. This cortical implementation of the 'von Mises' algorithm explains many otherwise disconnected observations ranging from neural tuning curves in monkeys to looking times in neonates and cortical numerotopy in adults. The theory clarifies the origin of Weber-Fechner's Law and yields a novel and empirically validated prediction of multi-peak number neurons. Random matrices constitute a novel mechanism for the emergence of brain states coding for quantity.This article is part of a discussion meeting issue 'The origins of numerical abilities'.},
author = {Hannagan, T and Nieder, A and Viswanathan, P and Dehaene, S},
doi = {10.1098/rstb.2017.0253},
file = {:home/andrew/Documents/grad/Papers/20170253.full.pdf:pdf},
issn = {1471-2970},
journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
keywords = {Weber–Fechner Law,neonates,number neurons,number sense,numerotopy,random-matrix theory},
number = {1740},
pages = {20170253},
pmid = {29292354},
title = {{A random-matrix theory of the number sense.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29292354},
volume = {373},
year = {2017}
}
@book{Naumann2017,
annote = {Good overview of difficulties of homology and importance of comparative neuroanatomy, if you're into that sort of thing},
author = {Naumann, Robert K. and Laurent, Gilles},
booktitle = {Evolution of Nervous Systems},
doi = {10.1016/B978-0-12-804042-3.00022-1},
edition = {Second Edi},
file = {:home/andrew/Documents/grad/Papers/3-s2.0-B9780128040423000221-main.pdf:pdf},
isbn = {9780128040423},
pages = {491--512},
publisher = {Elsevier},
title = {{Function and Evolution of the Reptilian Cerebral Cortex}},
url = {http://dx.doi.org/10.1016/B978-0-12-804042-3.00022-1},
volume = {1},
year = {2017}
}
@article{Suhr2017,
abstract = {We present a new visual reasoning lan-guage dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sen-tences. We describe a method of crowd-sourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phe-nomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.},
author = {Suhr, Alane and Lewis, Mike and Yeh, James and Artzi, Yoav},
doi = {10.18653/v1/P17-2034},
file = {:home/andrew/Documents/grad/Papers/suhr2017.pdf:pdf},
isbn = {9781945626760},
journal = {Acl},
pages = {217----223},
title = {{A Corpus of Natural Language for Visual Reasoning}},
url = {http://yoavartzi.com/pub/slya-acl.2017.pdf},
year = {2017}
}
@article{Forbus2017,
author = {Forbus, Kenneth D and Ferguson, Ronald W and Lovett, Andrew},
doi = {10.1111/cogs.12377},
file = {:home/andrew/Documents/grad/Papers/Forbus{\_}et{\_}al-2016-Cognitive{\_}Science.pdf:pdf},
keywords = {2133 sheri-,analogical learning,analogical reasoning,analogy,artificial intelligence,cognitive psychology,cognitive simulation,correspondence should be sent,eecs department,northwestern university,similarity,symbolic modeling,to ken forbus},
pages = {1152--1201},
title = {{Extending SME to Handle Large-Scale Cognitive Modeling}},
volume = {41},
year = {2017}
}
@article{Pandarinath2017,
author = {Pandarinath, Chethan and O'Shea, Daniel J. and Collins, Jasmine and Jozefowicz, Rafal and Stavisky, Sergey D. and Kao, Johnathan C. and Trautmann, Eric M. and Kaufman, Matthew T. and Ryu, Stephen I. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shenoy, Krishna V. and Abbott, L. F. and Sussillo, David},
file = {:home/andrew/Documents/grad/Papers/152884.full.pdf:pdf},
journal = {bioRxiv},
pages = {1--4},
title = {{Inferring single-trial neural population dynamics using sequential auto-encoders}},
year = {2017}
}
@article{Gentner2017,
author = {Gentner, Dedre and Asmuth, Jennifer},
doi = {10.1080/23273798.2017.1410560},
file = {:home/andrew/Documents/grad/Papers/Metaphoric extension relational categories and abstraction.pdf:pdf},
issn = {2327-3798},
journal = {Language, Cognition and Neuroscience},
keywords = {analogical abstraction,career,concept,of metaphor,progressive alignment},
number = {0},
pages = {1--10},
publisher = {Taylor {\&} Francis},
title = {{Metaphoric extension , relational categories , and abstraction}},
url = {https://doi.org/10.1080/23273798.2017.1410560},
volume = {0},
year = {2017}
}
@article{Gentner2017a,
abstract = {A central question in human development is how young children gain knowledge so fast. We propose that analogical generalization drives much of this early learning and allows children to generate new abstractions from experience. In this paper, we review evidence for analogical gen-eralization in both children and adults. We discuss how analogical processes interact with the child's changing knowledge base to predict the course of learning, from conservative to domain-general understanding. This line of research leads to challenges to existing assumptions about learning. It shows that (a) it is not enough to consider the distribution of examples given to learn-ers; one must consider the processes learners are applying; (b) contrary to the general assumption, maximizing variability is not always the best route for maximizing generalization and transfer.},
author = {Gentner, Dedre and Hoyos, Christian},
doi = {10.1111/tops.12278},
file = {:home/andrew/Documents/grad/Papers/Gentner{\_}et{\_}al-2017-Topics{\_}in{\_}Cognitive{\_}Science.pdf:pdf},
issn = {17568765},
journal = {Topics in Cognitive Science},
keywords = {Abstraction,Analogy,Overhypotheses},
number = {3},
pages = {672--693},
title = {{Analogy and Abstraction}},
volume = {9},
year = {2017}
}
@inproceedings{Rahimi2017,
author = {Rahimi, Ali},
booktitle = {Neural Information Processing Systems},
title = {{Test of time award presentation}},
url = {https://www.youtube.com/watch?v=Qi1Yry33TQE},
year = {2017}
}
@incollection{Mickey2017,
author = {Mickey, Kevin and Mcclelland, James L},
booktitle = {Acquisition of Complex Arithmetic Skills and Higher-Order Mathematics Concepts.},
editor = {Geary, D. C. and Berch, D. B. and Ochsendorf, R. and {Mann Koepke}, K.},
file = {:home/andrew/Documents/grad/Papers/MickeyMcCIPUCasGroundedCS.pdf:pdf},
keywords = {conceptual grounding,learning,learning trigonometry,visuospatial representation},
pages = {1--47},
publisher = {Elsevier/Academic Press},
title = {{The Unit Circle as a Grounded Conceptual Structure in Pre-Calculus Trigonometry}},
year = {2017}
}
@article{Lampinen2017b,
author = {Lampinen, Andrew K. and McClelland, James L.},
doi = {10.1037/edu0000235},
file = {:home/andrew/Documents/grad/Papers/mine/Lampinen{\_}McClelland{\_}2017{\_}PrePrint.pdf:pdf},
issn = {1939-2176},
journal = {Journal of Educational Psychology},
title = {{Different Presentations of a Mathematical Concept Can Support Learning in Complementary Ways.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/edu0000235},
year = {2017}
}
@article{Herbelot2017,
abstract = {Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn 'a good vector' for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.},
archivePrefix = {arXiv},
arxivId = {1707.06556},
author = {Herbelot, Aurelie and Baroni, Marco},
eprint = {1707.06556},
file = {:home/andrew/Documents/grad/Papers/D17-1030.pdf:pdf},
journal = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
pages = {304--309},
title = {{High-risk learning: acquiring new word vectors from tiny data}},
url = {http://arxiv.org/abs/1707.06556},
year = {2017}
}
@article{Serdyuk2017,
abstract = {Being able to model long-term dependencies in sequential data, such as text, has been among the long-standing challenges of recurrent neural networks (RNNs). This issue is strictly related to the absence of explicit planning in current RNN architectures. More explicitly, the RNNs are trained to predict only the next token given previous ones. In this paper, we introduce a simple way of encouraging the RNNs to plan for the future. In order to accomplish this, we introduce an additional neural network which is trained to generate the sequence in reverse order, and we require closeness between the states of the forward RNN and backward RNN that predict the same token. At each step, the states of the forward RNN are required to match the future information contained in the backward states. We hypothesize that the approach eases modeling of long-term dependencies thus helping in generating more globally consistent samples. The model trained with conditional generation for a speech recognition task achieved 12$\backslash${\%} relative improvement (CER of 6.7 compared to a baseline of 7.6).},
archivePrefix = {arXiv},
arxivId = {1708.06742},
author = {Serdyuk, Dmitriy and Ke, Rosemary Nan and Sordoni, Alessandro and Pal, Chris and Bengio, Yoshua},
eprint = {1708.06742},
file = {:home/andrew/Documents/grad/Papers/1708.06742.pdf:pdf},
journal = {arXiv},
pages = {3--6},
title = {{Twin Networks: Using the Future as a Regularizer}},
url = {http://arxiv.org/abs/1708.06742},
year = {2017}
}
@inproceedings{Siddharth2017,
abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
archivePrefix = {arXiv},
arxivId = {1706.00400},
author = {Siddharth, N. and Paige, Brooks and van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank and Torr, Philip H. S.},
booktitle = {Neural Information Processing Systems},
eprint = {1706.00400},
file = {:home/andrew/Documents/grad/Papers/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf:pdf},
title = {{Learning Disentangled Representations with Semi-Supervised Deep Generative Models}},
url = {http://arxiv.org/abs/1706.00400},
year = {2017}
}
@inproceedings{Devlin2017,
abstract = {Most recently proposed methods for Neural Program Induction work under the assumption of having a large set of input/output (I/O) examples for learning any underlying input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a {\$}k{\$}-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance.},
archivePrefix = {arXiv},
arxivId = {1710.04157},
author = {Devlin, Jacob and Bunel, Rudy and Singh, Rishabh and Hausknecht, Matthew and Kohli, Pushmeet},
booktitle = {Neural Information Processing Systems},
eprint = {1710.04157},
file = {:home/andrew/Documents/grad/Papers/6803-neural-program-meta-induction.pdf:pdf},
title = {{Neural Program Meta-Induction}},
url = {http://arxiv.org/abs/1710.04157},
year = {2017}
}
@inproceedings{Raghu2017,
abstract = {With the continuing empirical successes of deep networks, it becomes increasingly important to develop better methods for understanding training of models and the representations learned within. In this paper we propose Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.},
archivePrefix = {arXiv},
arxivId = {1706.05806},
author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
booktitle = {Neural Information Processing Systems},
doi = {1706.05806},
eprint = {1706.05806},
file = {:home/andrew/Documents/grad/Papers/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf:pdf},
pages = {1--10},
title = {{SVCCA: Singular Vector Canonical Correlation Analysis for Deep Understanding and Improvement}},
url = {http://arxiv.org/abs/1706.05806},
year = {2017}
}
@article{Lake2017,
abstract = {Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.},
archivePrefix = {arXiv},
arxivId = {1711.00350},
author = {Lake, Brenden M. and Baroni, Marco},
eprint = {1711.00350},
file = {:home/andrew/Documents/grad/Papers/LakeBaroniArXivNotSystematic.pdf:pdf},
journal = {arXiv},
pages = {1--12},
title = {{Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks}},
url = {http://arxiv.org/abs/1711.00350},
year = {2017}
}
@article{Bowers2017,
author = {Bowers, Jeffrey S.},
doi = {10.1016/j.tics.2017.09.013},
file = {:home/andrew/Documents/grad/Papers/Bowers17TiCSPDPInAgeOFDeepNetworks.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
pages = {1--12},
pmid = {29100738},
publisher = {Elsevier Ltd},
title = {{Parallel Distributed Processing Theory in the Age of Deep Networks}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661317302164},
year = {2017}
}
@article{Schapiro2017,
abstract = {A growing literature suggests that the hippocampus is critical for the rapid extraction of regularities from the environment. Although this fits with the known role of the hippocampus in rapid learning, it seems at odds with the idea that the hippocampus specializes in memorizing individual episodes. In particular, the Complementary Learning Systems theory argues that there is a computational trade-off between learning the specifics of individual experiences and regularities that hold across those experiences. We asked whether it is possible for the hippocampus to handle both statistical learning and memorization of individual episodes. We exposed a neural network model that instantiates known properties of hippocampal projections and subfields to sequences of items with temporal regularities. We found that the monosynaptic pathway - the pathway connecting entorhinal cortex directly to region CA1 - was able to support statistical learning, while the trisynaptic pathway - connecting entorhinal cortex to CA1 through dentate gyrus and CA3 - learned only individual episodes, with apparent representations of regularities resulting from associative reactivation through recurrence. Thus, in paradigms involving rapid learning, the computational trade-off between learning episodes and regularities may be handled by separate anatomical pathways within the hippocampus itself.},
author = {Schapiro, Anna C. and Turk-Browne, Nicholas B. and Botvinick, Matthew M. and Norman, Kenneth A.},
doi = {10.1098/rstb.2016.0049},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schapiro et al. - 2017 - Complementary learning systems within the hippocampus a neural network modelling approach to reconciling episod.pdf:pdf},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {behaviour,cognition,neuroscience},
number = {1711},
pages = {20160049},
pmid = {1000305518},
title = {{Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning}},
url = {http://rstb.royalsocietypublishing.org/lookup/doi/10.1098/rstb.2016.0049},
volume = {372},
year = {2017}
}
@article{Momennejad2017,
abstract = {Theories of reinforcement learning in neuroscience have focused on two families of algorithms. Model-free algorithms cache action values, making them cheap but inflexible: a candidate mechanism for adaptive and maladaptive habits. Model-based algorithms achieve flexibility at computational expense, by rebuilding values from a model of the environment. We examine an intermediate class of algorithms, the successor representation (SR), which caches long-run state expectancies, blending model-free efficiency with model-based flexibility. Although previous reward revaluation studies distinguish model-free from model-based learning algorithms, such designs cannot discriminate between model-based and SR-based algorithms, both of which predict sensitivity to reward revaluation. However, changing the transition structure (" transition revaluation ") should selectively impair revaluation for the SR. In two studies we provide evidence that humans are differentially sensitive to reward vs. transition revaluation, consistent with SR predictions. These results support a new neuro-computational mechanism for flexible choice, while introducing a subtler, more cognitive notion of habit.},
author = {Momennejad, I. and Russek, E. M. and Cheong, J. H. and Botvinick, M. M. and Daw, N. D. and Gershman, S. J.},
doi = {10.1038/s41562-017-0180-8},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Momennejad et al. - 2017 - The successor representation in human reinforcement learning.pdf:pdf},
issn = {2397-3374},
journal = {Nature Human Behaviour},
keywords = {decision making,human behavior,planning,reinforcement learning,retrospective revaluation,successor representation},
number = {9},
pages = {680--692},
title = {{The successor representation in human reinforcement learning}},
url = {http://www.nature.com/articles/s41562-017-0180-8},
volume = {1},
year = {2017}
}
@article{Neyshabur2017,
abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
archivePrefix = {arXiv},
arxivId = {1706.08947},
author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
eprint = {1706.08947},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neyshabur et al. - 2017 - Exploring Generalization in Deep Learning.pdf:pdf},
title = {{Exploring Generalization in Deep Learning}},
year = {2017}
}
@article{Nie2017,
abstract = {Sentence vectors represent an appealing approach to meaning: learn an embedding that encompasses the meaning of a sentence in a single vector, that can be used for a variety of semantic tasks. Existing models for learning sentence embeddings either require extensive computational resources to train on large corpora, or are trained on costly, manually curated datasets of sentence relations. We observe that humans naturally annotate the relations between their sentences with discourse markers like "but" and "because". These words are deeply linked to the meanings of the sentences they connect. Using this natural signal, we automatically collect a classification dataset from unannotated text. Training a model to predict these discourse markers yields high quality sentence embeddings. Our model captures complementary information to existing models and achieves comparable generalization performance to state of the art models.},
archivePrefix = {arXiv},
arxivId = {1710.04334},
author = {Nie, Allen and Bennett, Erin D. and Goodman, Noah D.},
eprint = {1710.04334},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nie, Bennett, Goodman - 2017 - DisSent Sentence Representation Learning from Explicit Discourse Relations.pdf:pdf},
journal = {arXiv},
title = {{DisSent: Sentence Representation Learning from Explicit Discourse Relations}},
year = {2017}
}
@article{Advani2017,
abstract = {We perform an average case analysis of the generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant "high-dimensional" regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
archivePrefix = {arXiv},
arxivId = {1710.03667},
author = {Advani, Madhu S. and Saxe, Andrew M.},
eprint = {1710.03667},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Advani, Saxe - 2017 - High-dimensional dynamics of generalization error in neural networks.pdf:pdf},
journal = {arXiv},
pages = {1--32},
title = {{High-dimensional dynamics of generalization error in neural networks}},
year = {2017}
}
@article{Andreasa,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.02799v4},
author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
eprint = {arXiv:1511.02799v4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas et al. - 2017 - Deep Compositional Question Answering with Neural Module Networks.pdf:pdf},
title = {{Deep Compositional Question Answering with Neural Module Networks}},
year = {2017}
}
@article{Chaplot2017,
abstract = {To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.},
archivePrefix = {arXiv},
arxivId = {1706.07230},
author = {Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan},
eprint = {1706.07230},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaplot et al. - 2017 - Gated-Attention Architectures for Task-Oriented Language Grounding.pdf:pdf},
journal = {arXiv},
title = {{Gated-Attention Architectures for Task-Oriented Language Grounding}},
year = {2017}
}
@article{Lampinen2017a,
author = {Lampinen, Andrew and Hsu, Shaw and Mcclelland, James L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lampinen, Hsu, Mcclelland - 2017 - Analogies Emerge from Learning Dyamics in Neural Networks.pdf:pdf},
journal = {Proceedings of the 39th Annual Conference of the Cognitive Science Society},
keywords = {analogy,neural networks,representa-,structure learning,tion,transfer},
pages = {2512--2517},
title = {{Analogies Emerge from Learning Dyamics in Neural Networks}},
year = {2017}
}
@inproceedings{Kaliszyk2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.00426v1},
author = {Kaliszyk, Cezary and Chollet, Fran{\c{c}}ois and Szegedy, Christian},
booktitle = {ICLR},
eprint = {arXiv:1703.00426v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaliszyk, Chollet, Szegedy - 2017 - HOLSTEP A MACHINE LEARNING DATASET FOR HIGHER-ORDER LOGIC THEOREM PROVING.pdf:pdf},
pages = {1--12},
title = {{HOLSTEP: A MACHINE LEARNING DATASET FOR HIGHER-ORDER LOGIC THEOREM PROVING}},
year = {2017}
}
@article{Machens,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.08026v2},
author = {Machens, Christian K},
eprint = {arXiv:1705.08026v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Machens - 2017 - Learning arbitrary dynamics in efficient, balanced spiking networks using local plasticity rules.pdf:pdf},
journal = {arXiv preprint},
title = {{Learning arbitrary dynamics in efficient, balanced spiking networks using local plasticity rules}},
year = {2017}
}
@article{Christiano2017,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul and Leike, Jan and Brown, Tom B and Martic, Miljan and Legg, Shane and Amodei, Dario},
eprint = {1706.03741},
journal = {arXi},
title = {{Deep reinforcement learning from human preferences}},
year = {2017}
}
@article{Nguyen2017,
abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [36] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 × 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models “Plug and Play Generative Networks.” PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable “condition” network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [39], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1738978},
author = {Nguyen, Anh and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff},
doi = {10.1109/CVPR.2017.374},
eprint = {1738978},
journal = {ICCV},
number = {3},
pages = {33},
primaryClass = {arXiv:submit},
title = {{Plug {\{}{\&}{\}} Play Generative Networks: Conditional Iterative Generation of Images in Latent Space}},
year = {2017}
}
@article{Sun2017,
abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
archivePrefix = {arXiv},
arxivId = {1707.02968},
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
doi = {1707.02968},
eprint = {1707.02968},
journal = {arXiv},
title = {{Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}},
year = {2017}
}
@article{Elgammal2017,
abstract = {We propose a new system for generating art. The system generates art by look-ing at art and learning about style; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles. We build over Generative Adversarial Networks (GAN), which have shown the ability to learn to generate novel images simulating a given distribution. We argue that such net-works are limited in their ability to generate creative products in their original design. We propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing de-viation from art distribution. We conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists. The results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.07068},
author = {Elgammal, Ahmed and Liu, Bingchen and Elhoseiny, Mohamed and Mazzone, Marian},
eprint = {arXiv:1706.07068},
journal = {arXiv},
number = {Iccc},
pages = {1--22},
title = {{CAN: Creative Adversarial Networks Generating "Art" by Learning About Styles and Deviating from Style Norms}},
year = {2017}
}
@article{Wang2017,
abstract = {We test whether distributional models can do one-shot learning of definitional properties from text only. Using Bayesian models, we find that first learning overarching structure in the known data, regularities in textual contexts and in properties, helps one-shot learning, and that individual context items can be highly informative.},
archivePrefix = {arXiv},
arxivId = {1704.04550},
author = {Wang, Su and Roller, Stephen and Erk, Katrin},
eprint = {1704.04550},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Roller, Erk - 2017 - Distributional model on a diet One-shot word learning from text only.pdf:pdf},
journal = {arXiv preprint},
title = {{Distributional model on a diet: One-shot word learning from text only}},
year = {2017}
}
@article{Lazaridou2017,
abstract = {By the time they reach early adulthood, English speakers are familiar with the meaning of thousands of words. In the last decades, computational simulations known as distributional semantic models have demonstrated that it is possible to induce word meaning representations solely from word co-occurrence statis- tics extracted from a large amount of text. However, while these models learn in batch mode from large corpora, human word learning proceeds incrementally after minimal exposure to new words. In this study, we run a set of experiments investigating whether minimal distributional evidence from very short passages suffices to trigger successful word learning in subjects, testing their linguistic and visual intuitions about the concepts associated to new words. After con- firming that subjects are indeed very efficient distributional learners even from small amounts of evidence, we test a distributional semantic model on the same multimodal task, finding that it behaves in a remarkable human-like way. We conclude that distributional semantic models provide a convincing computa- tional account of word learning even at the early stages in which a word is first encountered, and the way they build meaning representations can offer new in- sights into human language acquisition.},
author = {Lazaridou, Angeliki and Marelli, Marco and Baroni, Marco},
doi = {10.1111/cogs.12481},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazaridou, Marelli, Baroni - 2017 - Multimodal Word Meaning Induction From Minimal Exposure to Natural Text.pdf:pdf},
issn = {15516709},
journal = {Cognitive Science},
keywords = {Distributional semantics,Language and the visual world,Multimodality,One-shot learning,Word learning},
pages = {677--705},
title = {{Multimodal Word Meaning Induction From Minimal Exposure to Natural Text}},
volume = {41},
year = {2017}
}
@article{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
archivePrefix = {arXiv},
arxivId = {1706.01427},
author = {Santoro, Adam and Raposo, David and Barrett, David G.T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
eprint = {1706.01427},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santoro et al. - 2017 - A simple neural network module for relational reasoning.pdf:pdf},
journal = {Arxiv},
pages = {1--16},
title = {{A simple neural network module for relational reasoning}},
year = {2017}
}
@article{Trench2017,
author = {Trench, M{\'{a}}ximo and Taverini, Lucia M and Goldstone, Robert L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trench, Taverini, Goldstone - 2017 - Promoting Spontaneous Analogical Transfer by Idealizing Target Representations.pdf:pdf},
journal = {Proceedings of the 39th Annual Conference of the Cognitive Science Society},
keywords = {analogy,idealization,retrieval,transfer},
pages = {1206--1211},
title = {{Promoting Spontaneous Analogical Transfer by Idealizing Target Representations}},
year = {2017}
}
@article{Hermann2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.06551v2},
author = {Hermann, Karl Moritz and Hill, Felix and Green, Simon and Wang, Fumin and Faulkner, Ryan and Soyer, Hubert and Szepesvari, David and Czarnecki, Wojciech Marian and Jaderberg, Max and Teplyashin, Denis and Wainwright, Marcus and Apps, Chris and Hassabis, Demis},
eprint = {arXiv:1706.06551v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hermann et al. - 2017 - Grounded Language Learning in a Simulated 3D World.pdf:pdf},
journal = {arXiv},
pages = {1--22},
title = {{Grounded Language Learning in a Simulated 3D World}},
year = {2017}
}
@article{Vaswani,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.03762v4},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
eprint = {arXiv:1706.03762v4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
journal = {arXiv},
title = {{Attention Is All You Need}},
year = {2017}
}
@article{Higgins2017,
author = {Higgins, Irina and Sonnerar, Nicolas and Et al., ,},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Higgins, Sonnerar, Et al. - 2017 - SCAN Learning Abstract Hierarchical Compositional Visual Concepts.pdf:pdf},
journal = {arXiv},
title = {{SCAN: Learning Abstract Hierarchical
Compositional Visual Concepts}},
year = {2017}
}
@article{Elgammal2017,
abstract = {We propose a new system for generating art. The system generates art by looking at art and learning about style; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles. We build over Generative Adversarial Networks (GAN), which have shown the ability to learn to generate novel images simulating a given distribution. We argue that such networks are limited in their ability to generate creative products in their original design. We propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing deviation from art distribution. We conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists. The results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs. Human subjects even rated the generated images higher on various scales.},
archivePrefix = {arXiv},
arxivId = {1706.07068},
author = {Elgammal, Ahmed and Liu, Bingchen and Elhoseiny, Mohamed and Mazzone, Marian},
eprint = {1706.07068},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elgammal et al. - 2017 - CAN Creative Adversarial Networks, Generating {\&}quotArt{\&}quot by Learning About Styles and Deviating from Style N.pdf:pdf},
number = {Iccc},
pages = {1--22},
title = {{CAN: Creative Adversarial Networks, Generating "Art" by Learning About Styles and Deviating from Style Norms}},
year = {2017}
}
@article{Ericsson2017,
author = {Ericsson, K. Anders},
doi = {10.1002/wcs.1382},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ericsson - 2017 - Expertise and individual differences the search for the structure and acquisition of experts superior performance.pdf:pdf},
issn = {19395086},
journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
number = {1-2},
pages = {1--6},
title = {{Expertise and individual differences: the search for the structure and acquisition of experts??? superior performance}},
volume = {8},
year = {2017}
}
@article{Ritter2017,
abstract = {Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.},
archivePrefix = {arXiv},
arxivId = {1706.08606},
author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
eprint = {1706.08606},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ritter et al. - 2017 - Cognitive Psychology for Deep Neural Networks A Shape Bias Case Study.pdf:pdf},
title = {{Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study}},
year = {2017}
}
@article{Christiano2017,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
eprint = {1706.03741},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:pdf},
journal = {arXi},
title = {{Deep reinforcement learning from human preferences}},
year = {2017}
}
@article{Xu2017,
abstract = {Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from the expensive computation. We propose a new strategy, $\backslash$emph{\{}feature squeezing{\}}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training.},
archivePrefix = {arXiv},
arxivId = {1704.01155},
author = {Xu, Weilin and Evans, David and Qi, Yanjun},
eprint = {1704.01155},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Evans, Qi - 2017 - Feature Squeezing Detecting Adversarial Examples in Deep Neural Networks.pdf:pdf},
title = {{Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks}},
year = {2017}
}
@inproceedings{Lampinen2017,
author = {Lampinen, Andrew and Hsu, Shaw and Mcclelland, James L},
booktitle = {Proceedings of the Cognitive Science Society 2017 (Accepted)},
keywords = {analogy,neural networks,representa-,structure learning,tion,transfer},
title = {{Analogies Emerge from Learning Dyamics in Neural Networks}},
year = {2017}
}
@article{Ha,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.03477v1},
author = {Ha, David and Eck, Douglas},
eprint = {arXiv:1704.03477v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ha, Eck - 2017 - A Neural Representation of Sketch Drawings.pdf:pdf},
journal = {arXiv},
pages = {1--20},
title = {{A Neural Representation of Sketch Drawings}},
year = {2017}
}
@article{Mcshane2017,
abstract = {A typical behavioral research paper features multiple studies of a common phenomenon that are analyzed solely in isolation. Because the studies are of a common phenomenon, this practice is inefficient and foregoes important benefits that be obtained only by analyzing them jointly in a single paper meta-analysis (SPM). To facilitate SPM, we introduce metaanalytic methodology that is user-friendly, widely applicable, and specially tailored to the SPM of the set of studies that appear in a typical behavioral research paper. Our SPM methodology provides important benefits for study summary, theory-testing, and replicability that we illustrate via three case studies that include papers recently published in the Journal of Consumer Research and the Journal of Marketing Research. We advocate that authors of typical behavioral research papers use it to supplement the single-study analyses that independently discuss the multiple studies in the body of their papers as well as the "qualitative meta-analysis" that verbally synthesizes the studies in the general discussion of their papers. When used as such, this requires only a minor modification of current practice. We provide an easy-to-use website that implements our SPM methodology.},
author = {Mcshane, Blakeley B. and B{\"{o}}ckenholt, Ulf},
doi = {10.1093/jcr/ucw085},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcshane, B{\"{o}}ckenholt - 2017 - Single paper meta-analysis Benefits for study summary, theory-testing, and replicability.pdf:pdf},
issn = {0093-5301},
journal = {Journal of Consumer Research},
keywords = {between-study variation,heterogeneity,hierarchical,meta-analysis,multilevel,random effects},
pages = {ucw085},
title = {{Single paper meta-analysis: Benefits for study summary, theory-testing, and replicability}},
volume = {43},
year = {2017}
}
@article{Strub2017,
abstract = {End-to-end design of dialogue systems has recently become a popular research topic thanks to powerful tools such as encoder-decoder architectures for sequence-to-sequence learning. Yet, most current approaches cast human-machine dialogue management as a supervised learning problem, aiming at predicting the next utterance of a participant given the full history of the dialogue. This vision is too simplistic to render the intrinsic planning problem inherent to dialogue as well as its grounded nature, making the context of a dialogue larger than the sole history. This is why only chit-chat and question answering tasks have been addressed so far using end-to-end architectures. In this paper, we introduce a Deep Reinforcement Learning method to optimize visually grounded task-oriented dialogues, based on the policy gradient algorithm. This approach is tested on a dataset of 120k dialogues collected through Mechanical Turk and provides encouraging results at solving both the problem of generating natural dialogues and the task of discovering a specific object in a complex picture.},
archivePrefix = {arXiv},
arxivId = {1703.05423},
author = {Strub, Florian and de Vries, Harm and Mary, Jeremie and Piot, Bilal and Courville, Aaron and Pietquin, Olivier},
eprint = {1703.05423},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strub et al. - 2017 - End-to-end optimization of goal-driven and visually grounded dialogue systems.pdf:pdf},
journal = {arXiv preprint},
title = {{End-to-end optimization of goal-driven and visually grounded dialogue systems}},
year = {2017}
}
@article{Hansen2017,
author = {Hansen, Steven S. and Lampinen, Andrew and Suri, Gaurav and McClelland, James L.},
journal = {Behavioral and Brain Sciences},
title = {{Building on prior knowledge without building it in}},
volume = {40},
year = {2017}
}
@article{Shwartz-Ziv2017,
abstract = {Despite their great success, there is still no com-prehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their in-ner organization. Previous work [Tishby {\&} Za-slavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the In-formation Bottleneck (IB) tradeoff between com-pression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. We first show that the stochastic gradient descent (SGD) epochs have two distinct phases: fast empirical error minimization followed by slow representation compression, for each layer. We then argue that the DNN layers end up very close to the IB theo-retical bound, and present a new theoretical argu-ment for the computational benefit of the hidden layers.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1703.00810},
author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
eprint = {1703.00810},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shwartz-Ziv, Tishby - 1999 - Opening the Black Box of Deep Neural Networks via Information.pdf:pdf},
journal = {arXiv},
title = {{Opening the Black Box of Deep Neural Networks via Information}},
year = {2017}
}
@article{Fernando2017,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1701.08734},
author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
eprint = {1701.08734},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fernando et al. - 2017 - PathNet Evolution Channels Gradient Descent in Super Neural Networks.pdf:pdf},
keywords = {basal ganglia,continual learning,evolution and,giant networks,learning,multitask,path evolution algorithm,transfer learning},
title = {{PathNet: Evolution Channels Gradient Descent in Super Neural Networks}},
year = {2017}
}
@article{Singh2017,
author = {Singh, Kusum and Granville, Monique and Dika, Sandra},
doi = {10.1080/00220670209596607},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh, Granville, Dika - 2017 - Mathematics and Science Achievement Effects of Motivation, Interest, and Academic Engagement.pdf:pdf},
journal = {The Journal of Education Research},
keywords = {1988,academic engagement effects,are a crit-,grades 5 through 8,he middle school years,interest,longitudinal study,mathematics and science achievement,motivation,national education},
number = {February},
title = {{Mathematics and Science Achievement: Effects of Motivation, Interest, and Academic Engagement}},
volume = {0671},
year = {2017}
}
@article{Oh2017,
annote = {Seems like a lot of the advantage is built in by their "analogy regularization" which is more of building in the solution to an entirely new cost function than a regularization per se.},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oh et al. - 2017 - COMMUNICATING HIERARCHICAL NEURAL CONTROLLERS FOR LEARNING ZERO - SHOT TASK GENERALIZATION.pdf:pdf},
journal = {Submitted to ICLR 2017},
title = {{COMMUNICATING HIERARCHICAL NEURAL CONTROLLERS FOR LEARNING ZERO - SHOT TASK GENERALIZATION}},
year = {2017}
}
@article{Cvpr2017,
archivePrefix = {arXiv},
arxivId = {1612.02297},
author = {Cvpr, Anonymous and Id, Paper},
eprint = {1612.02297},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cvpr, Id - 2017 - Spatially Adaptive Computation Time for Residual Networks.pdf:pdf},
pages = {1--9},
title = {{Spatially Adaptive Computation Time for Residual Networks}},
year = {2017}
}
@article{Lake2016,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving perfor- mance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recog- nition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
archivePrefix = {arXiv},
arxivId = {1604.00289},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
eprint = {1604.00289},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake et al. - 2017 - Building Machines that learn and think like people.pdf:pdf},
journal = {Behavioral and Brain Sciences},
pages = {1--55},
pmid = {1000303116},
title = {{Building Machines that learn and think like people}},
year = {2017}
}
@article{Silver2016,
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Driessche, George Van Den and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray},
doi = {10.1038/nature16961},
file = {:home/andrew/Documents/grad/Papers/AlphaGoNaturePaper.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7585},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Zhang2016,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1611.03530},
file = {:home/andrew/Documents/grad/Papers/1611.03530.pdf:pdf},
isbn = {1506.02142},
issn = {10414347},
pmid = {88045},
title = {{Understanding deep learning requires rethinking generalization}},
url = {http://arxiv.org/abs/1611.03530},
year = {2016}
}
@article{Duncan2016,
abstract = {Using memory to guide decisions allows past experience to improve future outcomes. However, the circumstances that modulate how and when memory influences decisions are not well understood. Here, we report that the use of memories to guide decisions depends on the context in which these decisions are made. We show that decisions made in the context of familiar images are more likely to be influenced by past events than are decisions made in the context of novel images (Experiment 1), that this bias persists even when a temporal gap is introduced between the image presentation and the decision (Experiment 2), and that contextual novelty facilitates value learning whereas familiarity facilitates the retrieval and use of previously learned values (Experiment 3). These effects are consistent with neurobiological and computational models of memory, which propose that familiar images evoke a lingering “retrieval state” that facilitates the recollection of other episodic memories. Together, these experiments highlight the importance of episodic memory for decision-making and provide an example of how computational and neurobiological theories can lead to new insights into how and when different types of memories guide our choices.},
author = {Duncan, Katherine D. and Shohamy, Daphna},
doi = {10.1037/xge0000231},
file = {:home/andrew/Documents/grad/Papers/Duncan{\_}2016.pdf:pdf},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
keywords = {Context,Decision-making,Episodic memory,Memory states,Novelty},
number = {11},
pages = {1420--1426},
pmid = {27797556},
title = {{Memory states influence value-based decisions}},
volume = {145},
year = {2016}
}
@article{Andreas2016,
abstract = {We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them---specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor--critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.},
archivePrefix = {arXiv},
arxivId = {1611.01796},
author = {Andreas, Jacob and Klein, Dan and Levine, Sergey},
eprint = {1611.01796},
file = {:home/andrew/Documents/grad/Papers/1611.01796.pdf:pdf},
issn = {1938-7228},
title = {{Modular Multitask Reinforcement Learning with Policy Sketches}},
url = {http://arxiv.org/abs/1611.01796},
year = {2016}
}
@article{Lahiri2016,
abstract = {Interesting data often concentrate on low dimensional smooth manifolds inside a high dimensional ambient space. Random projections are a simple, powerful tool for dimensionality reduction of such data. Previous works have studied bounds on how many projections are needed to accurately preserve the geometry of these manifolds, given their intrinsic dimensionality, volume and curvature. However, such works employ definitions of volume and curvature that are inherently difficult to compute. Therefore such theory cannot be easily tested against numerical simulations to understand the tightness of the proven bounds. We instead study typical distortions arising in random projections of an ensemble of smooth Gaussian random manifolds. We find explicitly computable, approximate theoretical bounds on the number of projections required to accurately preserve the geometry of these manifolds. Our bounds, while approximate, can only be violated with a probability that is exponentially small in the ambient dimension, and therefore they hold with high probability in cases of practical interest. Moreover, unlike previous work, we test our theoretical bounds against numerical experiments on the actual geometric distortions that typically occur for random projections of random smooth manifolds. We find our bounds are tighter than previous results by several orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1607.04331},
author = {Lahiri, Subhaneil and Gao, Peiran and Ganguli, Surya},
eprint = {1607.04331},
file = {:home/andrew/Documents/grad/Papers/1607.04331.pdf:pdf},
isbn = {0001406108},
journal = {arXiv},
pages = {1--45},
title = {{Random projections of random manifolds}},
url = {http://arxiv.org/abs/1607.04331},
year = {2016}
}
@article{Ha2016,
abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
archivePrefix = {arXiv},
arxivId = {1609.09106},
author = {Ha, David and Dai, Andrew and Le, Quoc V.},
eprint = {1609.09106},
file = {:home/andrew/Documents/grad/Papers/1609.09106.pdf:pdf},
journal = {arXiv},
title = {{HyperNetworks}},
url = {http://arxiv.org/abs/1609.09106},
year = {2016}
}
@article{Yang2016,
author = {Yang, Chunliang and Shanks, David R.},
file = {:home/andrew/Documents/grad/Papers/Yang {\%}26 Shanks (JEPLMC).pdf:pdf},
title = {{The forward testing effect: interim testing enhances inductive learning}},
year = {2016}
}
@article{Gauthier2016,
abstract = {A distinguishing property of human intelligence is the ability to flexibly use language in order to communicate complex ideas with other humans in a variety of contexts. Research in natural language dialogue should focus on designing communicative agents which can integrate themselves into these contexts and productively collaborate with humans. In this abstract, we propose a general situated language learning paradigm which is designed to bring about robust language agents able to cooperate productively with humans.},
archivePrefix = {arXiv},
arxivId = {1610.03585},
author = {Gauthier, Jon and Mordatch, Igor},
eprint = {1610.03585},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gauthier, Mordatch - 2016 - A Paradigm for Situated and Goal-Driven Language Learning.pdf:pdf},
journal = {arXiv},
title = {{A Paradigm for Situated and Goal-Driven Language Learning}},
year = {2016}
}
@inproceedings{Cotterell2016,
author = {Cotterell, Ryan and Schuetze, Hinrich and Eisner, Jason},
booktitle = {Proceedings of the 54th Annual Meeting of the ACL},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cotterell, Schuetze, Eisner - 2016 - Morphological Smoothing and Extrapolation of Word Embeddings.pdf:pdf},
title = {{Morphological Smoothing and Extrapolation of Word Embeddings}},
year = {2016}
}
@article{Liu2016,
abstract = {An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.},
archivePrefix = {arXiv},
arxivId = {1611.02770},
author = {Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
eprint = {1611.02770},
journal = {arXiv},
number = {2},
pages = {1--24},
title = {{Delving into Transferable Adversarial Examples and Black-box Attacks}},
year = {2016}
}
@article{Nguyen2016,
abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
archivePrefix = {arXiv},
arxivId = {1602.03616},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
eprint = {1602.03616},
journal = {arXiv},
title = {{Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks}},
year = {2016}
}
@article{Szegedy2016,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2{\{}{\%}{\}} top-1 and 5.6{\{}{\%}{\}} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1002/2014GB005021},
eprint = {1512.00567},
isbn = {9781617796029},
issn = {08866236},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2818--2826},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
year = {2016}
}
@article{Odena2016,
abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7{\{}{\%}{\}} of the classes have samples exhibiting diversity comparable to real ImageNet data.},
archivePrefix = {arXiv},
arxivId = {1610.09585},
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
eprint = {1610.09585},
issn = {1938-7228},
journal = {arXiv},
pages = {1--14},
title = {{Conditional Image Synthesis With Auxiliary Classifier GANs}},
year = {2016}
}
@article{Rothe2016,
author = {Rothe, Sascha and Schuetze, Hinrich},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rothe, Schuetze - 2016 - Word Embedding Calculus in Meaningful Ultradense Subspaces.pdf:pdf},
pages = {512--517},
title = {{Word Embedding Calculus in Meaningful Ultradense Subspaces}},
year = {2016}
}
@article{Odena2016,
abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7{\%} of the classes have samples exhibiting diversity comparable to real ImageNet data.},
archivePrefix = {arXiv},
arxivId = {1610.09585},
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
eprint = {1610.09585},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Odena, Olah, Shlens - 2016 - Conditional Image Synthesis With Auxiliary Classifier GANs.pdf:pdf},
journal = {arXiv},
pages = {1--16},
title = {{Conditional Image Synthesis With Auxiliary Classifier GANs}},
year = {2016}
}
@article{Schulz,
author = {Schulz, Eric and Tenenbaum, Joshua B and Duvenaud, David and Speekenbrink, Maarten and Gershman, Samuel J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulz et al. - 2016 - Compositional Inductive Biases in Function Learning.pdf:pdf},
journal = {bioRxiv},
title = {{Compositional Inductive Biases in Function Learning}},
year = {2016}
}
@article{Gentner2016,
author = {Gentner, Dedre},
doi = {10.1037/amp0000082},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gentner - 2016 - Language as Cognitive Tool Kit How Language Supports Relational Thought.pdf:pdf},
issn = {0003066X},
journal = {American Psychologist},
keywords = {language and cognition,relational learning,relational meaning},
number = {8},
pages = {650--657},
title = {{Language as Cognitive Tool Kit : How Language Supports Relational Thought}},
volume = {71},
year = {2016}
}
@article{Forbus2016,
abstract = {Analogy and similarity are central phenomena in human cognition, involved in processes ranging from visual perception to conceptual change. To capture this centrality requires that a model of comparison must be able to integrate with other processes and handle the size and complexity of the representations required by the tasks being modeled. This paper describes extensions to Structure-Mapping Engine (SME) since its inception in 1986 that have increased its scope of operation. We first review the basic SME algorithm, describe psychological evidence for SME as a process model, and summarize its role in simulating similarity-based retrieval and generalization. Then we describe five techniques now incorporated into the SME that have enabled it to tackle large-scale modeling tasks: (a) Greedy merging rapidly constructs one or more best interpretations of a match in polynomial time: O(n2 log(n)); (b) Incremental operation enables mappings to be extended as new information is retrieved or derived about the base or target, to model situations where information in a task is updated over time; (c) Ubiquitous predicates model the varying degrees to which items may suggest alignment; (d) Structural evaluation of analogical inferences models aspects of plausibility judgments; (e) Match filters enable large-scale task models to communicate constraints to SME to influence the mapping process. We illustrate via examples from published studies how these enable it to capture a broader range of psychological phenomena than before.},
author = {Forbus, Kenneth D. and Ferguson, Ronald W. and Lovett, Andrew and Gentner, Dedre},
doi = {10.1111/cogs.12377},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Forbus et al. - 2016 - Extending SME to Handle Large-Scale Cognitive Modeling.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Analogical learning,Analogical reasoning,Analogy,Artificial intelligence,Cognitive psychology,Cognitive simulation,Similarity,Symbolic modeling},
number = {4},
pages = {1--109},
pmid = {27322750},
title = {{Extending SME to Handle Large-Scale Cognitive Modeling}},
year = {2016}
}
@article{Papernot2016,
abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95{\%} to less than 0.5{\%} on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10{\^{}}30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800{\%} on one of the DNNs we tested.},
archivePrefix = {arXiv},
arxivId = {1511.04508},
author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
doi = {10.1109/SP.2016.41},
eprint = {1511.04508},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks.pdf:pdf},
isbn = {9781509008247},
journal = {Proceedings - 2016 IEEE Symposium on Security and Privacy, SP 2016},
pages = {582--597},
pmid = {7546524},
title = {{Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks}},
year = {2016}
}
@article{Moosavi-Dezfooli2016,
abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool framework to efficiently compute perturbations that fools deep network and thus reliably quantify the robustness of arbitrary classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust. To encourage reproducible research, the code of DeepFool will be available online.},
archivePrefix = {arXiv},
arxivId = {1511.04599},
author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
doi = {10.1109/CVPR.2016.282},
eprint = {1511.04599},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moosavi-Dezfooli, Fawzi, Frossard - 2016 - DeepFool a simple and accurate method to fool deep neural networks.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {Cvpr},
pages = {2574--2582},
title = {{DeepFool: a simple and accurate method to fool deep neural networks}},
year = {2016}
}
@article{Carlini2016,
abstract = {We show that defensive distillation is not secure: it is no more resistant to targeted misclassification attacks than unprotected neural networks.},
archivePrefix = {arXiv},
arxivId = {1607.04311},
author = {Carlini, Nicholas and Wagner, David},
eprint = {1607.04311},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlini, Wagner - 2016 - Defensive Distillation is Not Robust to Adversarial Examples.pdf:pdf},
journal = {Arxiv},
pages = {1--3},
title = {{Defensive Distillation is Not Robust to Adversarial Examples}},
volume = {0},
year = {2016}
}
@article{Papernot2016a,
abstract = {Deep Learning is increasingly used in several machine learning tasks as Deep Neural Networks (DNNs) frequently outperform other techniques. Yet, previous work showed that, once deployed, DNNs are vulnerable to integrity attacks. Indeed, adversaries can control DNN outputs and for instance force them to misclassify inputs by adding a carefully crafted and undistinguishable perturbation. However, these attacks assumed knowledge of the targeted DNN's architecture and parameters. In this paper however, we release these assumptions and introduce an attack conducted under the more realistic, yet more complex, threat model of an oracle: adversaries are only capable of accessing DNN label predictions for chosen inputs. We evaluate our attack in real-world settings by successfully forcing an oracle served by MetaMind, an online API for DNN classifiers, to misclassify inputs at a 84.24{\%} rate. We also perform an evaluation to fine-tune our attack strategy and maximize the oracle's misclassification rate for adversarial samples.},
archivePrefix = {arXiv},
arxivId = {1602.02697},
author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
eprint = {1602.02697},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papernot et al. - 2016 - Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples.pdf:pdf},
isbn = {9781450349444},
journal = {arXiv},
title = {{Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples}},
year = {2016}
}
@inproceedings{Drozd2016,
author = {Drozd, Aleksandr and Gladkova, Anna and Matsuoka, Satoshi},
booktitle = {Proceedings of COLING 2016},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drozd, Gladkova, Matsuoka - 2016 - Word Embeddings, Analogies, and Machine Learning Beyond King - M an W oman = Queen.pdf:pdf},
pages = {3519--3530},
title = {{Word Embeddings, Analogies, and Machine Learning: Beyond King - M an + W oman = Queen}},
year = {2016}
}
@article{Reed2016,
author = {Reed, Stephen K},
doi = {10.1177/1745691616646304},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reed - 2016 - A Taxonomic Analysis of Abstraction.pdf:pdf},
journal = {Perspectives on Psychological Science},
keywords = {a vivid illustration of,abstraction,attributes,by an engineer describing,categories,hierarchies,his,his weekly meeting with,instances,is revealed,the power of abstraction},
number = {1968},
title = {{A Taxonomic Analysis of Abstraction}},
year = {2016}
}
@article{Vezhnevets2016a,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.01161v2},
author = {Vezhnevets, Alexander Sasha and Com, Korayk Google},
eprint = {arXiv:1703.01161v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vezhnevets, Com - 2016 - FeUdal Networks for Hierarchical Reinforcement Learning arXiv 1703 . 01161v2 cs . AI 6 Mar 2017.pdf:pdf},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning arXiv : 1703 . 01161v2 [ cs . AI ] 6 Mar 2017}},
year = {2016}
}
@article{DeVries2016,
abstract = {We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.},
archivePrefix = {arXiv},
arxivId = {1611.08481},
author = {de Vries, Harm and Strub, Florian and Chandar, Sarath and Pietquin, Olivier and Larochelle, Hugo and Courville, Aaron},
eprint = {1611.08481},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Vries et al. - 2016 - GuessWhat! Visual object discovery through multi-modal dialogue.pdf:pdf},
journal = {arXiv preprint},
pages = {23},
title = {{GuessWhat?! Visual object discovery through multi-modal dialogue}},
year = {2016}
}
@article{Mnih2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.01783v2},
author = {Mnih, Volodymyr and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David},
eprint = {arXiv:1602.01783v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
journal = {International Conference on Machine Learning},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
year = {2016}
}
@article{Long2016,
author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Long et al. - 2016 - Unsupervised Domain Adaptation with Residual Transfer Networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Unsupervised Domain Adaptation with Residual Transfer Networks}},
year = {2016}
}
@article{Vezhnevets2016,
author = {Vezhnevets, Alexander Sasha and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vezhnevets et al. - 2016 - Strategic Attentive Writer for Learning Macro-Actions.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Strategic Attentive Writer for Learning Macro-Actions}},
year = {2016}
}
@article{Alemi2016,
abstract = {We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method "Deep Variational Information Bottleneck", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1612.00410},
author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
eprint = {1612.00410},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alemi et al. - 2016 - Deep Variational Information Bottleneck.pdf:pdf},
pages = {13},
title = {{Deep Variational Information Bottleneck}},
year = {2016}
}
@article{Kirkpatrick2016,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1612.00796},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
eprint = {1612.00796},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirkpatrick et al. - 2016 - Overcoming catastrophic forgetting in neural networks.pdf:pdf},
journal = {arXiv preprint},
title = {{Overcoming catastrophic forgetting in neural networks}},
year = {2016}
}
@misc{NCES2016,
author = {NCES},
title = {{NCES SAT Statistics}},
year = {2016}
}
@article{Haimovitz2016,
author = {Haimovitz, Kyla and Dweck, Carol S},
doi = {10.1177/0956797616639727},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haimovitz, Dweck - 2016 - What Predicts Children's Fixed and Growth Intelligence Mind-Sets Not Their Parents' Views of Intelligence.pdf:pdf},
journal = {Psychological Science},
keywords = {15,16,17,25,academic achievement,and policymakers agree that,childhood development,educational psychology,educators,motivation,open materials,par-,received 9,researchers,revision accepted 2},
number = {6},
pages = {859--869},
title = {{What Predicts Children's Fixed and Growth Intelligence Mind-Sets? Not Their Parents' Views of Intelligence but Their Parents' Views of Failure}},
volume = {27},
year = {2016}
}
@article{Rau2016,
abstract = {Visual representations play a critical role in enhancing science, technology, engi- neering, and mathematics (STEM) learning. Educational psychology research shows that adding visual representations to text can enhance students' learning of content knowl- edge, compared to text-only. But should students learn with a single type of visual representation or with multiple different types of visual representations? This article addresses this question from the perspective of the representation dilemma, namely that students often learn content they do not yet understand from representations they do not yet understand. To benefit from visual representations, students therefore need representational competencies, that is, knowledge about how visual representations depict information about the content. This article reviews literature on representational competencies involved in students' learning of content knowledge. Building on this review, this article analyzes how the number of visual representations affects the role these representational competencies play during students' learning of content knowl- edge. To this end, the article compares two common scenarios: text plus a single type of visual representations (T+SV) and text plus multiple types of visual representations (T+MV). The comparison yields seven hypotheses that describe under which condi- tions T+MV scenarios are more effective than T+SV scenarios. Finally, the article reviews empirical evidence for each hypothesis and discusses open questions about the representation dilemma.},
author = {Rau, Martina A.},
doi = {10.1007/s10648-016-9365-3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rau - 2016 - Conditions for the Effectiveness of Multiple Visual Representations in Enhancing STEM Learning.pdf:pdf},
issn = {1573336X},
journal = {Educational Psychology Review},
keywords = {Conceptual and perceptual knowledge,Inductive learning processes,Multiple external representations,Sense-making processes,Visualizations},
pages = {1--45},
publisher = {Educational Psychology Review},
title = {{Conditions for the Effectiveness of Multiple Visual Representations in Enhancing STEM Learning}},
year = {2016}
}
@article{Andreas,
archivePrefix = {arXiv},
arxivId = {arXiv:1601.01705v4},
author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
eprint = {arXiv:1601.01705v4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreas et al. - 2016 - Learning to Compose Neural Networks for Question Answering.pdf:pdf},
journal = {arXiv preprint},
title = {{Learning to Compose Neural Networks for Question Answering}},
year = {2016}
}
@article{Wang2016,
abstract = {We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players.},
archivePrefix = {arXiv},
arxivId = {1606.02447},
author = {Wang, Sida I. and Liang, Percy and Manning, Christopher D.},
doi = {10.18653/v1/P16-1224},
eprint = {1606.02447},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Liang, Manning - 2016 - Learning Language Games through Interaction.pdf:pdf},
journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)},
pages = {2368--2378},
title = {{Learning Language Games through Interaction}},
year = {2016}
}
@incollection{Mcclelland,
author = {Mcclelland, J L and Sadeghi, Zahra and Saxe, A M},
booktitle = {Neurocomputational Models of Cognitive Development and Processing},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcclelland, Sadeghi, Saxe - 2016 - A Critique of Pure Hierarchy Uncovering Cross-Cutting Structure in a Natural Dataset.pdf:pdf},
pages = {51--68},
title = {{A Critique of Pure Hierarchy : Uncovering Cross-Cutting Structure in a Natural Dataset}},
year = {2016}
}
@article{Yamins2016a,
author = {Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1038/nn.4244},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yamins, Dicarlo - 2016 - Using goal-driven deep learning models to understand sensory cortex.pdf:pdf},
journal = {Nature Neuroscience},
number = {3},
pages = {356--365},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
volume = {19},
year = {2016}
}
@article{Johnson2016a,
abstract = {We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-the-art results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.},
archivePrefix = {arXiv},
arxivId = {1611.04558},
author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'{e}}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1611.04558},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2016 - Google's Multilingual Neural Machine Translation System Enabling Zero-Shot Translation.pdf:pdf},
journal = {arXiv},
pages = {1--16},
title = {{Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation}},
year = {2016}
}
@article{Jaderberg2016,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the- art on Atari, averaging 880{\%} expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87{\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.03044v2},
author = {Jaderberg, Max and Mnih, Volodymyr and Czarneckim, Marian Wojciech and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
eprint = {arXiv:1509.03044v2},
journal = {arXiv},
pages = {1--11},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
year = {2016}
}
@article{Luong2016,
abstract = {Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three settings to multi-task sequence to sequence learning: (a) the one-to-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on parsing and image caption generation improves translation accuracy and vice versa. We also present novel findings on the benefit of the different unsupervised learning objectives: we found that the skip-thought objective is beneficial to translation while the sequence autoencoder objective is not.},
archivePrefix = {arXiv},
arxivId = {1511.06114},
author = {Luong, Minh-Thang and Le, Quoc V. and Sutskever, Ilya and Vinyals, Oriol and Kaiser, Lukasz},
eprint = {1511.06114},
isbn = {9789876400817},
journal = {Iclr},
pages = {1--9},
title = {{Multi-task Sequence to Sequence Learning}},
year = {2016}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144v2},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
journal = {arXiv},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Learning},
pages = {1--23},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
year = {2016}
}
@article{Lake2016a,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving perfor- mance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recog- nition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
archivePrefix = {arXiv},
arxivId = {1604.00289},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
doi = {1511.09249v1},
eprint = {1604.00289},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake et al. - 2016 - Building Machines that learn and think like people.pdf:pdf},
issn = {14691825},
journal = {arXiv:1604.00289v1[cs.AI]},
number = {2012},
pages = {1--55},
pmid = {1000303116},
title = {{Building Machines that learn and think like people}},
year = {2016}
}
@article{Maddison2016,
abstract = {The reparameterization trick enables the optimization of large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack continuous reparameterizations due to the discontinuous nature of discrete states. In this work we introduce concrete random variables -- continuous relaxations of discrete random variables. The concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-likelihood of latent stochastic nodes) on the corresponding discrete graph. We demonstrate their effectiveness on density estimation and structured prediction tasks using neural networks.},
archivePrefix = {arXiv},
arxivId = {1611.00712},
author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
eprint = {1611.00712},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maddison, Mnih, Teh - 2016 - The Concrete Distribution A Continuous Relaxation of Discrete Random Variables.pdf:pdf},
pages = {1--17},
title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
year = {2016}
}
@article{Rezende2016,
abstract = {Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning.},
archivePrefix = {arXiv},
arxivId = {1603.05106},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Danihelka, Ivo and Gregor, Karol and Wierstra, Daan},
eprint = {1603.05106},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rezende et al. - 2016 - One-Shot Generalization in Deep Generative Models.pdf:pdf},
journal = {Arxiv},
title = {{One-Shot Generalization in Deep Generative Models}},
volume = {48},
year = {2016}
}
@article{Graves2016,
abstract = {Modern computers separate computation and memory. Computation is performed by a processor, which can use an addressable memory to bring operands in and out of play. This confers two important benefits: the use of extensible storage to write new information and the ability to treat the contents of memory as variables. Variables are critical to algorithm generality: to perform the same procedure on one datum or another, an algorithm merely has to change the address it reads from. In contrast to computers, the computational and memory resources of artificial neural networks are mixed together in the network weights and neuron activity. This is a major liability: as the memory demands of a task increase, these networks cannot allocate new storage dynam-ically, nor easily learn algorithms that act independently of the values realized by the task variables. Although recent breakthroughs demonstrate that neural networks are remarkably adept at sensory processing 1 , sequence learning 2,3 and reinforcement learning 4 , cognitive scientists and neuroscientists have argued that neural networks are limited in their ability to represent variables and data structures 5–9 , and to store data over long timescales without interference 10,11 . We aim to combine the advantages of neu-ral and computational processing by providing a neural network with read–write access to external memory. The access is narrowly focused, minimizing interference among memoranda and enabling long-term storage 12,13 . The whole system is differentiable, and can therefore be trained end-to-end with gradient descent, allowing the network to learn how to operate and organize the memory in a goal-directed manner.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and {G{\'{o}}mez Colmenarejo}, Sergio and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and {Moritz Hermann}, Karl and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1038/nature20101},
eprint = {arXiv:1410.5401v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves et al. - 2016 - Hybrid computing using a neural network with dynamic external memory.pdf:pdf},
issn = {0028-0836},
journal = {Nature Publishing Group},
number = {7626},
pages = {471--476},
publisher = {Nature Publishing Group},
title = {{Hybrid computing using a neural network with dynamic external memory}},
volume = {538},
year = {2016}
}
@article{Kumaran2016,
author = {Kumaran, Dharshan and Hassabis, Demis and McClelland, James L},
doi = {10.1016/j.tics.2016.05.004},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumaran, Hassabis, McClelland - 2016 - What Learning Systems do Intelligent Agents Need Complementary Learning Systems Theory Updated.pdf:pdf},
issn = {1364-6613},
journal = {Trends in Cognitive Sciences},
number = {7},
pages = {512--534},
publisher = {Elsevier Ltd},
title = {{What Learning Systems do Intelligent Agents Need ? Complementary Learning Systems Theory Updated}},
volume = {20},
year = {2016}
}
@article{Johnson2016,
author = {Johnson, Mark H},
doi = {10.1080/17470218.2011.590596},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson - 2016 - Face processing as a brain adaptation at multiple timescales EPS Mid-Career Award 2010 Face processing as a brain adapt.pdf:pdf},
journal = {The Quarterly Journal of Experimental Psychology},
number = {September},
title = {{Face processing as a brain adaptation at multiple timescales EPS Mid-Career Award 2010 Face processing as a brain adaptation at multiple timescales}},
volume = {0218},
year = {2016}
}
@article{Westfall2016,
author = {Westfall, Jacob and Yarkoni, Tal},
doi = {10.1371/journal.pone.0152719},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Westfall, Yarkoni - 2016 - Statistically Controlling for Confounding Constructs Is Harder than You Think.pdf:pdf},
pages = {1--22},
title = {{Statistically Controlling for Confounding Constructs Is Harder than You Think}},
year = {2016}
}
@article{Judd2016,
author = {Judd, Charles M and Westfall, Jacob and Kenny, David A.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Judd, Westfall, Kenny - 2016 - Experiments with More Than One Factor Designs, Analytic Models, and Statistical Power.pdf:pdf},
pages = {1--59},
title = {{Experiments with More Than One Factor: Designs, Analytic Models, and Statistical Power}},
year = {2016}
}
@article{Vinyals2016,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
archivePrefix = {arXiv},
arxivId = {1606.04080},
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1606.04080},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinyals et al. - 2016 - Matching Networks for One Shot Learning.pdf:pdf},
title = {{Matching Networks for One Shot Learning}},
year = {2016}
}
@article{Yamins2016,
abstract = {Propelled by advances in biologically inspired computer vision and artificial intelligence, the past five years have seen significant progress in using deep neural networks to model response patterns of neurons in visual cortex. In this paper, we briefly review this progress and then discuss eight key 'open questions' that we believe will drive research in computational models of sensory systems over the next five years, both in visual cortex and beyond. Any scientific development of long-term value opens up as many new questions as it answers. This is certainly the case with recent progress in building deep neural network models of visual cortex. In this piece, our goal is to briefly describe these recent advances, and to outline what we consider to be the most interesting open problems in cortical modeling, both in vision and beyond. We focus is on questions that will require both cutting-edge algorith-mic developments as well as next-generation neurosci-ence and cognitive science experiments.},
author = {Yamins, Daniel LK and DiCarlo, James J},
doi = {10.1016/j.conb.2016.02.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yamins, DiCarlo - 2016 - Eight open questions in the computational modeling of higher sensory cortex Brief review of recent progress.pdf:pdf},
isbn = {1873-6882 (Electronic)0959-4388 (Linking)},
issn = {1873-6882},
journal = {Current Opinion in Neurobiology},
pages = {114--120},
pmid = {26921828},
title = {{Eight open questions in the computational modeling of higher sensory cortex Brief review of recent progress}},
volume = {37},
year = {2016}
}
@article{Fausey2016,
abstract = {Human development takes place in a social context. Two pervasive sources of social information are faces and hands. Here, we provide the first report of the visual frequency of faces and hands in the everyday scenes available to infants. These scenes were collected by having infants wear head cameras during unconstrained everyday activities. Our corpus of 143hours of infant-perspective scenes, collected from 34 infants aged 1month to 2years, was sampled for analysis at 1/5Hz. The major finding from this corpus is that the faces and hands of social partners are not equally available throughout the first two years of life. Instead, there is an earlier period of dense face input and a later period of dense hand input. At all ages, hands in these scenes were primarily in contact with objects and the spatio-temporal co-occurrence of hands and faces was greater than expected by chance. The orderliness of the shift from faces to hands suggests a principled transition in the contents of visual experiences and is discussed in terms of the role of developmental gates on the timing and statistics of visual experiences.},
author = {Fausey, Caitlin M and Jayaraman, Swapnaa and Smith, Linda B},
doi = {10.1016/j.cognition.2016.03.005},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fausey, Jayaraman, Smith - 2016 - From faces to hands Changing visual input in the first two years.pdf:pdf},
issn = {1873-7838},
journal = {Cognition},
keywords = {Egocentric vision,Faces,Hands,Head camera,Infancy,Scene statistics},
pages = {101--107},
pmid = {27043744},
publisher = {Elsevier B.V.},
title = {{From faces to hands: Changing visual input in the first two years.}},
volume = {152},
year = {2016}
}
@article{Andrychowicz2016,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
archivePrefix = {arXiv},
arxivId = {1606.04474},
author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and de Freitas, Nando},
eprint = {1606.04474},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient descent.pdf:pdf},
pages = {1--16},
title = {{Learning to learn by gradient descent by gradient descent}},
year = {2016}
}
@article{PooleB.2016,
archivePrefix = {arXiv},
arxivId = {1606.05340},
author = {{et al. Poole, B.}},
eprint = {1606.05340},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/et al. Poole, B. - 2016 - Exponential expressivity in deep neural networks through transient chaos.pdf:pdf},
journal = {arXiv preprint arXiv:1606.05340v2},
title = {{Exponential expressivity in deep neural networks through transient chaos}},
year = {2016}
}
@article{Lupyan2016,
abstract = {For over 300 years, the humble triangle has served as the paradigmatic example of the problem of abstraction. How can we have the idea of a general triangle even though every experience with triangles is with specific ones? Classical cognitive science seemed to provide an answer in symbolic representation. With its easily enumerated necessary and sufficient conditions, the triangle would appear to be a ideal candidate for being represented in a symbolic form. I show that it is not. Across a variety of tasks—drawing, speeded recognition, unspeeded visual judgments, and inference—representations of triangles appear to be graded and contextdependent. I show that using the category name “triangle” activates a more prototypical representation than using an arguably coextensive cue, “three-sided polygon.” For example, when asked to draw “triangles” people draw more typical triangles than when asked to draw “three-sided polygons.” Altogether, the results support the view that (even formal) concepts have a graded and flexible structure which takes on a more prototypical and stable form when activated by category labels.},
author = {Lupyan, Gary},
doi = {10.1080/17470218.2015.1130730},
issn = {1747-0218},
journal = {The Quarterly Journal of Experimental Psychology},
number = {January},
pages = {1--69},
title = {{The paradox of the universal triangle: concepts, language, and prototypes}},
year = {2016}
}
@article{Wagenmakers2016,
abstract = {The practical advantages of Bayesian inference are demonstrated through two concrete examples. In the first example, we wish to learn about a crimi-nal's IQ: a problem of parameter estimation. In the second example, we wish to quantify support in favor of a null hypothesis, and track this support as the data accumulate: a problem of hypothesis testing. The Bayesian frame-work unifies both problems within a coherent predictive framework, where parameters and models that predict the data successfully will receive a boost in plausibility, whereas parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be more in-formative, more elegant, and more flexible than the orthodox methodology that remains dominant within the field of psychology. On a sunny morning in Florida, while the birds were singing and the crickets chirping, Bob decided to throw his wife from the bedroom balcony, killing her instantly. The case is},
author = {Wagenmakers, Eric-jan and Morey, Richard D and Lee, Michael D},
doi = {10.1177/0963721416643289},
issn = {0963-7214},
keywords = {hypothesis testing,parameter estimation,prediction,updating},
pages = {1--9},
title = {{Bayesian Benefits for the Pragmatic Researcher}},
year = {2016}
}
@article{PooleB.2016,
archivePrefix = {arXiv},
arxivId = {1606.05340},
author = {et al. {Poole, B.}},
eprint = {1606.05340},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/et al. Poole, B. - 2016 - Exponential expressivity in deep neural networks through transient chaos.pdf:pdf},
journal = {arXiv preprint arXiv:1606.05340v2},
title = {{Exponential expressivity in deep neural networks through transient chaos}},
year = {2016}
}
@article{Wagenmakers2016,
abstract = {The practical advantages of Bayesian inference are demonstrated through two concrete examples. In the first example, we wish to learn about a crimi-nal's IQ: a problem of parameter estimation. In the second example, we wish to quantify support in favor of a null hypothesis, and track this support as the data accumulate: a problem of hypothesis testing. The Bayesian frame-work unifies both problems within a coherent predictive framework, where parameters and models that predict the data successfully will receive a boost in plausibility, whereas parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be more in-formative, more elegant, and more flexible than the orthodox methodology that remains dominant within the field of psychology. On a sunny morning in Florida, while the birds were singing and the crickets chirping, Bob decided to throw his wife from the bedroom balcony, killing her instantly. The case is},
author = {Wagenmakers, Eric-jan and Morey, Richard D and Lee, Michael D},
doi = {10.1177/0963721416643289},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wagenmakers, Morey, Lee - 2016 - Bayesian Benefits for the Pragmatic Researcher.pdf:pdf},
issn = {0963-7214},
keywords = {hypothesis testing,parameter estimation,prediction,updating},
pages = {1--9},
title = {{Bayesian Benefits for the Pragmatic Researcher}},
year = {2016}
}
@article{Lupyan2016,
abstract = {For over 300 years, the humble triangle has served as the paradigmatic example of the problem of abstraction. How can we have the idea of a general triangle even though every experience with triangles is with specific ones? Classical cognitive science seemed to provide an answer in symbolic representation. With its easily enumerated necessary and sufficient conditions, the triangle would appear to be a ideal candidate for being represented in a symbolic form. I show that it is not. Across a variety of tasks—drawing, speeded recognition, unspeeded visual judgments, and inference—representations of triangles appear to be graded and contextdependent. I show that using the category name “triangle” activates a more prototypical representation than using an arguably coextensive cue, “three-sided polygon.” For example, when asked to draw “triangles” people draw more typical triangles than when asked to draw “three-sided polygons.” Altogether, the results support the view that (even formal) concepts have a graded and flexible structure which takes on a more prototypical and stable form when activated by category labels.},
author = {Lupyan, Gary},
doi = {10.1080/17470218.2015.1130730},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lupyan - 2016 - The paradox of the universal triangle concepts, language, and prototypes.pdf:pdf},
issn = {1747-0218},
journal = {The Quarterly Journal of Experimental Psychology},
number = {January},
pages = {1--69},
title = {{The paradox of the universal triangle: concepts, language, and prototypes}},
year = {2016}
}
@article{Kriegeskorte2015,
author = {Kriegeskorte, Nikolaus},
file = {:home/andrew/Documents/grad/Papers/nikokri-2015-39700.pdf:pdf},
journal = {Annual Review of Vision Science},
keywords = {artificial intelligence,biological vision,computational neuroscience,computer vision,deep learning,neural network,object recognition},
pages = {417--446},
title = {{Deep neural networks: a new framework for modelling biological vision and brain information processing}},
year = {2015}
}
@article{Anzulewicz2015,
abstract = {Recently, Windey, Gevers, and Cleeremans (2013) proposed a level of processing (LoP) hypothesis claiming that the transition from unconscious to conscious perception is influenced by the level of processing imposed by task requirements. Here, we carried out two experiments to test the LoP hypothesis. In both, participants were asked to classify briefly presented pairs of letters as same or different, based either on the letters' physical features (a low-level task), or on a semantic rule (a high-level task). Stimulus awareness was measured by means of the four-point Perceptual Awareness Scale (PAS). The results showed that low or moderate stimulus visibility was reported more frequently in the low-level task than in the high-level task, suggesting that the transition from unconscious to conscious perception is more gradual in the former than in the latter. Therefore, although alternative interpretations remain possible, the results of the present study fully support the LoP hypothesis.},
author = {Anzulewicz, Anna and Asanowicz, Dariusz and Windey, Bert and Paulewicz, Borys{\l}aw and Wierzcho{\'{n}}, Micha{\l} and Cleeremans, Axel},
doi = {10.1016/j.concog.2015.05.004},
file = {:home/andrew/Documents/grad/Papers/1-s2.0-S1053810015001129-main.pdf:pdf},
issn = {10902376},
journal = {Consciousness and Cognition},
keywords = {Awareness,Consciousness,Dichotomous,Gradual,Levels of processing,Vision},
pages = {1--11},
pmid = {26057402},
title = {{Does level of processing affect the transition from unconscious to conscious perception?}},
volume = {36},
year = {2015}
}
@article{Simonsohn2015,
abstract = {When studies examine true effects, they generate right-skewed p-curves, distributions of statistically significant results with more low (.01 s) than high (.04 s) p values. What else can cause a right-skewed p-curve? First, we consider the possibility that researchers report only the smallest significant p value (as conjectured by Ulrich {\&} Miller, 2015), concluding that it is a very uncommon problem. We then consider more common problems, including (a) p-curvers selecting the wrong p values, (b) fake data, (c) honest errors, and (d) ambitiously p-hacked (beyond p {\textless} .05) results. We evaluate the impact of these common problems on the validity of p-curve analysis, and provide practical solutions that substantially increase its robustness.},
author = {Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D.},
doi = {10.1037/xge0000104},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonsohn, Simmons, Nelson - 2015 - Better P-curves Making P-curve analysis more robust to errors, fraud, and ambitious P-hacking,(2015).pdf:pdf},
isbn = {0096-3445},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
keywords = {are more likely to,as a consequence,be,because statistically significant results,is biased,it is difficult to,know whether a set,of,p -curve,p -hacking,publication bias,published scientific evidence,published than nonsignificant ones},
number = {6},
pages = {1146--1152},
pmid = {26595842},
title = {{Better P-curves: Making P-curve analysis more robust to errors, fraud, and ambitious P-hacking, a Reply to Ulrich and Miller (2015).}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000104},
volume = {144},
year = {2015}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P and Ba, Jimmy},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
journal = {Iclr},
pages = {1--15},
pmid = {172668},
title = {{Adam: A Method for Stochastic Optimization}},
year = {2015}
}
@article{Parisotto2015,
abstract = {The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed "Actor-Mimic", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.},
archivePrefix = {arXiv},
arxivId = {1511.06342},
author = {Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
eprint = {1511.06342},
journal = {arXiv},
pages = {1--16},
title = {{Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning}},
year = {2015}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
doi = {10.1063/1.4931082},
eprint = {1503.02531},
isbn = {3531207857},
issn = {0022-2488},
journal = {arXiv},
pages = {1--9},
pmid = {18249735},
title = {{Distilling the Knowledge in a Neural Network}},
year = {2015}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Gershman2015,
abstract = {Computational models of semantics have emerged as powerful tools for natural language processing. Recent work has developed models to handle compositionality, but these models have typically been evaluated on large, uncontrolled corpora. In this paper, we constructed a controlled set of phrase pairs and collected phrase similarity judgments, revealing novel insights into human semantic representation. None of the computational models that we considered were able to capture the pattern of human judgments. The results of a second experiment, using the same stimuli with a transformational judgment task, support a transformational account of similarity, according to which the similarity between phrases is inversely related to the number of edits required to transform one mental model into another. Taken together, our results indicate that popular models of compositional semantics do not capture important facets of human semantic representation.},
author = {Gershman, Samuel J and Tenenbaum, Joshua B},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gershman, Tenenbaum - 2015 - Phrase similarity in humans and machines.pdf:pdf},
journal = {Proceedings of the 37th Annual Conference of the Cognitive Science Society},
keywords = {neural networks,semantics,similarity},
pages = {776--781},
title = {{Phrase similarity in humans and machines}},
year = {2015}
}
@article{DeLeeuw2015,
author = {de Leeuw, J. R.},
journal = {Behavior Research Methods},
number = {1},
pages = {1--12},
title = {{jsPsych: A JavaScript library for creating behavioral experiments in a web browser.}},
volume = {47},
year = {2015}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
year = {2015}
}
@article{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{Goodfellow2015,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow, Shlens, Szegedy - 2015 - Explaining and Harnessing Adversarial Examples.pdf:pdf},
isbn = {1412.6572},
journal = {Iclr 2015},
pages = {1--11},
pmid = {729514},
title = {{Explaining and Harnessing Adversarial Examples}},
year = {2015}
}
@article{Day2015,
author = {Day, Samuel B and Motz, Benjamin A and Goldstone, Robert L and Bastian, Claudia C Von and Day, Samuel B},
doi = {10.3389/fpsyg.2015.01876},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day et al. - 2015 - The Cognitive Costs of Context The Effects of Concreteness and Immersiveness in Instructional.pdf:pdf},
journal = {Frontiers in Psychology},
keywords = {analogical reasoning,analogical transfer,cognition,concreteness,context,learning,transfer},
number = {December},
pages = {1--13},
title = {{The Cognitive Costs of Context : The Effects of Concreteness and Immersiveness in Instructional}},
volume = {6},
year = {2015}
}
@article{Aitchison2015,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.04544v2},
author = {Aitchison, Laurence and Latham, Peter E},
eprint = {arXiv:1505.04544v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aitchison, Latham - 2015 - Synaptic sampling A connection between PSP variability and uncertainty explains neurophysiological observati.pdf:pdf},
journal = {arXiv},
pages = {1--11},
title = {{Synaptic sampling : A connection between PSP variability and uncertainty explains neurophysiological observations}},
year = {2015}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{Mikolov2015,
abstract = {The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.},
archivePrefix = {arXiv},
arxivId = {1511.08130},
author = {Mikolov, Tomas and Joulin, Armand and Baroni, Marco},
eprint = {1511.08130},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Joulin, Baroni - 2015 - A Roadmap towards Machine Intelligence.pdf:pdf},
journal = {ArXiv},
pages = {1--39},
title = {{A Roadmap towards Machine Intelligence}},
year = {2015}
}
@article{Dong2015,
abstract = {In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.},
author = {Dong, Daxiang and Wu, Hua and He, Wei and Yu, Dianhai and Wang, Haifeng},
isbn = {9781941643723},
journal = {Acl},
pages = {1723--1732},
title = {{Multi-Task Learning for Multiple Language Translation}},
year = {2015}
}
@article{Rusu2015,
abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
archivePrefix = {arXiv},
arxivId = {1511.06295},
author = {Rusu, Andrei A and {Gomez Colmenarejo}, Sergio and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
doi = {10.1038/nature14236},
eprint = {1511.06295},
isbn = {978-1-4799-0356-6},
issn = {0028-0836},
journal = {arXiv},
pages = {1--12},
pmid = {25719670},
title = {{Policy Distillation}},
year = {2015}
}
@article{Parisotto2015,
abstract = {The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed "Actor-Mimic", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.},
archivePrefix = {arXiv},
arxivId = {1511.06342},
author = {Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
eprint = {1511.06342},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parisotto, Ba, Salakhutdinov - 2015 - Actor-Mimic Deep Multitask and Transfer Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint},
pages = {1--15},
title = {{Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning}},
year = {2015}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
volume = {7},
year = {2015}
}
@article{GoogleResearch2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.04467v2},
author = {GoogleResearch},
eprint = {arXiv:1603.04467v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/GoogleResearch - 2015 - TensorFlow Large-scale machine learning on heterogeneous systems.pdf:pdf},
title = {{TensorFlow: Large-scale machine learning on heterogeneous systems}},
year = {2015}
}
@article{Lake2015,
abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several “visual Turing tests” probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
archivePrefix = {arXiv},
arxivId = {10.1126/science.aab3050},
author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
doi = {10.1126/science.aab3050},
eprint = {science.aab3050},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake, Salakhutdinov, Tenenbaum - 2015 - Human-level concept learning through probabilistic program induction.pdf:pdf},
isbn = {0036-8075},
issn = {0036-8075},
journal = {Science},
number = {6266},
pages = {1332--1338},
pmid = {26659050},
primaryClass = {10.1126},
title = {{Human-level concept learning through probabilistic program induction}},
volume = {350},
year = {2015}
}
@article{Gregor2015,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04623v1},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
eprint = {arXiv:1502.04623v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gregor et al. - 2015 - DRAW A Recurrent Neural Network For Image Generation.pdf:pdf},
journal = {arXiv preprint},
pages = {1--16},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
year = {2015}
}
@incollection{Schwartz2015,
author = {Schwartz, Daniel L. and Goldstone, Robert L.},
booktitle = {Handbook of educational psychology},
chapter = {5},
edition = {3rd},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwartz, Goldstone - 2015 - Learning as Coordination.pdf:pdf},
title = {{Learning as Coordination}},
year = {2015}
}
@article{Markant2015,
author = {Markant, Douglas B and Settles, Burr and Gureckis, M},
doi = {10.1111/cogs.12220},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Markant, Settles, Gureckis - 2015 - Self-Directed Learning Favors Local , Rather Than Global , Uncertainty.pdf:pdf},
journal = {Cognitive Science},
keywords = {active learning,information sampling,machine learning,self-directed learning},
number = {1},
pages = {1--21},
title = {{Self-Directed Learning Favors Local , Rather Than Global , Uncertainty}},
volume = {40},
year = {2015}
}
@article{Rich2015,
author = {Rich, Alexander S and Gureckis, Todd M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rich, Gureckis - 2015 - The Attentional Learning Trap and How to Avoid It.pdf:pdf},
journal = {Proceedings of the 37th Annual Conference of the Cognitive Science Society.},
keywords = {approach-avoid behavior,attention and the hot,biases,categorization,decision-making,effect as a prob-,learning traps,on the hot stove,past work has focused,selective atten-,stove effect,tion},
title = {{The Attentional Learning Trap and How to Avoid It}},
year = {2015}
}
@article{Bates2015,
abstract = {The analysis of experimental data with mixed-effects models requires decisions about the specifica- tion of the appropriate random-effects structure. Recently, Barr et al. (2013) recommended fitting ‘maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too com- plex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification. Finally, we clarify that the simulations on which Barr et al. base their recommendations are atyp- ical for real data. A detailed example is provided of how subject-related attentional fluctuation across trials may further qualify statistical inferences about fixed effects, and of how such nonlinear effects can be accommodated within the mixed-effects modeling framework.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.04967v1},
author = {Bates, Douglas M and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
doi = {arXiv:1506.04967},
eprint = {arXiv:1506.04967v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bates et al. - 2015 - Parsimonious mixed models.pdf:pdf},
journal = {arXiv preprint arXiv:1506.04967},
keywords = {crossed random effects,generalized additive mixed models,linear mixed models,model selection,model simplicity},
pages = {1--27},
title = {{Parsimonious mixed models}},
year = {2015}
}
@article{Jayaraman2015,
abstract = {Mature face perception has its origins in the face experiences of infants. However, little is known about the basic statistics of faces in early visual environments. We used head- cameras to capture and analyze over 72,000 infant-perspective scenes from 22 infants aged 1-11 months as they engaged in daily activities. The frequency of faces in these scenes declined markedly with age: for the youngest infants, faces were present 15 minutes in every waking hour but only 5 minutes for the oldest infants. In general, the available faces were well characterized by three properties: (1) they belonged to relatively few individuals; (2) they were close and visually large; and (3) they presented views showing both eyes. These three properties most strongly characterized the face corpora of our youngest infants and constitute environmental constraints on the early development of the visual system.},
author = {Jayaraman, Swapnaa and Fausey, Caitlin M. and Smith, Linda B.},
doi = {10.1371/journal.pone.0123780},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jayaraman, Fausey, Smith - 2015 - The faces in infant-perspective scenes change over the first year of life.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {5},
pages = {13--15},
title = {{The faces in infant-perspective scenes change over the first year of life}},
volume = {10},
year = {2015}
}
@article{Matthews2015,
author = {Matthews, P G and Lewis, M R and Hubbard, E M},
doi = {10.1177/0956797615617799},
issn = {0956-7976},
journal = {Psychological Science},
keywords = {15,19,21,and a,educational psychology,individual differences,know it,mathematical ability,number comprehension,perception,received 6,revision accepted 10,that a man may,what is a number},
title = {{Individual Differences in Nonsymbolic Ratio Processing Predict Symbolic Math Performance}},
year = {2015}
}
@article{Jiang2015,
abstract = {Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar concep-tual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by pri-or knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of pri-or knowledge while ignoring feedback about the learn-er. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a u-nified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress dur-ing training. In comparison to human education, SPCL is analogous to " instructor-student-collaborative " learn-ing mode, as opposed to " instructor-driven " in CL or " student-driven " in SPL. Empirically, we show that the advantage of SPCL on two tasks. Curriculum learning (Bengio et al. 2009) and self-paced learning (Kumar, Packer, and Koller 2010) have been at-tracting increasing attention in the field of machine learning and artificial intelligence. Both the learning paradigms are inspired by the learning principle underlying the cognitive process of humans and animals, which generally start with learning easier aspects of a task, and then gradually take more complex examples into consideration. The intuition can be explained in analogous to human education in which a pupil is supposed to understand elementary algebra be-fore he or she can learn more advanced algebra topics. This learning paradigm has been empirically demonstrated to be instrumental in avoiding bad local minima and in achieving a better generalization result (Khan, Zhu, and Mutlu 2011; Basu and Christensen 2013; Tang et al. 2012). A curriculum determines a sequence of training samples which essentially corresponds to a list of samples ranked in ascending order of learning difficulty. A major disparity Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. between curriculum learning (CL) and self-paced learning (SPL) lies in the derivation of the curriculum. In CL, the cur-riculum is assumed to be given by an oracle beforehand, and remains fixed thereafter. In SPL, the curriculum is dynami-cally generated by the learner itself, according to what the learner has already learned. The advantage of CL includes the flexibility to incorpo-rate prior knowledge from various sources. Its drawback stems from the fact that the curriculum design is determined independently of the subsequent learning, which may result in inconsistency between the fixed curriculum and the dy-namically learned models. From the optimization perspec-tive, since the learning proceeds iteratively, there is no guar-antee that the predetermined curriculum can even lead to a converged solution. SPL, on the other hand, formulates the learning problem as a concise biconvex problem, where the curriculum design is embedded and jointly learned with model parameters. Therefore, the learned model is consis-tent. However, SPL is limited in incorporating prior knowl-edge into learning, rendering it prone to overfitting. Ignoring prior knowledge is less reasonable when reliable prior infor-mation is available. Since both methods have their advan-tages, it is difficult to judge which one is better in practice. In this paper, we discover the missing link between CL and SPL. We formally propose a unified framework called Self-paced Curriculum Leaning (SPCL). SPCL represents a general learning paradigm that combines the merits from both the CL and SPL. On one hand, it inherits and further generalizes the theory of SPL. On the other hand, SPCL ad-dresses the drawback of SPL by introducing a flexible way to incorporate prior knowledge. This paper also discusses concrete implementations within the proposed framework, which can be useful for solving various problems. This paper offers a compelling insight on the relation-ship between the existing CL and SPL methods. Their re-lation can be intuitively explained in the context of human education, in which SPCL represents an " instructor-student collaborative " learning paradigm, as opposed to " instructor-driven " in CL or " student-driven " in SPL. In SPCL, instruc-tors provide prior knowledge on a weak learning sequence of samples, while leaving students the freedom to decide the actual curriculum according to their learning pace. Since an optimal curriculum for the instructor may not necessarily be optimal for all students, we hypothesize that given reason-able prior knowledge, the curriculum devised by instructors and students together can be expected to be better than the curriculum designed by either part alone. Empirically, we substantiate this hypothesis by demonstrating that the pro-posed method outperforms both CL and SPL on two tasks. The rest of the paper is organized as follows. We first briefly introduce the background knowledge on CL and SPL. Then we propose the model and the algorithm of SPCL. After that, we discuss concrete implementations of SPCL. The experimental results and conclusions are presented in the last two sections.},
author = {Jiang, Lu and Meng, Deyu and Zhao, Qian and Shan, Shiguang and Hauptmann, Alexander G},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2015 - Self-paced Curriculum Learning.pdf:pdf},
isbn = {9781577357025},
issn = {9781577357025},
journal = {Aaai},
pages = {1--30},
title = {{Self-paced Curriculum Learning}},
year = {2015}
}
@article{Hardt2015,
abstract = {We show that any model trained by a stochastic gradient method with few iterations has vanishing generalization error. We prove this by showing the method is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. Our results apply to both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new explanations for why multiple epochs of stochastic gradient descent generalize well in practice. In the nonconvex case, we provide a new interpretation of common practices in neural networks, and provide a formal rationale for stability-promoting mechanisms in training large, deep models. Conceptually, our findings underscore the importance of reducing training time beyond its obvious benefit.},
archivePrefix = {arXiv},
arxivId = {1509.01240},
author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
eprint = {1509.01240},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hardt, Recht, Singer - 2015 - Train faster, generalize better Stability of stochastic gradient descent.pdf:pdf},
journal = {srXiv:1509.01240},
pages = {1--24},
title = {{Train faster, generalize better: Stability of stochastic gradient descent}},
year = {2015}
}
@article{Bouchard2015,
abstract = {Stochastic Gradient Descent (SGD) is one of the most widely used techniques for online optimization in machine learning. In this work, we accelerate SGD by adaptively learning how to sample the most useful training examples at each time step. First, we show that SGD can be used to learn the best possible sampling distribution of an importance sampling estimator. Second, we show that the sampling distribution of a SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient. The resulting algorithm - called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to optimize, as well as a set of parameters to sample learning examples. We show that AW-SGD yields faster convergence in three different applications: (i) image classification with deep features, where the sampling of images depends on their labels, (ii) matrix factorization, where rows and columns are not sampled uniformly, and (iii) reinforcement learning, where the optimized and explore policies are estimated at the same time.},
archivePrefix = {arXiv},
arxivId = {1506.09016},
author = {Bouchard, Guillaume and Trouillon, Th{\'{e}}o and Perez, Julien and Gaidon, Adrien},
eprint = {1506.09016},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouchard et al. - 2015 - Accelerating Stochastic Gradient Descent via Online Learning to Sample.pdf:pdf},
journal = {Arxiv},
number = {iii},
pages = {5},
title = {{Accelerating Stochastic Gradient Descent via Online Learning to Sample}},
year = {2015}
}
@article{Birman2015,
author = {Birman, Daniel and Gardner, Justin L},
doi = {10.1038/nn.4204},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Birman, Gardner - 2015 - Parietal and prefrontal categorical differences.pdf:pdf},
isbn = {1546-1726 (Electronic)1097-6256 (Linking)},
issn = {1097-6256},
journal = {Nature Neuroscience},
number = {1},
pages = {5--7},
pmid = {26713741},
publisher = {Nature Publishing Group},
title = {{Parietal and prefrontal: categorical differences?}},
volume = {19},
year = {2015}
}
@article{Ba2015,
abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7755v2},
author = {Ba, Jimmy Lei and Mnih, Volodymyr and Kavukcuoglu, Koray},
eprint = {arXiv:1412.7755v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ba, Mnih, Kavukcuoglu - 2015 - Multiple Object Recognition With Visual Attention.pdf:pdf},
journal = {Iclr},
pages = {1--10},
title = {{Multiple Object Recognition With Visual Attention}},
year = {2015}
}
@article{Reed2015,
abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.},
archivePrefix = {arXiv},
arxivId = {1511.06279},
author = {Reed, Scott and de Freitas, Nando},
eprint = {1511.06279},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reed, de Freitas - 2015 - Neural Programmer-Interpreters.pdf:pdf},
pages = {1--12},
title = {{Neural Programmer-Interpreters}},
year = {2015}
}
@article{Piech2015,
abstract = {Knowledge tracing—where a machine models the knowledge of a student as they interact with coursework—is a well established problem in computer supported education. Though effectively modeling student knowledge would have high ed-ucational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neu-ral networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and dis-covery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.05908v1},
author = {Piech, Chris and Spencer, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas and Sohl-dickstein, Jascha and Academy, Khan},
eprint = {arXiv:1506.05908v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piech et al. - 2015 - Deep Knowledge Tracing.pdf:pdf},
pages = {1--13},
title = {{Deep Knowledge Tracing}},
year = {2015}
}
@article{Matthews2015,
author = {Matthews, P. G. and Lewis, M. R. and Hubbard, E. M.},
doi = {10.1177/0956797615617799},
issn = {0956-7976},
journal = {Psychological Science},
keywords = {15,19,21,and a,educational psychology,individual differences,know it,mathematical ability,number comprehension,perception,received 6,revision accepted 10,that a man may,what is a number},
title = {{Individual Differences in Nonsymbolic Ratio Processing Predict Symbolic Math Performance}},
year = {2015}
}
@article{Willingham2015,
abstract = {Theories of learning styles suggest that individuals think and learn best in different ways. These are not differences of ability but rather preferences for processing certain types of information or for processing information in certain types of way. If accurate, learning styles theories could have important implications for instruction because student achievement would be a product of the interaction of instruction and the student's style. There is reason to think that people view learning styles theories as broadly accurate, but, in fact, scientific support for these theories is lacking. We suggest that educators' time and energy are better spent on other theories that might aid instruction. Keywords},
author = {Willingham, D. T. and Hughes, E. M. and Dobolyi, D. G.},
doi = {10.1177/0098628315589505},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Willingham, Hughes, Dobolyi - 2015 - The Scientific Status of Learning Styles Theories.pdf:pdf},
issn = {0098-6283},
journal = {Teaching of Psychology},
keywords = {academic achievement,cognitive style,individual differences,learning styles,teaching methods},
number = {3},
pages = {266--271},
title = {{The Scientific Status of Learning Styles Theories}},
volume = {42},
year = {2015}
}
@article{Xu2015,
author = {Xu, Y. and Franconeri, S. L.},
doi = {10.1177/0956797615585002},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Franconeri - 2015 - Capacity for Visual Features in Mental Rotation.pdf:pdf},
isbn = {8474917859},
issn = {0956-7976},
journal = {Psychological Science},
keywords = {10,14,15,16,attention,capacity,creating and transforming representations,eye tracking,mental rotation,of structure is,received 6,revision accepted 4,selection,tracking,visual working memory},
pmid = {26174781},
title = {{Capacity for Visual Features in Mental Rotation}},
year = {2015}
}
@article{Dix2015,
abstract = {This study investigates cognitive resource allocation dependent on fluid and numerical intelligence in arithmetic/algebraic tasks varying in difficulty. Sixty-six 11th grade students participated in a mathematical verification paradigm, while pupil dilation as a measure of resource allocation was collected. Students with high fluid intelligence solved the tasks faster and more accurately than those with average fluid intelligence, as did students with high compared to average numerical intelligence. However, fluid intelligence sped up response times only in students with average but not high numerical intelligence. Further, high fluid but not numerical intelligence led to greater task-related pupil dilation. We assume that fluid intelligence serves as a domain-general resource that helps to tackle problems for which domain-specific knowledge (numerical intelligence) is missing. The allocation of this resource can be measured by pupil dilation.},
author = {Dix, Annika and van der Meer, Elke},
doi = {10.1111/psyp.12367},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dix, van der Meer - 2015 - Arithmetic and algebraic problem solving and resource allocation The distinct impact of fluid and numerical i.pdf:pdf},
issn = {00485772},
journal = {Psychophysiology},
number = {4},
pages = {544--554},
pmid = {25327870},
title = {{Arithmetic and algebraic problem solving and resource allocation: The distinct impact of fluid and numerical intelligence}},
volume = {52},
year = {2015}
}
@article{Shaughnessy2015,
author = {Shaughnessy, J Michael and Burger, William F and Burger, F},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shaughnessy, Burger, Burger - 2015 - Spadework in Geometry Prior to Deduction Secondary school students.pdf:pdf},
number = {6},
pages = {419--428},
title = {{Spadework in Geometry Prior to Deduction Secondary school students}},
volume = {78},
year = {2015}
}
@article{Dauphin2014b,
author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
file = {:home/andrew/Documents/grad/Papers/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf:pdf},
journal = {Neural Information Processing Systems},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
year = {2014}
}
@article{Cleeremans2014,
abstract = {Consciousness remains a mystery-"a phenomenon that people do not know how to think about-yet" (Dennett, , p. 21). Here, I consider how the connectionist perspective on information processing may help us progress toward the goal of understanding the computational principles through which conscious and unconscious processing differ. I begin by delineating the conceptual challenges associated with classical approaches to cognition insofar as understanding unconscious information processing is concerned, and to highlight several contrasting computational principles that are constitutive of the connectionist approach. This leads me to suggest that conscious and unconscious processing are fundamentally connected, that is, rooted in the very same computational principles. I further develop a perspective according to which the brain continuously and unconsciously learns to redescribe its own activity itself based on constant interaction with itself, with the world, and with other minds. The outcome of such interactions is the emergence of internal models that are metacognitive in nature and that function so as to make it possible for an agent to develop a (limited, implicit, practical) understanding of itself. In this light, plasticity and learning are constitutive of what makes us conscious, for it is in virtue of our own experiences with ourselves and with other people that our mental life acquires its subjective character. The connectionist framework continues to be uniquely positioned in the Cognitive Sciences to address the challenge of identifying what one could call the "computational correlates of consciousness" (Mathis {\&} Mozer, ) because it makes it possible to focus on the mechanisms through which information processing takes place.},
author = {Cleeremans, Axel},
doi = {10.1111/cogs.12149},
file = {:home/andrew/Documents/grad/Papers/Cleeremans-2014-Cognitive{\_}Science.pdf:pdf},
isbn = {9780080430768},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Connectionist modeling,Consciousness,Learning,Metacognition},
number = {6},
pages = {1286--1315},
pmid = {25087683},
title = {{Connecting conscious and unconscious processing}},
volume = {38},
year = {2014}
}
@article{Newell2014,
abstract = {{\textless}p{\textgreater}To what extent do we know our own minds when making decisions? Variants of this question have preoccupied researchers in a wide range of domains, from mainstream experimental psychology (cognition, perception, social behavior) to cognitive neuroscience and behavioral economics. A pervasive view places a heavy explanatory burden on an intelligent cognitive unconscious, with many theories assigning causally effective roles to unconscious influences. This article presents a novel framework for evaluating these claims and reviews evidence from three major bodies of research in which unconscious factors have been studied: multiple-cue judgment, deliberation without attention, and decisions under uncertainty. Studies of priming (subliminal and primes-to-behavior) and the role of awareness in movement and perception (e.g., timing of willed actions, blindsight) are also given brief consideration. The review highlights that inadequate procedures for assessing awareness, failures to consider artifactual explanations of “landmark” results, and a tendency to uncritically accept conclusions that fit with our intuitions have all contributed to unconscious influences being ascribed inflated and erroneous explanatory power in theories of decision making. The review concludes by recommending that future research should focus on tasks in which participants' attention is diverted away from the experimenter's hypothesis, rather than the highly reflective tasks that are currently often employed.{\textless}/p{\textgreater}},
author = {Newell, Ben R. and Shanks, David R.},
doi = {10.1017/S0140525X12003214},
file = {:home/andrew/Documents/grad/Papers/unconscious{\_}influences{\_}on{\_}decision{\_}making{\_}a{\_}critical{\_}review.pdf:pdf},
isbn = {0140-525X, 1469-1825},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {awareness,conscious,decision making,deliberation,intuition,judgment,perceptual-motor skills,unconscious},
number = {01},
pages = {1--19},
pmid = {24461214},
title = {{Unconscious influences on decision making: A critical review}},
url = {http://www.journals.cambridge.org/abstract{\_}S0140525X12003214},
volume = {38},
year = {2014}
}
@article{Schapiro2014,
abstract = {The sensory input that we experience is highly patterned, and we are experts at detecting these regularities. Although the extraction of such regularities, or statistical learning (SL), is typically viewed as a cortical process, recent studies have implicated the medial temporal lobe (MTL), including the hippocampus. These studies have employed fMRI, leaving open the possibility that the MTL is involved but not necessary for SL. Here, we examined this issue in a case study of LSJ, a patient with complete bilateral hippocampal loss and broader MTL damage. In Experiments 1 and 2, LSJ and matched control participants were passively exposed to a continuous sequence of shapes, syllables, scenes, or tones containing temporal regularities in the co-occurrence of items. In a subsequent test phase, the control groups exhibited reliable SL in all conditions, successfully discriminating regularities from recombinations of the same items into novel foil sequences. LSJ, however, exhibited no SL, failing to discriminate regularities from foils. Experiment 3 ruled out more general explanations for this failure, such as inattention during exposure or difficulty following test instructions, by showing that LSJ could discriminate which individual items had been exposed. These findings provide converging support for the importance of the MTL in extracting temporal regularities},
archivePrefix = {arXiv},
arxivId = {1511.04103},
author = {Schapiro, Anna and Gregory, Emma and Landau, Barbara and McCloskey, Michael and Turk-Browne, Nicholas},
doi = {10.1162/jocn},
eprint = {1511.04103},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schapiro et al. - 2014 - The Necessity of the Medial Temporal Lobe for Statistical Learning.pdf:pdf},
isbn = {9780192880512},
issn = {09528229},
journal = {Journal of Cognitive Neuroscience},
number = {8},
pages = {1736--1747},
pmid = {23647519},
title = {{The Necessity of the Medial Temporal Lobe for Statistical Learning}},
volume = {26},
year = {2014}
}
@article{Simonsohn2014,
abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that "work," readers must ask, "Are these effects true, or do they merely reflect selective reporting?" We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps {\textless} .05). Because only true effects are expected to generate right-skewed p-curves-containing more low (.01s) than high (.04s) significant p values--only right-skewed p--curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses.},
author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
doi = {10.1037/a0033242},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonsohn, Nelson, Simmons - 2014 - P-curve A key to the file-drawer.pdf:pdf},
isbn = {0096-3445},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
keywords = {10,1037,a0033242,doi,dx,false-positive psychology,http,hypothesis testing,org,p -hacking,publication bias,selective reporting,supp,supplemental materials},
number = {2},
pages = {534--547},
pmid = {23855496},
title = {{P-curve: A key to the file-drawer.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0033242},
volume = {143},
year = {2014}
}
@article{Yosinski2014,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\{}{\%}{\}} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
isbn = {9781467369640},
journal = {arXiv},
title = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
year = {2014}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
isbn = {0950-5849},
issn = {09505849},
journal = {arXiv},
pages = {1--10},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2014}
}
@article{Yamins2014,
abstract = {The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model's categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model's intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization--applied in a biologically appropriate model class--can be used to build quantitative predictive models of neural processing.},
archivePrefix = {arXiv},
arxivId = {0706.1062v1},
author = {Yamins, Daniel L K and Hong, Ha and Cadieu, Charles F and Solomon, Ethan A and Seibert, Darren and DiCarlo, James J},
doi = {10.1073/pnas.1403112111},
eprint = {0706.1062v1},
isbn = {0902262106},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {array electrophysiology,computational neuroscience,computer vision},
number = {23},
pages = {8619--8624},
pmid = {24812127},
title = {{Performance-optimized hierarchical models predict neural responses in higher visual cortex.}},
volume = {111},
year = {2014}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {Computer Vision–ECCV 2014},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and Understanding Convolutional Networks arXiv:1311.2901v3 [cs.CV] 28 Nov 2013}},
volume = {8689},
year = {2014}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ij and Pouget-Abadie, J and Mirza, Mehdi},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {arXiv:1406.2661v1},
isbn = {1406.2661},
issn = {10495258},
journal = {arXiv},
pages = {1--9},
pmid = {15040217},
title = {{Generative Adversarial Networks}},
year = {2014}
}
@article{Zaremba2014a,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
doi = {ng},
eprint = {1409.2329},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaremba, Sutskever, Vinyals - 2014 - Recurrent Neural Network Regularization.pdf:pdf},
isbn = {078036404X},
issn = {0157244X},
journal = {arXiv},
pages = {1--8},
pmid = {23259955},
title = {{Recurrent Neural Network Regularization}},
year = {2014}
}
@article{Brunsdon2014,
author = {Brunsdon, Victoria E A and Happ{\'{e}}, Francesca},
doi = {10.1177/1362361313499456},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunsdon, Happ{\'{e}} - 2014 - Exploring the ‘fractionation' of autism at the cognitive level.pdf:pdf},
isbn = {1362361313},
journal = {Autism},
keywords = {autism spectrum disorder,central coherence,cognitive theories,executive function,fractionable triad,theory of mind},
number = {1},
pages = {17--30},
title = {{Exploring the ‘fractionation' of autism at the cognitive level}},
volume = {18},
year = {2014}
}
@article{Dauphin2014a,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
eprint = {1406.2572},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization(2).pdf:pdf},
isbn = {1406.2572},
issn = {10495258},
journal = {Neural Information Processing Systems},
pages = {1--9},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
year = {2014}
}
@article{Shenhav2014,
abstract = {Nature Neuroscience, (2014). doi:10.1038/nn.3771},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Shenhav, Amitai and Straccia, Mark A and Cohen, Jonathan D and Botvinick, Matthew M},
doi = {10.1038/nn.3771},
eprint = {NIHMS150003},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shenhav et al. - 2014 - Anterior cingulate engagement in a foraging context reflects choice difficulty, not foraging value.pdf:pdf},
isbn = {6176321972},
issn = {1097-6256},
journal = {Nature Neuroscience},
number = {9},
pages = {1249--1254},
pmid = {25064851},
publisher = {Nature Publishing Group},
title = {{Anterior cingulate engagement in a foraging context reflects choice difficulty, not foraging value}},
volume = {17},
year = {2014}
}
@article{Levy2014,
abstract = {Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.'s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.},
author = {Levy, Omer and Goldberg, Yoav},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy, Goldberg - 2014 - Linguistic regularities in sparse and explicit word representations.pdf:pdf},
isbn = {9781941643020},
journal = {CoNLL},
pages = {171--180},
title = {{Linguistic regularities in sparse and explicit word representations}},
year = {2014}
}
@article{He2014,
author = {He, Kaiming},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He - 2014 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
title = {{Delving Deep into Rectifiers : Surpassing Human-Level Performance on ImageNet Classification}},
year = {2014}
}
@misc{Rust2014,
abstract = {(from the chapter) Many of our everyday perceptual and cognitive tasks require our brains to transform information from implicit representations in which task-relevant information exists but in a format that is difficult to extract, into explicit representations in which this information is accessible. For example, determining the identities of objects that are currently in view across naturally occurring variation, such as changes in an object's position, requires our brains to reformat the pattern of light-based representations encoded by our photoreceptors into representations that explicitly reflect object identity. The brain faces similar challenges for other perceptual tasks, such as identifying words spoken by different voices, as well as more cognitive challenges, such as determining whether a chair belongs to the category of furniture. Insight into these challenges, and the brain's solutions, can be understood using geometrical, population-based coding approaches. Once formulated, these population-based descriptions can be linked to the single- and multi-neuron mechanisms that support a successful task solution as well as provide important insights into the computations that the brain uses to process information. (PsycINFO Database Record (c) 2015 APA, all rights reserved)},
author = {Rust, Nicole C},
booktitle = {The cognitive neurosciences (5th ed.).},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rust - 2014 - Population-based representations From implicit to explicit.pdf:pdf},
isbn = {978-0-262-02777-9 (Hardcover)},
keywords = {*Age Differences,*Brain,*Cognitive Processes,*Neurons,*Task Analysis},
pages = {337--348},
title = {{Population-based representations: From implicit to explicit.}},
year = {2014}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {1411.1792},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27 (Proceedings of NIPS)},
title = {{How transferable are features in deep neural networks?}},
volume = {27},
year = {2014}
}
@article{Sristava2014,
author = {Sristava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sristava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Rezende2014,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4082v3},
author = {Rezende, Danilo J and Mohamed, Shakir and Wierstra, Daan},
eprint = {arXiv:1401.4082v3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rezende, Mohamed, Wierstra - 2014 - Stochastic Backpropagation and Approximate Inference.pdf:pdf},
journal = {arXiv preprint},
title = {{Stochastic Backpropagation and Approximate Inference}},
year = {2014}
}
@article{Belenky2014,
abstract = {Research in both cognitive and educational psychology has explored the effect of different types of external knowledge representations (e.g., manipulatives, graphical/pictorial representations, texts) on a variety of important outcome measures. We place this large and multifaceted research literature into an organizing framework, classifying three categories of external knowledge representations along a dimension of groundedness: (1) idealized, (2) grounded and including only relevant features, and (3) grounded and including irrelevant features. This organizing framework allows us to focus on the implications of these characteristics of external knowledge representations on three important educational outcomes: learning and immediate performance using the target knowledge, the degree to which that knowledge can transfer flexibly, and the interest engendered by the learning materials. We illustrate the frame- work by mapping a wide body of research from educational and cognitive psychology onto its dimensions. This framework can aid educators by clearly statingwhat the research literature says about these characteristics of external knowledge representations and how they activate and support the construction of internal knowledge representations. In particular, it will speak to how to best structure instruction using external knowledge representations with different characteris- tics, depending on the learning objective. Researcherswill benefit from the analysis of the current state of knowledge and by the description of what open questions still remain.},
author = {Belenky, Daniel M. and Schalk, Lennart},
doi = {10.1007/s10648-014-9251-9},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Belenky, Schalk - 2014 - The Effects of Idealized and Grounded Materials on Learning, Transfer, and Interest An Organizing Framework for.pdf:pdf},
isbn = {1040-726X$\backslash$n1573-336X},
issn = {1040726X},
journal = {Educational Psychology Review},
keywords = {Grounded,Idealized,Interest,Learning,Learning materials,Transfer},
number = {1},
pages = {27--50},
title = {{The Effects of Idealized and Grounded Materials on Learning, Transfer, and Interest: An Organizing Framework for Categorizing External Knowledge Representations}},
volume = {26},
year = {2014}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global Vectors for Word Representation.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
pages = {1532--1543},
pmid = {1710995},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@article{OReilly2014,
abstract = {This paper reviews the fate of the central ideas behind the complementary learning systems (CLS) framework as originally articulated in McClelland, McNaughton, and O'Reilly (1995). This framework explains why the brain requires two differentially specialized learning and memory systems, and it nicely specifies their central properties (i.e., the hippocampus as a sparse, pattern-separated system for rapidly learning episodic memories, and the neocortex as a distributed, overlapping system for gradually integrating across episodes to extract latent semantic structure). We review the application of the CLS framework to a range of important topics, including the following: the basic neural processes of hippocampal memory encoding and recall, conjunctive encoding, human recognition memory, consolidation of initial hippocampal learning in cortex, dynamic modulation of encoding versus recall, and the synergistic interactions between hippocampus and neocortex. Overall, the CLS framework remains a vital theoretical force in the field, with the empirical data over the past 15 years generally confirming its key principles.},
author = {O'Reilly, Randall C. and Bhattacharyya, Rajan and Howard, Michael D. and Ketz, Nicholas},
doi = {10.1111/j.1551-6709.2011.01214.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Reilly et al. - 2014 - Complementary learning systems.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Consolidation,Hippocampus,Learning,Memory,Neocortex,Neural network models},
number = {6},
pages = {1229--1248},
pmid = {22141588},
title = {{Complementary learning systems}},
volume = {38},
year = {2014}
}
@article{Markant2014a,
abstract = {People can test hypotheses through either selection or reception. In a selection task, the learner actively chooses observations to test his or her beliefs, whereas in reception tasks data are passively encountered. People routinely use both forms of testing in everyday life, but the critical psychological differences between selection and reception learning remain poorly understood. One hypothesis is that selection learning improves learning performance by enhancing generic cognitive processes related to motivation, attention, and engagement. Alternatively, we suggest that differences between these 2 learning modes derives from a hypothesis-dependent sampling bias that is introduced when a person collects data to test his or her own individual hypothesis. Drawing on influential models of sequential hypothesis-testing behavior, we show that such a bias (a) can lead to the collection of data that facilitates learning compared with reception learning and (b) can be more effective than observing the selections of another person. We then report a novel experiment based on a popular category learning paradigm that compares reception and selection learning. We additionally compare selection learners to a set of “yoked” participants who viewed the exact same sequence of observations under reception conditions. The results revealed systematic differences in performance that depended on the learner's role in collecting information and the abstract structure of the problem.},
author = {Markant, Douglas B. and Gureckis, Todd M.},
doi = {10.1037/a0032108},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Markant, Gureckis - 2014 - Is it better to select or to receive Learning via active and passive hypothesis testing(2).pdf:pdf},
isbn = {1939-2222 (Electronic)$\backslash$n0022-1015 (Linking)},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
keywords = {bayesian modeling,category learning,dependent sampling bias,hypothesis testing,hypothesis-,self-directed learning},
number = {1},
pages = {94--122},
pmid = {23527948},
title = {{Is it better to select or to receive? Learning via active and passive hypothesis testing}},
volume = {143},
year = {2014}
}
@article{Ganesh2014,
abstract = {Background: Cataracts are a major cause of childhood blindness globally. Although surgically treatable, it is unclear whether children would benefit from such interventions beyond the first few years of life, which are believed to constitute 'critical' periods for visual development. Aims To study visual acuity outcomes after late treatment of early-onset cataracts and also to determine whether there are longitudinal changes in postoperative acuity. Methods: We identified 53 children with dense cataracts with an onset within the first half-year after birth through a survey of over 20 000 rural children in India. All had accompanying nystagmus and were older than 8 years of age at the time of treatment. They underwent bilateral cataract surgery and intraocular lens implantation. We then assessed their best-corrected visual acuity 6 weeks and 6 months after surgery. Results: 48 children from the pool of 53 showed improvement in their visual acuity after surgery. Our longitudinal assessments demonstrated further improvements in visual acuity for the majority of these children proceeding from the 6-week to 6-month assessment. Interestingly, older children in our subject pool did not differ significantly from the younger ones in the extent of improvement they exhibit. Conclusions: and relevance Our results demonstrate that not only can significant vision be acquired until late in childhood, but that neural processes underlying even basic aspects of vision like resolution acuity remain malleable until at least adolescence. These data argue for the provision of cataract treatment to all children, irrespective of their age.},
author = {Ganesh, S and Arora, P and Sethi, S and Gandhi, T K and Kalia, A and Chatterjee, G and Sinha, P},
doi = {10.1136/bjophthalmol-2013-304475},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganesh et al. - 2014 - Results of late surgical intervention in children with early-onset bilateral cataracts.pdf:pdf},
issn = {0007-1161},
journal = {British Journal of Ophthalmology},
keywords = {age,article,best corrected visual acuity,cataract extraction,cataract/su [Surgery],child,disease association,early onset bilateral cataract/su [Surgery],female,follow up,human,lens implantation,longitudinal study,major clinical study,male,nystagmus,onset age,optical low vision aid,outcome assessment,prospective study,therapy delay,time to treatment,visual acuity,visual impairment},
number = {10},
pages = {1424--1428},
pmid = {2014737030},
title = {{Results of late surgical intervention in children with early-onset bilateral cataracts}},
volume = {98},
year = {2014}
}
@article{Afraz2014,
abstract = {Invariant visual object recognition and the underlying neural representations are fundamental to higher-level human cognition. To understand these neural underpinnings, we combine human and monkey psychophysics, large-scale neurophysiology, neural perturbation methods, and computational modeling to construct falsifiable, predictive models that aim to fully account for the neural encoding and decoding processes that underlie visual object recognition. A predictive encoding model must minimally describe the transformation of the retinal image to population patterns of neural activity along the entire cortical ventral stream of visual processing and must accurately predict the responses to any retinal image. A predictive decoding model must minimally describe the transformation from those population patterns of neural activity to observed object recognition behavior (i.e., subject reports), and, given that population pattern of activity, it must accurately predict behavior for any object recognition task. To date, we have focused on core object recognition-a remarkable behavior that is accomplished with image viewing durations of {\textless}200 msec. Our work thus far reveals that the neural encoding process is reasonably well explained by a largely feed-forward, highly complex, multistaged nonlinear neural network-the current best neuronal simulation models predict approximately one-half of the relevant neuronal response variance across the highest levels of the ventral stream (areas V4 and IT). Remarkably, however, the decoding process from IT to behavior for all object recognition tasks tested thus far is very accurately predicted by simple direct linear conversion of the inferior temporal neural population state to behavior choice. We have recently examined the behavioral consequences of direct suppression of IT neural activity using pharmacological and optogenetic methods and find them to be well-explained by the same linear decoding model.},
author = {Afraz, Arash and Yamins, Daniel L K and DiCarlo, James J},
doi = {10.1101/sqb.2014.79.024729},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Afraz, Yamins, DiCarlo - 2014 - Neural Mechanisms Underlying Visual Object Recognition.pdf:pdf},
issn = {1943-4456 (Electronic)},
journal = {Cold Spring Harbor Symposia on Quantitative Biology},
pages = {99--107},
pmid = {26092883},
title = {{Neural Mechanisms Underlying Visual Object Recognition.}},
volume = {79},
year = {2014}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ij and Pouget-Abadie, J and Mirza, Mehdi},
eprint = {arXiv:1406.2661v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodfellow, Pouget-Abadie, Mirza - 2014 - Generative Adversarial Networks.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
title = {{Generative Adversarial Networks}},
year = {2014}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--13},
title = {{Adam: A Method for Stochastic Optimization}},
year = {2014}
}
@article{Pascanu2014,
abstract = {We study the complexity of functions computable by deep feedforward neural networks with piece-wise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer's input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1869v1},
author = {Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {arXiv:1402.1869v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu, Cho, Bengio - 2014 - On the Number of Linear Regions of Deep Neural Networks ´.pdf:pdf},
issn = {10495258},
journal = {Nips},
keywords = {deep learning,input space partition,maxout,neural network,rectifier},
pages = {1--12},
title = {{On the Number of Linear Regions of Deep Neural Networks ´}},
year = {2014}
}
@article{Gu2014,
abstract = {—In the last few years, deep learning has lead to very good performance on a variety of problems, such as object recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Due to the lack of training data and computing power in early days, it is hard to train a large high-capacity convolutional neural network without overfitting. Recently, with the rapid growth of data size and the increasing power of graphics processor unit, many researchers have improved the convolutional neural networks and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. Besides, we also introduce some applications of convolutional neural networks in computer vision.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Krogh, Anders and Vedelsby, Jesper and Ukil, a. and Bernasconi, J. and Fukumizu, K and Poland, Jan and Zell, Andreas and Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and Papamakarios, George and Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram and Ba, Jimmy and Caruana, Rich and Bianchini, Monica and Scarselli, Franco and M., Bianchini and F., Scarselli and Bianchini, Monica and Scarselli, Franco and Larochelle, Hugo and Larochelle, Hugo and Bengio, Yoshua and Bengio, Yoshua and Lourador, Jerome and Lourador, Jerome and Lamblin, Pascal and Lamblin, Pascal and Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod and Welbl, Johannes},
doi = {Doi 10.1109/Tsmcc.2012.2220963},
eprint = {1503.02531},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2014 - Exploring Strategies for Training Deep Neural Networks.pdf:pdf},
isbn = {9783319117522},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Deep learning,Index Terms—Convolutional Neural Network,active learning,fisher,information matrix,multilayer perceptron,pruning},
number = {1},
pages = {1--16},
pmid = {18249735},
title = {{Exploring Strategies for Training Deep Neural Networks}},
volume = {11},
year = {2014}
}
@article{Dauphin2014,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
eprint = {1406.2572},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.pdf:pdf},
isbn = {1406.2572},
issn = {10495258},
journal = {arXiv},
pages = {1--14},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
year = {2014}
}
@article{Zaremba2014,
abstract = {Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99{\%} accuracy.},
archivePrefix = {arXiv},
arxivId = {1410.4615},
author = {Zaremba, Wojciech and Sutskever, Ilya},
doi = {10.1016/S0893-6080(96)00073-1},
eprint = {1410.4615},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaremba, Sutskever - 2014 - Learning to Execute.pdf:pdf},
isbn = {1410.4615},
issn = {08936080},
journal = {Iclr},
pages = {1--25},
title = {{Learning to Execute}},
year = {2014}
}
@article{Mnih2014,
abstract = {Nice paper that shifts an attentional window to recognize digits.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.6247v1},
author = {Mnih, Volodymyr and Hess, N. and Graves, Alex and Kavukcuoglu, Koray},
doi = {ng},
eprint = {arXiv:1406.6247v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2014 - Recurrent Models of Visual Attention.pdf:pdf},
journal = {Nips},
keywords = {deep learning,digit recognition,neural network},
pages = {arXiv:1406.6247},
title = {{Recurrent Models of Visual Attention}},
year = {2014}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory re-sources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demon-strate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {arXiv:1410.5401v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
journal = {Arxiv},
pages = {1--26},
title = {{Neural Turing Machines}},
year = {2014}
}
@article{Fyfe2014,
abstract = {A longstanding debate concerns the use of concrete versus abstract instructional materials, particularly in domains such as mathematics and science. Although decades of research have focused on the advantages and disadvantages of concrete and abstract materials considered independently, we argue for an approach that moves beyond this dichotomy and combines their advantages. Specifically, we recommend beginning with concretematerials and then explicitly and gradually fading to the more abstract. Theoretical benefits of this “con- creteness fading” technique for mathematics and science instruction include (1) helping learners interpret ambiguous or opaque abstract symbols in terms of well-understood concrete objects, (2) providing embodied perceptual and physical experiences that can ground abstract thinking, (3) enabling learners to build up a store of memorable images that can be used when abstract symbols lose meaning, and (4) guiding learners to strip away extraneous concrete properties and distill the generic, generalizable properties. In these ways, concreteness fading provides advantages that go beyond the sum of the benefits of concrete and abstract materials},
author = {Fyfe, Emily R and McNeil, Nicole M and Son, Ji Y and Goldstone, Robert L},
doi = {10.1007/s10648-014-9249-3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fyfe et al. - 2014 - Concreteness Fading in Mathematics and Science Instruction A Systematic Review.pdf:pdf},
issn = {1040726X},
journal = {Educational Psychology Review},
keywords = {Abstract symbols,Concrete manipulatives,Learning and instruction},
number = {1},
pages = {9--25},
title = {{Concreteness Fading in Mathematics and Science Instruction: A Systematic Review}},
volume = {26},
year = {2014}
}
@article{Farah2014,
author = {Farah, Martha J.},
doi = {10.1002/hast.295},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farah - 2014 - Brain Images, Babies, and Bathwater iCritiquing Critiques of Functional Neuroimagingi.pdf:pdf},
issn = {00930334},
journal = {Hastings Center Report},
number = {s2},
pages = {S19--S30},
title = {{Brain Images, Babies, and Bathwater: {\textless}i{\textgreater}Critiquing Critiques of Functional Neuroimaging{\textless}/i{\textgreater}}},
volume = {44},
year = {2014}
}
@article{Markant2014,
abstract = {People can test hypotheses through either selection or reception. In a selection task, the learner actively chooses observations to test his or her beliefs, whereas in reception tasks data are passively encountered. People routinely use both forms of testing in everyday life, but the critical psychological differences between selection and reception learning remain poorly understood. One hypothesis is that selection learning improves learning performance by enhancing generic cognitive processes related to motivation, attention, and engagement. Alternatively, we suggest that differences between these 2 learning modes derives from a hypothesis-dependent sampling bias that is introduced when a person collects data to test his or her own individual hypothesis. Drawing on influential models of sequential hypothesis-testing behavior, we show that such a bias (a) can lead to the collection of data that facilitates learning compared with reception learning and (b) can be more effective than observing the selections of another person. We then report a novel experiment based on a popular category learning paradigm that compares reception and selection learning. We additionally compare selection learners to a set of "yoked" participants who viewed the exact same sequence of observations under reception conditions. The results revealed systematic differences in performance that depended on the learner's role in collecting information and the abstract structure of the problem.},
author = {Markant, Douglas B and Gureckis, Todd M},
doi = {10.1037/a0032108},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Markant, Gureckis - 2014 - Is it better to select or to receive Learning via active and passive hypothesis testing.pdf:pdf},
isbn = {1939-2222 (Electronic)$\backslash$n0022-1015 (Linking)},
issn = {1939-2222},
journal = {Journal of experimental psychology. General},
keywords = {bayesian modeling,category learning,dependent sampling bias,hypothesis testing,hypothesis-,self-directed learning},
number = {1},
pages = {94--122},
pmid = {23527948},
title = {{Is it better to select or to receive? Learning via active and passive hypothesis testing.}},
volume = {143},
year = {2014}
}
@article{Fyfe2014,
abstract = {A longstanding debate concerns the use of concrete versus abstract instructional materials, particularly in domains such as mathematics and science. Although decades of research have focused on the advantages and disadvantages of concrete and abstract materials considered independently, we argue for an approach that moves beyond this dichotomy and combines their advantages. Specifically, we recommend beginning with concretematerials and then explicitly and gradually fading to the more abstract. Theoretical benefits of this “con- creteness fading” technique for mathematics and science instruction include (1) helping learners interpret ambiguous or opaque abstract symbols in terms of well-understood concrete objects, (2) providing embodied perceptual and physical experiences that can ground abstract thinking, (3) enabling learners to build up a store of memorable images that can be used when abstract symbols lose meaning, and (4) guiding learners to strip away extraneous concrete properties and distill the generic, generalizable properties. In these ways, concreteness fading provides advantages that go beyond the sum of the benefits of concrete and abstract materials},
author = {Fyfe, Emily R. and McNeil, Nicole M. and Son, Ji Y. and Goldstone, Robert L.},
doi = {10.1007/s10648-014-9249-3},
issn = {1040726X},
journal = {Educational Psychology Review},
keywords = {Abstract symbols,Concrete manipulatives,Learning and instruction},
number = {1},
pages = {9--25},
title = {{Concreteness Fading in Mathematics and Science Instruction: A Systematic Review}},
volume = {26},
year = {2014}
}
@article{Anderson2014,
abstract = {Multi-voxel pattern recognition techniques combined with Hidden Markov models can be used to discover the mental states that people go through in performing a task. The combined method identifies both the mental states and how their durations vary with experimental conditions. We apply this method to a task where participants solve novel mathematical problems. We identify four states in the solution of these problems: Encoding, Planning, Solving, and Respond. The method allows us to interpret what participants are doing on individual problem-solving trials. The duration of the planning state varies on a trial-to-trial basis with novelty of the problem. The duration of solution stage similarly varies with the amount of computation needed to produce a solution once a plan is devised. The response stage similarly varies with the complexity of the answer produced. In addition, we identified a number of effects that ran counter to a prior model of the task. Thus, we were able to decompose the overall problem-solving time into estimates of its components and in way that serves to guide theory.},
author = {Anderson, John R. and Fincham, Jon M.},
doi = {10.1111/cogs.12068},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson, Fincham - 2014 - Discovering the sequential structure of thought.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Cognitive models,Hidden markov models,Multi-voxel pattern recognition,Problem solving},
number = {2},
pages = {322--352},
pmid = {23941168},
title = {{Discovering the sequential structure of thought}},
volume = {38},
year = {2014}
}
@article{Nguyen2014,
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
eprint = {1412.1897},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Yosinski, Clune - 2014 - Deep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images.pdf:pdf},
keywords = {CNN},
mendeley-tags = {CNN},
month = {dec},
title = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
year = {2014}
}
@article{Birnbaum2013,
abstract = {Kornell and Bjork (Psychological science 19:585-592, 2008) found that interleaving exemplars of different categories enhanced inductive learning of the concepts based on those exemplars. They hypothesized that the benefit of mixing exemplars from different categories is that doing so highlights differences between the categories. Kang and Pashler (Applied cognitive psychology 26:97-103, 2012) obtained results consistent with this discriminative-contrast hypothesis: Interleaving enhanced inductive learning, but temporal spacing, which does not highlight category differences, did not. We further tested the discriminative-contrast hypothesis by examining the effects of interleaving and spacing, as well as their combined effects. In three experiments, using photographs of butterflies and birds as the stimuli, temporal spacing was harmful when it interrupted the juxtaposition of interleaved categories, even when total spacing was held constant, supporting the discriminative-contrast hypothesis. Temporal spacing also had value, however, when it did not interrupt discrimination processing.},
author = {Birnbaum, Monica S. and Kornell, Nate and Bjork, Elizabeth Ligon and Bjork, Robert A.},
doi = {10.3758/s13421-012-0272-7},
file = {:home/andrew/Documents/grad/Papers/10.3758{\%}2Fs13421-012-0272-7.pdf:pdf},
isbn = {0090-502X},
issn = {0090502X},
journal = {Memory and Cognition},
keywords = {Categorization,Induction,Interleaving,Metacognition,Spacing},
number = {3},
pages = {392--402},
pmid = {23138567},
title = {{Why interleaving enhances inductive learning: The roles of discrimination and retrieval}},
volume = {41},
year = {2013}
}
@article{Carpenter2013,
abstract = {Many studies have shown that students learn better when they are given repeated exposures to different concepts in a way that is shuffled or interleaved, rather than blocked (e.g., Rohrer Educational Psychology Review, 24, 355-367, 2012). The present study explored the effects of interleaving versus blocking on learning French pronunciations. Native English speakers learned several French words that conformed to specific pronunciation rules (e.g., the long "o" sound formed by the letter combination "eau," as in bateau), and these rules were presented either in blocked fashion (bateau, carreau, fardeau . . . mouton, genou, verrou . . . tandis, verglas, admis) or in interleaved fashion (bateau, mouton, tandis, carreau, genou, verglas . . .). Blocking versus interleaving was manipulated within subjects (Experiments 1-3) or between subjects (Experiment 4), and participants' pronunciation proficiency was later tested through multiple-choice tests (Experiments 1, 2, and 4) or a recall test (Experiment 3). In all experiments, blocking benefited the learning of pronunciations more than did interleaving, and this was true whether participants learned only 4 words per rule (Experiments 1-3) or 15 words per rule (Experiment 4). Theoretical implications of these findings are discussed.},
author = {Carpenter, Shana K. and Mueller, Frank E.},
doi = {10.3758/s13421-012-0291-4},
file = {:home/andrew/Documents/grad/Papers/10.3758{\%}2Fs13421-012-0291-4.pdf:pdf},
isbn = {1342101202914},
issn = {0090502X},
journal = {Memory and Cognition},
keywords = {Discriminative contrast,Interleaving,Pronunciation learning},
number = {5},
pages = {671--682},
pmid = {23322358},
title = {{The effects of interleaving versus blocking on foreign language pronunciation learning}},
volume = {41},
year = {2013}
}
@article{Saxe2013a,
author = {Saxe, Andrew M and Mcclelland, James L and Ganguli, Surya},
file = {:home/andrew/Documents/grad/Papers/Saxe, McClelland, Ganguli - 2013 - Learning hierarchical category structure in deep neural networks.pdf:pdf},
journal = {Proceedings of the 35th annual meeting of the Cognitive Science Society},
keywords = {hierarchical generative models,learning dynamics,neural networks,semantic cognition},
pages = {1271--1276},
title = {{Learning hierarchical category structure in deep neural networks}},
year = {2013}
}
@article{Stickgold2013,
abstract = {The brain does not retain all the information it encodes in a day. Much is forgotten, and of those memories retained, their subsequent evolution can follow any of a number of pathways. Emerging data makes clear that sleep is a compelling candidate for performing many of these operations. But how does the sleeping brain know which information to preserve and which to forget? What should sleep do with that information it chooses to keep? For information that is retained, sleep can integrate it into existing memory networks, look for common patterns and distill overarching rules, or simply stabilize and strengthen the memory exactly as it was learned. We suggest such 'memory triage' lies at the heart of a sleep-dependent memory processing system that selects new information, in a discriminatory manner, and assimilates it into the brain's vast armamentarium of evolving knowledge, helping guide each organism through its own, unique life.},
author = {Stickgold, Robert and Walker, Matthew P},
doi = {10.1038/nn.3303},
file = {:home/andrew/Documents/grad/Papers/Walker{\_}RV{\_}NatNeuro{\_}2013.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {1097-6256},
journal = {Nature Neuroscience},
number = {2},
pages = {139--145},
pmid = {23354387},
title = {{Sleep-dependent memory triage: evolving generalization through selective processing}},
url = {http://www.nature.com/doifinder/10.1038/nn.3303},
volume = {16},
year = {2013}
}
@article{Schapiro2013,
author = {Schapiro, Anna C and Rogers, Timothy T and Cordova, Natalia I and Turk-, Nicholas B and Botvinick, Matthew M},
doi = {10.1038/nn.3331.Neural},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schapiro et al. - 2013 - Neural representations of events arise from temporal community structure.pdf:pdf},
journal = {Nature Neuroscience},
number = {4},
pages = {486--492},
title = {{Neural representations of events arise from temporal community structure}},
volume = {16},
year = {2013}
}
@article{McClelland2013a,
abstract = {The complementary learning systems theory of the roles of hippocampus and neocortex (McClelland, McNaughton, {\&} O'Reilly, 1995) holds that the rapid integration of arbitrary new information into neocortical structures is avoided to prevent catastrophic interference with structured knowledge representations stored in synaptic connections among neocortical neurons. Recent studies (Tse et al., 2007, 2011) showed that neocortical circuits can rapidly acquire new associations that are consistent with prior knowledge. The findings challenge the complementary learning systems theory as previously presented. However, new simulations extending those reported in McClelland et al. (1995) show that new information that is consistent with knowledge previously acquired by a putatively cortexlike artificial neural network can be learned rapidly and without interfering with existing knowledge; it is when inconsistent new knowledge is acquired quickly that catastrophic interference ensues. Several important features of the findings of Tse et al. (2007, 2011) are captured in these simulations, indicating that the neural network model used in McClelland et al. has characteristics in common with neocortical learning mechanisms. An additional simulation generalizes beyond the network model previously used, showing how the rate of change of cortical connections can depend on prior knowledge in an arguably more biologically plausible network architecture. In sum, the findings of Tse et al. are fully consistent with the idea that hippocampus and neocortex are complementary learning systems. Taken together, these findings and the simulations reported here advance our knowledge by bringing out the role of consistency of new experience with existing knowledge and demonstrating that the rate of change of connections in real and artificial neural networks can be strongly prior-knowledge dependent.},
author = {McClelland, James L.},
doi = {10.1037/a0033812},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland - 2013 - Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems(2).pdf:pdf},
isbn = {6507251232},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
keywords = {1932,1979,2007,2011,bartlett,bransford,consolidation,hippocampus,in two recent articles,it has long been,known that it is,learning,memory,new,relatively easy to learn,schemas,things that are consistent,tse et al,with prior knowledge},
number = {4},
pages = {1190--1210},
pmid = {23978185},
title = {{Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems theory.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0033812},
volume = {142},
year = {2013}
}
@article{Shenhav2013,
author = {Shenhav, Amitai and Botvinick, Matthew M and Cohen, Jonathan D},
doi = {10.1016/j.neuron.2013.07.007},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shenhav, Botvinick, Cohen - 2013 - The Expected Value of Control An Integrative Theory of Anterior Cingulate Cortex Function.pdf:pdf},
issn = {0896-6273},
journal = {Neuron},
number = {2},
pages = {217--240},
publisher = {Elsevier Inc.},
title = {{The Expected Value of Control: An Integrative Theory of Anterior Cingulate Cortex Function}},
volume = {79},
year = {2013}
}
@article{Melby-Lervag2013,
author = {Melby-Lerv{\aa}g, Monica and Hulme, Charles},
doi = {10.1037/a0028228},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Melby-Lerv{\aa}g, Hulme - 2013 - Is working memory training effective A meta-analytic review.pdf:pdf},
isbn = {1939-0599 (Electronic)0012-1649 (Linking)},
issn = {1939-0599},
journal = {Developmental Psychology},
keywords = {adhd,at least,attention,constructs in cognitive psychology,from links between measures,in part,learning disabilities,of the most influential,of working memory capacity,theoretical,this influence derives,working memory is one,working memory training},
number = {2},
pages = {270--291},
pmid = {22612437},
title = {{Is working memory training effective? A meta-analytic review.}},
volume = {49},
year = {2013}
}
@article{Evans2013,
abstract = {Dual-process and dual-system theories in both cognitive and social psychology have been subjected to a number of recently published criticisms. However, they have been attacked as a category, incorrectly assuming there is a generic version that applies to all. We identify and respond to 5 main lines of argument made by such critics. We agree that some of these arguments have force against some of the theories in the literature but believe them to be overstated. We argue that the dual-processing distinction is supported by much recent evidence in cognitive science. Our preferred theoretical approach is one in which rapid autonomous processes (Type 1) are assumed to yield default responses unless intervened on by distinctive higher order reasoning processes (Type 2). What defines the difference is that Type 2 processing supports hypothetical thinking and load heavily on working memory.},
author = {Evans, Jonathan and Stanovich, K E},
doi = {10.1177/1745691612460685},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans, Stanovich - 2013 - Dual-process theories of higher cognition Advancing the debate.pdf:pdf},
isbn = {1745-6916$\backslash$n1745-6924},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
keywords = {Dual process theory,Dual systems,Individual differences,Rationality,Working memory},
number = {3},
pages = {223--241},
pmid = {26172965},
title = {{Dual-process theories of higher cognition: Advancing the debate}},
volume = {8},
year = {2013}
}
@inproceedings{Emruli2013,
abstract = {Analogy-making is a key function of human cognition. Therefore, the development of computational models of analogy that automatically learn from examples can lead to significant advances in cognitive systems. Analogies require complex, relational representations of learned structures, which is challenging for both symbolic and neurally inspired models. Vector symbolic architectures (VSAs) are a class of connectionist models for the representation and manipulation of compositional structures, which can be used to model analogy. We study a novel VSA network for the analogical mapping of compositional structures, which integrates an associative memory known as sparse distributed memory (SDM). The SDM enables non-commutative binding of compositional structures, which makes it possible to predict novel patterns in sequences. To demonstrate this property we apply the network to a commonly used intelligence test called Raven's Progressive Matrices. We present results of simulation experiments for the Raven's task and calculate the probability of prediction error at 95{\%} confidence level. We find that non-commutative binding requires sparse activation of the SDM and that 10–20{\%} concept-specific activation of neurons is optimal. The optimal dimensionality of the binary distributed representations of the VSA is of the order 10{\^{}}4, which is comparable with former results and the average synapse count of neurons in the cerebral cortex.},
author = {Emruli, Blerim and Gayler, Ross W and Sandin, Fredrik},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2013.6706829},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Emruli, Gayler, Sandin - 2013 - Analogical mapping and inference with binary spatter codes and sparse distributed memory.pdf:pdf},
isbn = {9781467361293},
title = {{Analogical mapping and inference with binary spatter codes and sparse distributed memory}},
year = {2013}
}
@article{Palmer2013,
abstract = {Guiding behavior requires the brain to make predictions about future sensory inputs. Here we show that efficient predictive computation starts at the earliest stages of the visual system. We estimate how much information groups of retinal ganglion cells carry about the future state of their visual inputs, and show that every cell we can observe participates in a group of cells for which this predictive information is close to the physical limit set by the statistical structure of the inputs themselves. Groups of cells in the retina also carry information about the future state of their own activity, and we show that this information can be compressed further and encoded by downstream predictor neurons, which then exhibit interesting feature selectivity. Efficient representation of predictive information is a candidate principle that can be applied at each stage of neural computation.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1307.0225},
author = {Palmer, Stephanie E. and Marre, Olivier and Berry, Michael J. and Bialek, William},
doi = {10.1073/pnas.0709640104},
eprint = {1307.0225},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palmer et al. - 2013 - Predictive information in a sensory population.pdf:pdf},
isbn = {1091-6490 (Electronic)$\backslash$n0027-8424 (Linking)},
issn = {1091-6490},
journal = {arXiv preprint arXiv: {\ldots}},
number = {1},
pages = {1--11},
pmid = {18056803},
title = {{Predictive information in a sensory population}},
year = {2013}
}
@article{Goodman2013,
annote = {NULL},
author = {Goodman, Noah D},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman - 2013 - Learning Stochastic Inverses ¨.pdf:pdf},
pages = {1--9},
title = {{Learning Stochastic Inverses ¨}},
year = {2013}
}
@phdthesis{Lackey2013,
annote = {NULL},
author = {Lackey, Christopher},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lackey - 2013 - Relationships Between Motivation, Self-Efficacy, Mindsets, Attributions, And Learning Strategies An Exploratory Study.pdf:pdf},
title = {{Relationships Between Motivation, Self-Efficacy, Mindsets, Attributions, And Learning Strategies: An Exploratory Study}},
year = {2013}
}
@techreport{Yeager2013,
author = {Yeager, David S and Paunesku, Dave and Walton, Gregory M and Dweck, Carol S},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeager et al. - 2013 - Excellence in Education The Importance of Academic Mindsets.pdf:pdf},
title = {{Excellence in Education: The Importance of Academic Mindsets}},
year = {2013}
}
@article{Boaler2013,
abstract = {Recent scientific evidence demonstrates both the incredible potential of the brain to grow and change and the powerful impact of growth mindset messages upon students' attainment. Schooling practices, however, particularly in England, are based upon notions of fixed ability thinking which limits students' attainment and increases inequality. This article reviews evidence for brain plasticity, the importance of mindset and the ways that mindset messages may be communicated through classroom and grouping practices.},
author = {Boaler, Jo},
doi = {10.2304/forum.2013.55.1.143},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boaler - 2013 - Ability and Mathematics The Mindset Revolution that Is Reshaping Education.pdf:pdf},
issn = {0963-8253},
journal = {FORUM: for promoting 3-19 comprehensive education},
keywords = {Ability Grouping,Brain,Brain Hemisphere Functions,Child Development,Cognitive Ability,Elementary Education,England,Foreign Countries,Mathematics Achievement,Mathematics Instruction,Teaching Methods},
number = {1},
pages = {143--152},
title = {{Ability and Mathematics: The Mindset Revolution that Is Reshaping Education}},
volume = {55},
year = {2013}
}
@article{Masters2013,
author = {Masters, Geoff N},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Masters - 2013 - Towards a growth mindset in assessment.pdf:pdf},
journal = {Australian Council for Educational Research Occasional Essays},
title = {{Towards a growth mindset in assessment}},
year = {2013}
}
@article{Kollias2013,
abstract = {We present a PDP model of binary choice verbal analogy problems (A:B as C:[D1|D2], where D1 and D2 represent choice alternatives). We train a recurrent neural network in item-relation-item triples and use this network to test performance on analogy questions. Without training on analogy problems per se, the model explains the developmental shift from associative to relational responding as an emergent consequence of learning upon the environment's statistics. Such learning allows gradual, item-specific acquisition of relational knowledge to overcome the influence of unbalanced association frequency, accounting for association effects of analogical reasoning seen in cognitive development. The network also captures the overall degradation in performance after anterior temporal damage by deleting a fraction of learned connections, while capturing the return of associative dominance after frontal damage by treating frontal structures as necessary for maintaining activation of A and B while seeking a relation between C and D. While our theory is still far from being complete it provides a unified explanation of findings that need to be considered together in any integrated account of analogical reasoning.},
author = {Kollias, Pavlos and McClelland, James L.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kollias, McClelland - 2013 - Context, cortex, and associations A connectionist developmental approach to verbal analogies.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Analogical reasoning,Cognitive control,Cognitive development,Connectionist models,FTLD,Word association},
number = {NOV},
pages = {1--14},
pmid = {24312068},
title = {{Context, cortex, and associations: A connectionist developmental approach to verbal analogies}},
volume = {4},
year = {2013}
}
@article{Fong2013,
abstract = {In this dissertation we develop a new formal graphical framework for causal reasoning. Starting with a review of monoidal categories and their associated graphical languages, we then revisit probability theory from a categorical perspective and introduce Bayesian networks, an existing structure for describing causal relationships. Motivated by these, we propose a new algebraic structure, which we term a causal theory. These take the form of a symmetric monoidal category, with the objects representing variables and morphisms ways of deducing information about one variable from another. A major advantage of reasoning with these structures is that the resulting graphical representations of morphisms match well with intuitions for flows of information between these variables. These categories can then be modelled in other categories, providing concrete interpretations for the variables and morphisms. In particular, we shall see that models in the category of measurable spaces and stochastic maps provide a slight generalisation of Bayesian networks, and naturally form a category themselves. We conclude with a discussion of this category, classifying the morphisms and discussing some basic universal constructions. ERRATA: (i) Pages 41-42: Objects of a causal theory are words, not collections, in {\$}V{\$}, and we include swaps as generating morphisms, subject to the identities defining a symmetric monoidal category. (ii) Page 46: A causal model is a strong symmetric monoidal functor.},
archivePrefix = {arXiv},
arxivId = {1301.6201},
author = {Fong, Brendan},
eprint = {1301.6201},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong - 2013 - Causal Theories A Categorical Perspective on Bayesian Networks.pdf:pdf},
pages = {72},
title = {{Causal Theories: A Categorical Perspective on Bayesian Networks}},
year = {2013}
}
@article{Mante2013,
abstract = {Prefrontal cortex is thought to have a fundamental role in flexible, context-dependent behaviour, but the exact nature of the computations underlying this role remains largely unknown. In particular, individual prefrontal neurons often generate remarkably complex responses that defy deep understanding of their contribution to behaviour. Here we study prefrontal cortex activity in macaque monkeys trained to flexibly select and integrate noisy sensory inputs towards a choice. We find that the observed complexity and functional roles of single neurons are readily understood in the framework of a dynamical process unfolding at the level of the population. The population dynamics can be reproduced by a trained recurrent neural network, which suggests a previously unknown mechanism for selection and integration of task-relevant inputs. This mechanism indicates that selection and integration are two aspects of a single dynamical process unfolding within the same prefrontal circuits, and potentially provides a novel, general framework for understanding context-dependent computations.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V and Newsome, William T},
doi = {10.1038/nature12742},
eprint = {15334406},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mante et al. - 2013 - Context-dependent computation by recurrent dynamics in prefrontal cortex.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$n0028-0836 (Linking)},
issn = {1476-4687},
journal = {Nature},
keywords = {Animals,Choice Behavior,Choice Behavior: physiology,Discrimination Learning,Macaca mulatta,Macaca mulatta: physiology,Male,Models, Neurological,Nerve Net,Nerve Net: cytology,Nerve Net: physiology,Neurons,Neurons: physiology,Prefrontal Cortex,Prefrontal Cortex: cytology,Prefrontal Cortex: physiology},
number = {7474},
pages = {78--84},
pmid = {24201281},
publisher = {Nature Publishing Group},
title = {{Context-dependent computation by recurrent dynamics in prefrontal cortex.}},
volume = {503},
year = {2013}
}
@article{Saxe2013,
abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
archivePrefix = {arXiv},
arxivId = {1312.6120},
author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
eprint = {1312.6120},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saxe, McClelland, Ganguli - 2013 - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf:pdf},
isbn = {1312.6120},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}},
year = {2013}
}
@article{Bengio2013,
abstract = {Unsupervised learning of representations has been found useful in many applications and benefits from several advantages, e.g., where there are many un- labeled examples and few labeled ones (semi-supervised learning), or where the unlabeled or labeled examples are from a distribution different but related to the one of interest (self-taught learning, multi-task learning, and domain adaptation). Some of these algorithms have successfully been used to learn a hierarchy of features, i.e., to build a deep architecture, either as initialization for a supervised predictor, or as a generative model. Deep learning algorithms can yield representations that are more abstract and better disentangle the hidden factors of variation underlying the unknown generating distribution, i.e., to capture invariances and discover non-local structure in that distribution. This chapter reviews the main motivations and ideas behind deep learning algorithms and their representation-learning components, as well as recent results in this area, and proposes a vision of challenges and hopes on the road ahead, focusing on the questions of invariance and disentangling.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.0445v1},
author = {Bengio, Yoshua and Courville, Aaron},
doi = {10.1007/978-3-642-36657-4_1},
eprint = {arXiv:1305.0445v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Courville - 2013 - Deep Learning of Representations.pdf:pdf},
isbn = {9783642366567},
issn = {18684394},
journal = {Intelligent Systems Reference Library},
pages = {1--28},
title = {{Deep Learning of Representations}},
volume = {49},
year = {2013}
}
@article{Gulcehre2013,
abstract = {We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hy- pothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experi- ments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first level of the two-tiered MLP is pre-trained with intermediate level targets being the presence of sprites at each location, while the second level takes the output of the first level as input and predicts the final task target binary event. The two-tiered MLP architecture, with a few tens of thou- sand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, de- cision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not per- formed is due to the composition of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective lo- cal minima.},
archivePrefix = {arXiv},
arxivId = {1301.4083},
author = {G{\"{u}}l{\c{c}}ehre, {\c{C}}ağlar and Bengio, Yoshua},
eprint = {1301.4083},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/G{\"{u}}l{\c{c}}ehre, Bengio - 2013 - Knowledge Matters Importance of Prior Information for Optimization.pdf:pdf},
journal = {arXiv preprint arXiv:1301.4083},
pages = {1--12},
title = {{Knowledge Matters : Importance of Prior Information for Optimization}},
year = {2013}
}
@article{Lupyan2013,
abstract = {It is shown that educated adults routinely make errors in placing stimuli into familiar, well-defined categories such as triangle and odd number. Scalene triangles are often rejected as instances of triangles and 798 is categorized by some as an odd number. These patterns are observed both in timed and untimed tasks, hold for people who can fully express the necessary and sufficient conditions for category membership, and for individuals with varying levels of education. A sizeable minority of people believe that 400 is more even than 798 and that an equilateral triangle is the most "trianglest" of triangles. Such beliefs predict how people instantiate other categories with necessary and sufficient conditions, e.g., grandmother. I argue that the distributed and graded nature of mental representations means that human algorithms, unlike conventional computer algorithms, only approximate rule-based classification and never fully abstract from the specifics of the input. This input-sensitivity is critical to obtaining the kind of cognitive flexibility at which humans excel, but comes at the cost of generally poor abilities to perform context-free computations. If human algorithms cannot be trusted to produce unfuzzy representations of odd numbers, triangles, and grandmothers, the idea that they can be trusted to do the heavy lifting of moment-to-moment cognition that is inherent in the metaphor of mind as digital computer still common in cognitive science, needs to be seriously reconsidered. ?? 2013 Elsevier B.V.},
author = {Lupyan, Gary},
doi = {10.1016/j.cognition.2013.08.015},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lupyan - 2013 - The difficulties of executing simple algorithms Why brains make mistakes computers don't.pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Categorization,Concepts,Distributed representations,Human algorithms,Inference,Prototypes},
number = {3},
pages = {615--636},
pmid = {24156803},
publisher = {Elsevier B.V.},
title = {{The difficulties of executing simple algorithms: Why brains make mistakes computers don't}},
volume = {129},
year = {2013}
}
@article{Mikolov2013,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov, Yih, Zweig - 2013 - Linguistic regularities in continuous space word representations.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT},
number = {June},
pages = {746--751},
pmid = {1938007},
title = {{Linguistic regularities in continuous space word representations}},
year = {2013}
}
@article{DeSmedt2013,
abstract = {Many studies tested the association between numerical magnitude processing and mathematics achievement, results differ depending on the number format used. For symbolic numbers (digits), data are consistent and robust across studies and populations: weak performance correlates with low math achievement and dyscalculia. For non-symbolic formats (dots), many conflicting findings have been reported. These inconsistencies might be explained by methodological issues. Alternatively, it might be that the processes measured by non-symbolic tasks are not critical for school-relevant mathematics. A few neuroimaging studies revealed that brain activation during number comparison correlates with the children's mathematics achievement level, but the consistency of such relationships for symbolic and non-symbolic processing is unclear. These neurocognitive data provide ground for educational interventions, which seem to have positive effects on children's numerical development in (a)typical populations.},
author = {{De Smedt}, Bert and No{\"{e}}l, Marie-Pascale and Gilmore, Camilla and Ansari, Daniel},
doi = {10.1023/A},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Smedt et al. - 2013 - The relationship between symbolic and non-symbolic numerical magnitude processing skills and the typical and at.pdf:pdf},
isbn = {1024471506938},
journal = {Trends in Neuroscience {\&} Education},
pages = {48--55},
title = {{The relationship between symbolic and non-symbolic numerical magnitude processing skills and the typical and atypical development of mathematics: a review of evidence from brain and behavior}},
volume = {2},
year = {2013}
}
@article{Price2013,
author = {Price, G. R. and Mazzocco, M. M. M. and Ansari, D.},
doi = {10.1523/JNEUROSCI.2936-12.2013},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Price, Mazzocco, Ansari - 2013 - Why Mental Arithmetic Counts Brain Activation during Single Digit Arithmetic Predicts High School Math.pdf:pdf},
issn = {0270-6474},
journal = {Journal of Neuroscience},
number = {1},
pages = {156--163},
title = {{Why Mental Arithmetic Counts: Brain Activation during Single Digit Arithmetic Predicts High School Math Scores}},
volume = {33},
year = {2013}
}
@article{Davis2013,
author = {Davis, Tyler and Poldrack, Russell a.},
doi = {10.1111/nyas.12156},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Poldrack - 2013 - Measuring neural representations with fMRI practices and pitfalls.pdf:pdf},
issn = {00778923},
journal = {Annals of the New York Academy of Sciences},
keywords = {adaptation,fmri,mvpa,representation},
number = {1},
pages = {108--134},
title = {{Measuring neural representations with fMRI: practices and pitfalls}},
volume = {1296},
year = {2013}
}
@article{McClelland2013,
abstract = {The complementary learning systems theory of the roles of hippocampus and neocortex (McClelland, McNaughton, {\&} O'Reilly, 1995) holds that the rapid integration of arbitrary new information into neocortical structures is avoided to prevent catastrophic interference with structured knowledge representations stored in synaptic connections among neocortical neurons. Recent studies (Tse et al., 2007, 2011) showed that neocortical circuits can rapidly acquire new associations that are consistent with prior knowledge. The findings challenge the complementary learning systems theory as previously presented. However, new simulations extending those reported in McClelland et al. (1995) show that new information that is consistent with knowledge previously acquired by a putatively cortexlike artificial neural network can be learned rapidly and without interfering with existing knowledge; it is when inconsistent new knowledge is acquired quickly that catastrophic interference ensues. Several important features of the findings of Tse et al. (2007, 2011) are captured in these simulations, indicating that the neural network model used in McClelland et al. has characteristics in common with neocortical learning mechanisms. An additional simulation generalizes beyond the network model previously used, showing how the rate of change of cortical connections can depend on prior knowledge in an arguably more biologically plausible network architecture. In sum, the findings of Tse et al. are fully consistent with the idea that hippocampus and neocortex are complementary learning systems. Taken together, these findings and the simulations reported here advance our knowledge by bringing out the role of consistency of new experience with existing knowledge and demonstrating that the rate of change of connections in real and artificial neural networks can be strongly prior-knowledge dependent.},
author = {McClelland, James L},
doi = {10.1037/a0033812},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland - 2013 - Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems th.pdf:pdf},
isbn = {6507251232},
issn = {1939-2222},
journal = {Journal of experimental psychology. General},
keywords = {1932,1979,2007,2011,bartlett,bransford,consolidation,hippocampus,in two recent articles,it has long been,known that it is,learning,memory,new,relatively easy to learn,schemas,things that are consistent,tse et al,with prior knowledge},
pages = {1190--210},
pmid = {23978185},
title = {{Incorporating rapid neocortical learning of new schema-consistent information into complementary learning systems theory.}},
volume = {142},
year = {2013}
}
@article{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
eprint = {1312.6199},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:pdf},
keywords = {CNN},
mendeley-tags = {CNN},
month = {dec},
pages = {1--10},
title = {{Intriguing properties of neural networks}},
year = {2013}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using ConvolutionalNetworks for classi- fication, localization and detection.We showhowamultiscale and slidingwindow approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound- aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul- taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNetLarge ScaleVisual RecognitionChallenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competitionwork, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
journal = {arXiv preprint arXiv:1312.6229},
keywords = {CNN,Computer Vision},
mendeley-tags = {CNN,Computer Vision},
pages = {1--15},
title = {{OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks}},
year = {2013}
}
@article{Darrah2013,
author = {Darrah, Erika and Giles, JT and Ols, ML},
doi = {10.1126/scitranslmed.3005370.Erosive},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Darrah, Giles, Ols - 2013 - Erosive rheumatoid arthritis is associated with antibodies that activate PAD4 by increasing calcium sensitiv.pdf:pdf},
journal = {Science translational {\ldots}},
keywords = {Auto-immune,PAD4,Physiology},
mendeley-tags = {Auto-immune,PAD4,Physiology},
number = {186},
pages = {1--19},
title = {{Erosive rheumatoid arthritis is associated with antibodies that activate PAD4 by increasing calcium sensitivity}},
volume = {5},
year = {2013}
}
@article{Li2012,
abstract = {Efficient exploration is widely recognized as a fundamental challenge in- herent in reinforcement-learning problems. Algorithms that explore efficiently con- verge faster to better policies. While heuristics techniques are popular in practice, they lack formal guarantees and may not work well in general. This chapter studies principled algorithms, both model-based and model-free ones, in a unified manner based on a powerful theorem. These so-called PAC-MDP algorithms enjoy polyno- mial sample complexity of exploration, and hence behave near-optimally except in a “small” number of steps with high probability. Furthermore, a new learning model known as KWIK is used to unify most existing model-based PAC-MDP algorithms as well as to facilitate development of new ones for various subclasses of MDPs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.07669v1},
author = {Li, Lihong},
doi = {10.1007/978-3-642-27645-3_6},
eprint = {arXiv:1512.07669v1},
file = {:home/andrew/Documents/grad/Papers/Li.12.SampleComplexity.pdf:pdf},
isbn = {9783642015267},
issn = {18674542},
journal = {Adaptation, Learning, and Optimization},
keywords = {Covariance,Nite},
pages = {175--204},
pmid = {17082042},
title = {{Sample complexity bounds of exploration}},
volume = {12},
year = {2012}
}
@article{Jadhav2012,
author = {Jadhav, Shantanu P and Kemere, Caleb and German, P Walter and Frank, Loren M},
doi = {10.1126/science.1217230},
file = {:home/andrew/Documents/grad/Papers/science.1217230.full.pdf:pdf},
isbn = {0036807510959203},
issn = {0036-8075, 1095-9203},
journal = {Science},
number = {May},
pmid = {22555434},
title = {{Awake Hippocampal Sharp-Wave Ripples Support Spatial Memory}},
year = {2012}
}
@article{Doyen2012,
abstract = {The perspective that behavior is often driven by unconscious determinants has become widespread in social psychology. Bargh, Chen, and Burrows' (1996) famous study, in which participants unwittingly exposed to the stereotype of age walked slower when exiting the laboratory, was instrumental in defining this perspective. Here, we present two experiments aimed at replicating the original study. Despite the use of automated timing methods and a larger sample, our first experiment failed to show priming. Our second experiment was aimed at manipulating the beliefs of the experimenters: Half were led to think that participants would walk slower when primed congruently, and the other half was led to expect the opposite. Strikingly, we obtained a walking speed effect, but only when experimenters believed participants would indeed walk slower. This suggests that both priming and experimenters' expectations are instrumental in explaining the walking speed effect. Further, debriefing was suggestive of awareness of the primes. We conclude that unconscious behavioral priming is real, while real, involves mechanisms different from those typically assumed to cause the effect.},
author = {Doyen, St{\'{e}}phane and Klein, Olivier and Pichon, Cora Lise and Cleeremans, Axel},
doi = {10.1371/journal.pone.0029081},
file = {:home/andrew/Documents/grad/Papers/journal.pone.0029081.PDF:PDF},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
pmid = {22279526},
title = {{Behavioral priming: It's all in the mind, but whose mind?}},
volume = {7},
year = {2012}
}
@article{Lu2012,
author = {Lu, Hongjing and Chen, Dawn and Holyoak, Keith J and Anderson, N John and Doumas, Alex and Hummel, John},
doi = {10.1037/a0028719},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2012 - Bayesian Analogy With Relational Transformations.pdf:pdf},
keywords = {10,1037,a0028719,analogy,bayesian models,doi,dx,generalization,http,org,relation learning,supp,supplemental materials},
number = {3},
pages = {617--648},
title = {{Bayesian Analogy With Relational Transformations}},
volume = {119},
year = {2012}
}
@article{Lobato2012,
author = {Lobato, Joanne},
doi = {10.1080/00461520.2012.693353},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lobato - 2012 - The Actor-Oriented Transfer Perspective and Its Contributions to Educational Research and Practice.pdf:pdf},
journal = {Educational Psychologist},
number = {3},
pages = {232--247},
title = {{The Actor-Oriented Transfer Perspective and Its Contributions to Educational Research and Practice}},
volume = {47},
year = {2012}
}
@article{Rattan2012,
author = {Rattan, Aneeta and Good, Catherine and Dweck, Carol S},
doi = {10.1016/j.jesp.2011.12.012},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rattan, Good, Dweck - 2012 - “It's ok — Not everyone can be good at math” Instructors with an entity theory comfort (and demotivat.pdf:pdf},
issn = {0022-1031},
journal = {Journal of Experimental Social Psychology},
keywords = {Implicit theory,Intelligence,Pedagogical practice,Teaching},
number = {3},
pages = {731--737},
publisher = {Elsevier Inc.},
title = {{“It's ok — Not everyone can be good at math”: Instructors with an entity theory comfort (and demotivate) students}},
volume = {48},
year = {2012}
}
@incollection{Davis2012,
abstract = {In this chapter, three interrelated concepts—student engagement, motiva- tion, and resilience—are examined through the lens of “mindsets.” Mindsets are assumptions that we possess about ourselves and others that guide our behavior. The mindset that educators hold about the factors that contribute to student engagement, motivation, and resilience determines their expectations, teaching practices, and relationships with students. We identify the key components of these three concepts, highlighting those that overlap. We distinguish between extrinsic and intrinsic motivation and the ways in which the latter is more closely attuned with student engagement and resilience than the former. We encourage the ongoing discussion of mindsets at staff meetings so that teachers become increas- ingly aware of the mindset of engaged, motivated learners and consider how to nurture this mindset in the classroom. We offer many strategies to facilitate the enrichment of this mindset in all students},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Davis, Marcia H and Mcpartland, James M},
booktitle = {Handbook of Research on Student Engagement},
doi = {10.1007/978-1-4614-2018-7},
eprint = {arXiv:1011.1669v3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Mcpartland - 2012 - The Power of Mindsets Nurturing Engagement, Motivation, and Resilience in Students.pdf:pdf},
isbn = {978-1-4614-2017-0},
issn = {03610365},
pages = {515--539},
pmid = {6370941},
title = {{The Power of Mindsets: Nurturing Engagement, Motivation, and Resilience in Students}},
year = {2012}
}
@article{Koedinger2012,
abstract = {Despite the accumulation of substantial cognitive science research relevant to education, there remains confusion and controversy in the application of research to educational practice. In support of a more systematic approach, we describe the Knowledge-Learning-Instruction (KLI) framework. KLI promotes the emergence of instructional principles of high potential for generality, while explicitly identifying constraints of and opportunities for detailed analysis of the knowledge students may acquire in courses. Drawing on research across domains of science, math, and language learning, we illustrate the analyses of knowledge, learning, and instructional events that the KLI framework affords. We present a set of three coordinated taxonomies of knowledge, learning, and instruction. For example, we identify three broad classes of learning events (LEs): (a) memory and fluency processes, (b) induction and refinement processes, and (c) understanding and sense-making processes, and we show how these can lead to different knowledge changes and constraints on optimal instructional choices.},
author = {Koedinger, Kenneth R. and Corbett, Albert T. and Perfetti, Charles},
doi = {10.1111/j.1551-6709.2012.01245.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koedinger, Corbett, Perfetti - 2012 - The Knowledge-Learning-Instruction Framework Bridging the Science-Practice Chasm to Enhance Robust.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Cognitive modeling,Cognitive task analysis,Education,Experimentation,Instructional principles,Knowledge representation,Learning principles},
number = {5},
pages = {757--798},
pmid = {22486653},
title = {{The Knowledge-Learning-Instruction Framework: Bridging the Science-Practice Chasm to Enhance Robust Student Learning}},
volume = {36},
year = {2012}
}
@article{Day2012,
author = {Day, Samuel B and Goldstone, Robert L},
doi = {10.1080/00461520.2012.696438},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day, Goldstone - 2012 - The Import of Knowledge Export Connecting Findings and Theories of Transfer of Learning.pdf:pdf},
journal = {Educational Psychologist},
number = {3},
pages = {153--176},
title = {{The Import of Knowledge Export : Connecting Findings and Theories of Transfer of Learning}},
volume = {47},
year = {2012}
}
@article{Richland2012a,
abstract = {Many students graduate from K–12 mathematics programs without flexible, conceptual mathematics knowledge. This article reviews psychological and educational research to propose that refining K–12 classroom instruction such that students draw connections through relational comparisons may enhance their long-term ability to transfer and engage with mathematics as a meaningful system. We begin by examining the mathematical knowledge of students in one community college, reviewing results that show even after completing a K–12 required mathematics sequence, these students were unlikely to flexibly reason about mathematics. Rather than drawing relationships between presented problems or inferences about the representations, students preferred to attempt previously memorized (often incorrect) procedures (Givvin, Stigler, {\&} Thompson, 2011; Stigler, Givvin, {\&} Thompson, 2010). We next describe the relations between the cognition of flexible, comparative reasoning and experimentally derived strategies for supporting students' ability to make these connections. A cross-cultural study found that U.S. teachers currently use these strategies much less frequently than their international counterparts (Hiebert et al., 2003; Richland, Zur, {\&} Holyoak, 2007), suggesting that these practices may be correlated with high student performance. Finally, we articulate a research agenda for improving and studying pedagogical practices for fostering students' relational thinking about mathematics.},
author = {Richland, Lindsey E. and Stigler, James W. and Holyoak, Keith J.},
doi = {10.1080/00461520.2012.667065},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Richland, Stigler, Holyoak - 2012 - Teaching the Conceptual Structure of Mathematics(2).pdf:pdf},
isbn = {0046-1520$\backslash$r1532-6985},
issn = {0046-1520},
journal = {Educational Psychologist},
number = {3},
pages = {189--203},
title = {{Teaching the Conceptual Structure of Mathematics}},
volume = {47},
year = {2012}
}
@article{Davidson2012,
abstract = {We tested the hypothesis that, when children learn to correctly count sets, they make a semantic induction about the meanings of their number words. We tested the logical understanding of number words in 84 children that were classified as "cardinal-principle knowers" by the criteria set forth by Wynn (1992). Results show that these children often do not know (1) which of two numbers in their count list denotes a greater quantity, and (2) that the difference between successive numbers in their count list is 1. Among counters, these abilities are predicted by the highest number to which they can count and their ability to estimate set sizes. Also, children's knowledge of the principles appears to be initially item-specific rather than general to all number words, and is most robust for very small numbers (e.g., 5) compared to larger numbers (e.g., 25), even among children who can count much higher (e.g., above 30). In light of these findings, we conclude that there is little evidence to support the hypothesis that becoming a cardinal-principle knower involves a semantic induction over all items in a child's count list. ?? 2011 Elsevier B.V.},
author = {Davidson, Kathryn and Eng, Kortney and Barner, David},
doi = {10.1016/j.cognition.2011.12.013},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davidson, Eng, Barner - 2012 - Does learning to count involve a semantic induction.pdf:pdf},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
keywords = {Cardinal principle,Counting,Induction,Number knowledge acquisition},
number = {1},
pages = {162--173},
pmid = {22245033},
publisher = {Elsevier B.V.},
title = {{Does learning to count involve a semantic induction?}},
volume = {123},
year = {2012}
}
@article{Mason2012,
abstract = {Amazon's Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.},
archivePrefix = {arXiv},
arxivId = {http://ssrn.com/abstract=1691163},
author = {Mason, Winter and Suri, Siddharth},
doi = {10.3758/s13428-011-0124-6},
eprint = {/ssrn.com/abstract=1691163},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mason, Suri - 2012 - Conducting behavioral research on Amazon's Mechanical Turk.pdf:pdf},
isbn = {1554-351X},
issn = {1554-3528},
journal = {Behavior Research Methods},
keywords = {Behavioral Research,Data Collection,Humans,Research Design},
number = {1},
pages = {1--23},
pmid = {21717266},
primaryClass = {http:},
title = {{Conducting behavioral research on Amazon's Mechanical Turk}},
volume = {44},
year = {2012}
}
@article{Braithwaite2012,
author = {Braithwaite, David W and Goldstone, Robert L},
journal = {Proceedings of the 34th Annual Conference of the Cognitive Science Society},
keywords = {analogy,comparison,instruction,mathematics,schemas,transfer},
number = {c},
pages = {138--143},
title = {{Inducing mathematical concepts from specific examples: The role of schema-level variation}},
year = {2012}
}
@article{Bengio2012,
abstract = {We propose a theory that relates difficulty of learning in deep architectures to culture and language. It is articulated around the following hypotheses: (1) learning in an individual human brain is hampered by the presence of effective local minima; (2) this optimization difficulty is particularly important when it comes to learning higher-level abstractions, i.e., concepts that cover a vast and highly-nonlinear span of sensory configurations; (3) such high-level abstrac- tions are best represented in brains by the composition of many levels of representation, i.e., by deep architectures; (4) a human brain can learn such high-level abstractions if guided by the signals produced by other humans, which act as hints or indirect supervision for these high-level abstractions; and (5), language and the recombination and optimization of mental concepts provide an efficient evolutionary recombination operator, and this gives rise to rapid search in the space of communicable ideas that help humans build up better high-level internal representations of their world. These hypotheses put together imply that human culture and the evolution of ideas have been crucial to counter an optimization difficulty: this optimization difficulty would otherwise make it very difficult for human brains to capture high-level knowl- edge of the world. The theory is grounded in experimental observations of the difficulties of training deep artificial neural networks. Plausible consequences of this theory for the efficiency of cultural evolution are sketched.},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.2990v2},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-55337-0_3},
eprint = {arXiv:1203.2990v2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2012 - Evolving culture vs local minima.pdf:pdf},
isbn = {978-3-642-55336-3, 978-3-642-55337-0},
issn = {1860949X},
journal = {arXiv preprint arXiv:1203.2990},
pages = {1--28},
title = {{Evolving culture vs local minima}},
volume = {2006},
year = {2012}
}
@incollection{VanHulle2012,
abstract = {A topographic map is a two-dimensional, nonlinear approximation of a potentially high-dimensional data manifold, which makes it an appealing instrument for visualizing and exploring high-dimensional data. The self-organizing map (SOM) is the most widely used algorithm, and it has led to thousands of applications in very diverse areas. In this chapter we introduce the SOM algorithm, discuss its properties and applications, and also discuss some of its extensions and new types of topographic map formation, such as those that can be used for processing categorical data, time series, and tree-structured data.},
author = {{Van Hulle}, Marc M.},
booktitle = {Handbook of Natural Computing},
doi = {10.1007/978-3-540-92910-9_19},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Hulle - 2012 - 19 - Self-organizing Maps.pdf:pdf},
isbn = {978-3-540-92909-3},
pages = {585--622},
title = {{19 - Self-organizing Maps}},
year = {2012}
}
@article{Richland2012,
abstract = {Many students graduate from K–12 mathematics programs without flexible, conceptual mathematics knowledge. This article reviews psychological and educational research to propose that refining K–12 classroom instruction such that students draw connections through relational comparisons may enhance their long-term ability to transfer and engage with mathematics as a meaningful system. We begin by examining the mathematical knowledge of students in one community college, reviewing results that show even after completing a K–12 required mathematics sequence, these students were unlikely to flexibly reason about mathematics. Rather than drawing relationships between presented problems or inferences about the representations, students preferred to attempt previously memorized (often incorrect) procedures (Givvin, Stigler, {\&} Thompson, 2011; Stigler, Givvin, {\&} Thompson, 2010). We next describe the relations between the cognition of flexible, comparative reasoning and experimentally derived strategies for supporting students' ability to make these connections. A cross-cultural study found that U.S. teachers currently use these strategies much less frequently than their international counterparts (Hiebert et al., 2003; Richland, Zur, {\&} Holyoak, 2007), suggesting that these practices may be correlated with high student performance. Finally, we articulate a research agenda for improving and studying pedagogical practices for fostering students' relational thinking about mathematics.},
author = {Richland, Lindsey E. and Stigler, James W. and Holyoak, Keith J.},
doi = {10.1080/00461520.2012.667065},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Richland, Stigler, Holyoak - 2012 - Teaching the Conceptual Structure of Mathematics.pdf:pdf},
isbn = {0046-1520$\backslash$r1532-6985},
issn = {0046-1520},
journal = {Educational Psychologist},
number = {3},
pages = {189--203},
title = {{Teaching the Conceptual Structure of Mathematics}},
volume = {47},
year = {2012}
}
@article{Braithwaite2012,
author = {Braithwaite, David W. and Goldstone, Robert L.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Braithwaite, Goldstone - 2012 - Inducing mathematical concepts from specific examples The role of schema-level variation.pdf:pdf},
journal = {Proceedings of the 34th Annual Conference of the Cognitive Science Society},
keywords = {analogy,comparison,instruction,mathematics,schemas,transfer},
number = {c},
pages = {138--143},
title = {{Inducing mathematical concepts from specific examples: The role of schema-level variation}},
year = {2012}
}
@misc{RonnyForsBerndtStenberg2012,
abstract = {Background. Studies have shown conflicting results on the association between nickelexposure from orthodontic appliances and nickel sensitization.Objectives {\&} Method. In a cross-sectional study, we investigated the associationbetween nickel sensitization and exposure to orthodontic appliances and piercings. 4376adolescents were patch tested following a questionnaire asking for earlier piercing andorthodontic treatment. Exposure to orthodontic appliances was verified in dental records.Results. Questionnaire data demonstrated a reduced risk of nickel sensitization whenorthodontic treatment preceded piercing (OR 0.46; CI 0.27–0.78). Data from dentalrecords demonstrated similar results (OR 0.61, CI 0.36–1.02), but statistical significancewas lost when adjusting for background factors. Exposure to full, fixed applianceswith NiTi-containing alloys (OR 0.31, CI 0.10–0.98) as well as a pooled ‘high nickel-releasing' appliance group (OR 0.56, CI 0.32–0.97) prior to piercing was associated witha significantly reduced risk of nickel sensitization.Conclusion. High nickel-containing orthodontic appliances preceding piercingreduces the risk of nickel sensitization by a factor 1.5–2. The risk reduction is associatedwith estimated nickel release of the appliance and length of treatment. Sex, age at piercingand number of piercings are also important risk indicators. Research on the role of dentalmaterials in the development of immunological tolerance is needed.},
author = {{Ronny Fors, Berndt Stenberg}, Hans Stenlund and Maurits Persson},
booktitle = {Contact Dermatitis},
doi = {10.1111/j.1600-0536.2012.02097.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronny Fors, Berndt Stenberg - 2012 - Epdf @ Onlinelibrary.Wiley.Com(2).html:html},
keywords = {cross-sectional,dental braces,patch test,questionnaire,tolerance.},
number = {6},
pages = {342--350},
title = {{Epdf @ Onlinelibrary.Wiley.Com}},
volume = {67},
year = {2012}
}
@article{Nathan2012,
abstract = {I explore a belief about learning and teaching that is commonly held in education and society at large that nonetheless is deeply flawed. The belief asserts that mastery of formalisms—specialized representations such as symbolic equations and diagrams with no inherent meaning except that which is established by convention—is prerequisite to applied knowledge. A formalisms first (FF) view of learning, rooted in Western dualist philosophy, incorrectly advocates the introduction of formalisms too early in the development of learners' conceptual understanding and can encourage a formalisms-only mind-set toward learning and instruction. I identify the prevalence of FF in curriculum and instruction and outline some of the serious problems engendered by FF approaches. I then turn to promising alternatives that support progressive formalization, problem-based learning, and inquiry learning, which capitalize on the strengths of formalisms but avoid some of the most costly problems found in FF approaches.},
author = {Nathan, Mitchell J.},
doi = {10.1080/00461520.2012.667063},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nathan - 2012 - Rethinking Formalisms in Formal Education.pdf:pdf},
issn = {0046-1520},
journal = {Educational Psychologist},
number = {2},
pages = {125--148},
title = {{Rethinking Formalisms in Formal Education}},
volume = {47},
year = {2012}
}
@misc{RonnyForsBerndtStenberg2012a,
abstract = {Background. Studies have shown conflicting results on the association between nickelexposure from orthodontic appliances and nickel sensitization.Objectives {\&} Method. In a cross-sectional study, we investigated the associationbetween nickel sensitization and exposure to orthodontic appliances and piercings. 4376adolescents were patch tested following a questionnaire asking for earlier piercing andorthodontic treatment. Exposure to orthodontic appliances was verified in dental records.Results. Questionnaire data demonstrated a reduced risk of nickel sensitization whenorthodontic treatment preceded piercing (OR 0.46; CI 0.27–0.78). Data from dentalrecords demonstrated similar results (OR 0.61, CI 0.36–1.02), but statistical significancewas lost when adjusting for background factors. Exposure to full, fixed applianceswith NiTi-containing alloys (OR 0.31, CI 0.10–0.98) as well as a pooled ‘high nickel-releasing' appliance group (OR 0.56, CI 0.32–0.97) prior to piercing was associated witha significantly reduced risk of nickel sensitization.Conclusion. High nickel-containing orthodontic appliances preceding piercingreduces the risk of nickel sensitization by a factor 1.5–2. The risk reduction is associatedwith estimated nickel release of the appliance and length of treatment. Sex, age at piercingand number of piercings are also important risk indicators. Research on the role of dentalmaterials in the development of immunological tolerance is needed.},
author = {{Ronny Fors, Berndt Stenberg}, Hans Stenlund and Maurits Persson},
booktitle = {Contact Dermatitis},
doi = {10.1111/j.1600-0536.2012.02097.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronny Fors, Berndt Stenberg - 2012 - Epdf @ Onlinelibrary.Wiley.Com.html:html},
keywords = {cross-sectional,dental braces,patch test,questionnaire,tolerance.},
number = {6},
pages = {342--350},
title = {{Epdf @ Onlinelibrary.Wiley.Com}},
volume = {67},
year = {2012}
}
@article{Kumaran2012,
abstract = {In this article, we present a perspective on the role of the hippocampal system in generalization, instantiated in a computational model called REMERGE (recurrency and episodic memory results in generalization). We expose a fundamental, but neglected, tension between prevailing computational theories that emphasize the function of the hippocampus in pattern separation (Marr, 1971; McClelland, McNaughton, {\&} O'Reilly, 1995), and empirical support for its role in generalization and flexible relational memory (Cohen {\&} Eichenbaum, 1993; Eichenbaum, 1999). Our account provides a means by which to resolve this conflict, by demonstrating that the basic representational scheme envisioned by complementary learning systems theory (McClelland et al., 1995), which relies upon orthogonalized codes in the hippocampus, is compatible with efficient generalization-as long as there is recurrence rather than unidirectional flow within the hippocampal circuit or, more widely, between the hippocampus and neocortex. We propose that recurrent similarity computation, a process that facilitates the discovery of higher-order relationships between a set of related experiences, expands the scope of classical exemplar-based models of memory (e.g., Nosofsky, 1984) and allows the hippocampus to support generalization through interactions that unfold within a dynamically created memory space.},
author = {Kumaran, Dharshan and McClelland, James L.},
doi = {10.1037/a0028681},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumaran, McClelland - 2012 - Generalization through the recurrent interaction of episodic memories A model of the hippocampal system.pdf:pdf},
isbn = {1939-1471 (Electronic)$\backslash$r0033-295X (Linking)},
issn = {0033-295X},
journal = {Psychological Review},
keywords = {and the general structure,both specific events from,car,complementary learning systems,e,g,generalization,hippocampus,of our experiences,pattern separation,recurrence,remember,that dogs,the,the past,understanding the world relies,upon the ability to,where we just parked},
number = {3},
pages = {573--616},
pmid = {22775499},
title = {{Generalization through the recurrent interaction of episodic memories: A model of the hippocampal system.}},
volume = {119},
year = {2012}
}
@article{Carr2011,
abstract = {The hippocampus is required for the encoding, consolidation and retrieval of event memories. Although the neural mechanisms that underlie these processes are only partially understood, a series of recent papers point to awake memory replay as a potential contributor to both consolidation and retrieval. Replay is the sequential reactivation of hippocampal place cells that represent previously experienced behavioral trajectories and occurs frequently in the awake state, particularly during periods of relative immobility. Awake replay may reflect trajectories through either the current environment or previously visited environments that are spatially remote. The repetition of learned sequences on a compressed time scale is well suited to promote memory consolidation in distributed circuits beyond the hippocampus, suggesting that consolidation occurs in both the awake and sleeping animal. Moreover, sensory information can influence the content of awake replay, suggesting a role for awake replay in memory retrieval.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Carr, Margaret F. and Jadhav, Shantanu P. and Frank, Loren M.},
doi = {10.1038/nn.2732},
eprint = {NIHMS150003},
file = {:home/andrew/Documents/grad/Papers/nn.2732.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {10976256},
journal = {Nature Neuroscience},
number = {2},
pages = {147--153},
pmid = {21270783},
publisher = {Nature Publishing Group},
title = {{Hippocampal replay in the awake state: A potential substrate for memory consolidation and retrieval}},
url = {http://dx.doi.org/10.1038/nn.2732},
volume = {14},
year = {2011}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var-ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re-quirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1.1.231.4614},
eprint = {1103.0398},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
isbn = {1532-4435},
issn = {0891-2017},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
pmid = {1000183096},
title = {{Natural Language Processing (Almost) from Scratch}},
volume = {12},
year = {2011}
}
@article{Simmons2011,
author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
doi = {10.1177/0956797611417632},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simmons, Nelson, Simonsohn - 2011 - False-Positive Psychology Undisclosed Flexibility in Data Collection and Analysis Allows Presenting.pdf:pdf},
journal = {Psychological Science},
keywords = {11,17,23,about the world,disclosure,is to discover truths,methodology,motivated reasoning,our job as scientists,publication,received 3,revision accepted 5,we},
number = {11},
pages = {1359--1366},
title = {{False-Positive Psychology : Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant}},
volume = {22},
year = {2011}
}
@article{Haimovitz2011,
author = {Haimovitz, Kyla and Corpus, Jennifer H.},
doi = {10.1080/01443410.2011.585950},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haimovitz, Corpus - 2011 - Effects of person versus process praise on student motivation stability and change in emerging adulthood.pdf:pdf},
journal = {Educational Psychology},
number = {5},
pages = {595--609},
title = {{Effects of person versus process praise on student motivation: stability and change in emerging adulthood}},
volume = {31},
year = {2011}
}
@article{Day2011,
annote = {NULL},
author = {Day, Samuel B and Goldstone, Robert L},
doi = {10.1037/a0022333},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day, Goldstone - 2011 - Analogical Transfer From a Simulated Physical System.pdf:pdf},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
number = {3},
pages = {551--567},
title = {{Analogical Transfer From a Simulated Physical System}},
volume = {37},
year = {2011}
}
@article{Buhrmester2011,
author = {Buhrmester, M. and Kwang, T. and Gosling, S. D.},
doi = {10.1177/1745691610393980},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buhrmester, Kwang, Gosling - 2011 - Amazon's Mechanical Turk A New Source of Inexpensive, Yet High-Quality, Data.pdf:pdf},
isbn = {1745-6916},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
keywords = {amazon,amazon mechanical turk,com,data collection,getting work done by,here,internet,is a novel,mturk,online,open online marketplace for,others,research methods,s mechanical turk,web,www},
number = {1},
pages = {3--5},
pmid = {26162106},
title = {{Amazon's Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?}},
volume = {6},
year = {2011}
}
@article{Hinton2011,
abstract = {The artificial neural networks that are used to recognise shapes typcially use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered representations of the pose of the feature, like SIFT, that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instatiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adpating the features to the domain},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
doi = {10.1007/978-3-642-21735-7_6},
eprint = {9605103},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Krizhevsky, Wang - 2011 - Transforming auto-encoders.pdf:pdf},
isbn = {9783642217340},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Invariance,auto-encoder,shape representation},
number = {PART 1},
pages = {44--51},
pmid = {1000183096},
primaryClass = {cs},
title = {{Transforming auto-encoders}},
volume = {6791 LNCS},
year = {2011}
}
@article{Held2011,
abstract = {Would a blind subject, on regaining sight, be able to immediately visually recognize an object previously known only by touch? We addressed this question, first formulated by Molyneux three centuries ago, by working with treatable, congenitally blind individuals. We tested their ability to visually match an object to a haptically sensed sample after sight restoration. We found a lack of immediate transfer, but such cross-modal mappings developed rapidly.},
author = {Held, Richard and Ostrovsky, Yuri and de Gelder, Beatrice and DeGelder, Beatrice and Gandhi, Tapan and Ganesh, Suma and Mathur, Umang and Sinha, Pawan},
doi = {10.1038/nn.2795},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Held et al. - 2011 - The newly sighted fail to match seen with felt.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Adolescent,Blindness,Blindness: congenital,Blindness: physiopathology,Blindness: psychology,Child,Choice Behavior,Choice Behavior: physiology,Female,Humans,Male,Photic Stimulation,Photic Stimulation: methods,Sensory Deprivation,Sensory Deprivation: physiology,Touch,Touch: physiology,Vision, Ocular,Vision, Ocular: physiology,Visual Perception,Visual Perception: physiology},
number = {5},
pages = {551--3},
pmid = {21478887},
title = {{The newly sighted fail to match seen with felt.}},
volume = {14},
year = {2011}
}
@article{Goldin2011,
abstract = {Two thousand four hundred years ago Socrates gave a remarkable lesson of geometry, perhaps the first detailed record of a pedagogical method in vivo in history [Plato. (2008). Apolog{\'{i}}a de S{\'{o}}crates. Men{\'{o}}n. Cr{\'{a}}tilo. Madrid: Alianza Editorial]. Socrates asked Meno's slave 50 questions requiring simple additions or multiplications. At the end of the lesson the student discovered by himself how to duplicate a square using the diagonal of the given one as the side of the new square. We studied empirically the reproducibility of this dialogue in educated adults and adolescents of the 21st century. Our results show a remarkable agreement between Socratic and empiric dialogues. Even in questions in which Meno's slave made a mistake, within an unbounded number of possible erred responses, the vast majority of participants produced the same error as Meno's slave. Our results show that the Socratic dialogue is built on a strong intuition of human knowledge and reasoning which persists more than 24 centuries after its conception, providing one of the most striking demonstrations of universality across time and cultures. At the same time, they also emphasize its educational failure. After following every single question including Socrates' "diagonal argument," almost 50{\%} of the participants failed to learn the simplest generalization when asked to double the area of a square of different size. {\textcopyright} 2011 The Authors. Journal Compilation {\textcopyright} 2011 International Mind, Brain, and Education Society and Blackwell Publishing, Inc.},
author = {Goldin, Andrea P. and Pezzatti, Laura and Battro, Antonio M. and Sigman, Mariano},
doi = {10.1111/j.1751-228X.2011.01126.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldin et al. - 2011 - From ancient Greece to modern education Universality and lack of generalization of the socratic dialogue.pdf:pdf},
isbn = {17512271 (ISSN)},
issn = {17512271},
journal = {Mind, Brain, and Education},
number = {4},
pages = {180--185},
title = {{From ancient Greece to modern education: Universality and lack of generalization of the socratic dialogue}},
volume = {5},
year = {2011}
}
@article{Bennett2011,
author = {Bennett, Craig M and Baird, Abigail a and Miller, Michael B and Wolford, George L},
doi = {10.1016/S1053-8119(09)71202-9},
isbn = {1053-8119},
issn = {10538119},
journal = {Journal of Serendipitous and Unexpected Results},
pages = {1--5},
pmid = {20060480},
title = {{Neural Correlates of Interspecies Perspective Taking in the Post-Mortem Atlantic Salmon: An Argument For Proper Multiple Comparisons Correction}},
volume = {1},
year = {2011}
}
@article{DeSmedt2011,
abstract = {Most studies on mathematics learning in the field of educational neuroscience have focused on the neural correlates of very elementary numerical processing skills in children. Little is known about more complex mathematical skills that are formally taught in school, such as arithmetic. Using functional magnetic resonance imaging, the present study investigated how brain activation during single-digit addition and subtraction is modulated by problem size and arithmetic operation in 28 children aged 10-12. years with different levels of arithmetical fluency. Commensurate with adult data, large problems and subtractions activated a fronto-parietal network, including the intraparietal sulci, the latter of which indicates the influence of quantity-based processes during procedural strategy execution. Different from adults, the present findings revealed that particularly the left hippocampus was active during the solution of those problems that are expected to be solved by means of fact retrieval (i.e. small problems and addition), suggesting a specific role of the hippocampus in the early stages of learning arithmetic facts. Children with low levels of arithmetical fluency showed higher activation in the right intraparietal sulcus during the solution of problems with a relatively small problem size, indicating that they continued to rely to a greater extent on quantity-based strategies on those problems that the children with relatively higher arithmetical fluency already retrieved from memory. This might represent a neural correlate of fact retrieval impairments in children with mathematical difficulties. ?? 2010 Elsevier Inc.},
author = {{De Smedt}, Bert and Holloway, Ian D. and Ansari, Daniel},
doi = {10.1016/j.neuroimage.2010.12.037},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Smedt, Holloway, Ansari - 2011 - Effects of problem size and arithmetic operation on brain activation during calculation in children.pdf:pdf},
isbn = {1053-8119;},
issn = {10538119},
journal = {NeuroImage},
keywords = {Arithmetic fluency,Fact retrieval,Hippocampus,Intraparietal sulcus,Problem size effect,Procedural strategies},
number = {3},
pages = {771--781},
pmid = {21182966},
publisher = {Elsevier Inc.},
title = {{Effects of problem size and arithmetic operation on brain activation during calculation in children with varying levels of arithmetical fluency}},
volume = {57},
year = {2011}
}
@article{Desco2011,
abstract = {NeuroImage, 57 (2011) 281-292. doi:10.1016/j.neuroimage.2011.03.063},
author = {Desco, M and Navas-Sanchez, F J and Sanchez-Gonz{\'{a}}lez, J and Reig, S and Robles, O and Franco, C and Guzm{\'{a}}n-De-Villoria, J a and Garc{\'{i}}a-Barreno, P and Arango, C},
doi = {10.1016/j.neuroimage.2011.03.063},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Desco et al. - 2011 - Mathematically gifted adolescents use more extensive and more bilateral areas of the fronto-parietal network than.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$r1053-8119 (Linking)},
issn = {1053-8119},
journal = {NeuroImage},
keywords = {Complexity,Intelligence,Raven matrices,Tower of London,Visuospatial,Working memory,fMRI},
number = {1},
pages = {281--292},
pmid = {21463696},
publisher = {Elsevier Inc.},
title = {{Mathematically gifted adolescents use more extensive and more bilateral areas of the fronto-parietal network than controls during executive functioning and fluid reasoning tasks}},
volume = {57},
year = {2011}
}
@article{DeBock2011,
abstract = {Kaminski, Sloutsky, and Heckler (2008a) published in Science a study on "The advantage of abstract examples in learning math," in which they claim that students may benefit more from learning mathematics through a single abstract, symbolic representation than from multiple concrete examples. This publication elicited both enthusiastic and critical comments by mathematicians, mathematics educators, and policymakers worldwide. The current empirical study involves a partial replication-but also an important validation and extension-of this widely noticed study.},
author = {{De Bock}, Dirk and Deprez, Johan and {Van Dooren}, Wim and Roelens, Michel and Verschaffel, Lieven},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Bock et al. - 2011 - Abstract or Concrete Examples in Learning Mathematics A Replication and Elaboration of Kaminski, Sloutsky, and H.pdf:pdf},
journal = {Journal for Research in Mathematics Education},
keywords = {01,2006,algebra,college,developing adap-,education,from the research fund,k,learning,leuven,research issues,supported by grant goa,testing,this research was partially,tive expertise in mathematics,u,university},
number = {2},
pages = {109},
title = {{Abstract or Concrete Examples in Learning Mathematics? A Replication and Elaboration of Kaminski, Sloutsky, and Heckler's Study}},
volume = {42},
year = {2011}
}
@article{Kennedy2011,
author = {Kennedy, Bill},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy - 2011 - Notes on teaching ACT-R modeling.pdf:pdf},
number = {September},
title = {{Notes on teaching ACT-R modeling}},
year = {2011}
}
@article{Brochu2010,
abstract = {Eurographics/ ACM SIGGRAPH Symposium on Computer Animation (2010)},
author = {Brochu, Eric and Brochu, Tyson and Freitas, Nando De},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Brochu, Freitas - 2010 - A Bayesian Interactive Optimization Approach to Procedural Animation Design.pdf:pdf},
journal = {Symposium on Computer Animation},
pages = {103--12},
title = {{A Bayesian Interactive Optimization Approach to Procedural Animation Design}},
year = {2010}
}
@article{Parthemore2010,
author = {Parthemore, Joel and Morse, Anthony F},
doi = {10.1075/p},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parthemore, Morse - 2010 - Representations reclaimed Accounting for the co-emergence of concepts and experience.pdf:pdf},
journal = {Pragmatics {\&} Cognition},
keywords = {concept,conceptual spaces,enaction,mental representation,representation,sensorimotor,sensorimotor profile,symbol},
number = {2},
pages = {273--312},
title = {{Representations reclaimed: Accounting for the co-emergence of concepts and experience}},
volume = {18},
year = {2010}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
isbn = {9781937284275},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
volume = {9},
year = {2010}
}
@article{Gentner2010,
author = {Gentner, Dedre},
doi = {10.1111/j.1551-6709.2010.01114.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gentner - 2010 - Bootstrapping the Mind Analogical Processes and Symbol Systems.pdf:pdf},
keywords = {analogical learning,cognitive development,language and cogni-,structure-mapping},
pages = {752--775},
title = {{Bootstrapping the Mind : Analogical Processes and Symbol Systems}},
volume = {34},
year = {2010}
}
@article{Dweck2010,
author = {Dweck, Carol S},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dweck - 2010 - Mind-Sets and Equitable Education.pdf:pdf},
journal = {Principal Leadership},
number = {5},
pages = {26--29},
title = {{Mind-Sets and Equitable Education}},
volume = {10},
year = {2010}
}
@article{Atwood2010,
abstract = {The belief that a trait can be cultivated with effort, known as an incremental theory or growth mindset, promotes behavior that leads to higher levels of achievement, such as the enthusiastic embrace of challenges and resilience to obstacles. Roughly 40{\%} of the general student population in the United States, however, conceptualizes intelligence as an innate and immutable trait, a belief that tends to inhibit motivation and learning. To better inculcate an incremental theory of intelligence, educators and psychologists should identity traits that a majority of students believe are malleable, and investigate the dynamics that facilitate optimism about their developmental potential. In service to this end, the present study illuminates a bifurcation of both belief and behavior related to student engagement in the domains of school and sport. A survey of 251 middle school students confirmed two hypotheses: individuals are significantly more likely (a) to have a growth mindset of athletic ability compared to intelligence, and (b) to exhibit mastery-oriented responses in athletic versus academic environments. The organizational infrastructure of athletic programs, which institutionalizes practice, emphasizes effort, and values the coach as a developmental expert, is thought to powerfully cultivate the idea of athletic ability as a malleable traitand offers clues about how to design educational interventions that increase the number of students who believe intelligence is something they can improve with effort.},
author = {Atwood, Jason R},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Atwood - 2010 - Mindset, Motivation and Metaphor in School and Sport Bifurcated beliefs and behaviour in two different achievement domai.pdf:pdf},
journal = {American Education Research Association},
pages = {1--30},
title = {{Mindset, Motivation and Metaphor in School and Sport: Bifurcated beliefs and behaviour in two different achievement domains}},
year = {2010}
}
@article{Sanborn2010,
author = {Sanborn, Adam N. and Griffiths, Thomas L. and Navarro, Daniel J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanborn, Griffiths, Navarro - 2010 - Rational Approximations to Category Learning.pdf:pdf},
journal = {Psychological Review},
number = {4},
pages = {1144--1167},
title = {{Rational Approximations to Category Learning}},
volume = {117},
year = {2010}
}
@article{Landy2010,
abstract = {How does the physical structure of an arithmetic expression affect the computational processes engaged in by reasoners? In handwritten arithmetic expressions containing both multiplications and additions, terms that are multiplied are often placed physically closer together than terms that are added. Three experiments evaluate the role such physical factors play in how reasoners construct solutions to simple compound arithmetic expressions (such as "2 + 3 × 4"). Two kinds of influence are found: First, reasoners incorporate the physical size of the expression into numerical responses, tending to give larger responses to more widely spaced problems. Second, reasoners use spatial information as a cue to hierarchical expression structure: More narrowly spaced subproblems within an expression tend to be solved first and tend to be multiplied. Although spatial relationships besides order are entirely formally irrelevant to expression semantics, reasoners systematically use these relationships to support their success with various formal properties.},
author = {Landy, David and Goldstone, Robert L},
doi = {10.1080/17470211003787619},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Landy, Goldstone - 2010 - Proximity and precedence in arithmetic.pdf:pdf},
isbn = {1747-0226 (Electronic)$\backslash$n1747-0218 (Linking)},
issn = {1747-0218},
journal = {The Quarterly Journal of Experimental Psychology},
keywords = {challenges facing the cognitive,embodied cognition,how do people use,interpretation,is symbolic,mathematical cognition,one of the central,study of mathematical reasoning,symbol systems,symbolic reasoning},
number = {10},
pages = {1953--1968},
pmid = {20509096},
title = {{Proximity and precedence in arithmetic.}},
volume = {63},
year = {2010}
}
@article{Mood2010,
abstract = {Logistic regression estimates do not behave like linear regression estimates in one important respect: They are affected by omitted variables, even when these variables are unrelated to the independent variables in the model. This fact has important implications that have gone largely unnoticed by sociologists. Importantly, we cannot straightforwardly interpret log-odds ratios or odds ratios as effect measures, because they also reflect the degree of unobserved heterogeneity in the model. In addition, we cannot compare log-odds ratios or odds ratios for similar models across groups, samples, or time points, or across models with different independent variables in a sample. This article discusses these problems and possible ways of overcoming them. [ABSTRACT FROM PUBLISHER] Copyright of European Sociological Review is the property of Oxford University Press / UK and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
author = {Mood, Carina},
doi = {10.1093/esr/jcp006},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mood - 2010 - Logistic regression Why we cannot do what We think we can do, and what we can do about it.pdf:pdf},
isbn = {02667215},
issn = {02667215},
journal = {European Sociological Review},
number = {1},
pages = {67--82},
pmid = {48014827},
title = {{Logistic regression: Why we cannot do what We think we can do, and what we can do about it}},
volume = {26},
year = {2010}
}
@article{Riener2010,
abstract = {There is no credible evidence that learning styles exist. While we will elaborate on this assertion, it is important to counteract the real harm that may be done by equivocating on the matter. In what follows, we will begin by defining “learning styles”; then we will address the claims made by those who believe that they exist, in the process acknowledging what we consider the valid claims of learning-styles theorists. But in separating the wheat from the pseudoscientific chaff in learning-styles theory, we will make clear that the wheat is contained in other educational approaches as well. A belief in learning styles is not necessary to incorporating useful knowledge about learning into one's teaching. We will then discuss the reasons why learning styles beliefs are so prevalent. Finally, we will offer suggestions about collegiate pedagogy, given that we have no evidence learning styles do not exist.},
author = {Riener, Cedar and Willingham, Daniel},
doi = {10.1080/00091383.2010.503139},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riener, Willingham - 2010 - The Myth of Learning Styles.pdf:pdf},
isbn = {00091383},
issn = {0009-1383},
journal = {Change: The Magazine of Higher Learning},
number = {5},
pages = {32--35},
pmid = {53306479},
title = {{The Myth of Learning Styles}},
volume = {42},
year = {2010}
}
@article{Mcclelland2010,
author = {Mcclelland, James L and Botvinick, Matthew M and Noelle, David C and Plaut, David C and Rogers, Timothy T and Seidenberg, Mark S and Smith, Linda B},
doi = {10.1016/j.tics.2010.06.002},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcclelland et al. - 2010 - Letting structure emerge connectionist and dynamical systems approaches to cognition.pdf:pdf},
issn = {1364-6613},
journal = {Trends in Cognitive Sciences},
number = {8},
pages = {348--356},
publisher = {Elsevier Ltd},
title = {{Letting structure emerge : connectionist and dynamical systems approaches to cognition}},
volume = {14},
year = {2010}
}
@article{Mcclelland2010a,
abstract = {The study of human intelligence was once dominated by symbolic approaches, but over the last 30 years an alternative approach has arisen. Symbols and processes that operate on them are often seen today as approximate characterizations of the emergent consequences of sub- or nonsymbolic processes, and a wide range of constructs in cognitive science can be understood as emergents. These include representational constructs (units, structures, rules), architectural constructs (central execu- tive, declarative memory), and developmental processes and outcomes (stages, sensitive periods, neurocognitive modules, developmental disorders). The greatest achievements of human cognition may be largely emergent phenomena. It remains a challenge for the future to learn more about how these greatest achievements arise and to emulate them in artificial systems.},
author = {Mcclelland, James L.},
doi = {10.1111/j.1756-8765.2010.01116.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcclelland - 2010 - Emergence in Cognitive Science.pdf:pdf},
isbn = {1756-8765},
issn = {17568757},
journal = {Topics in Cognitive Science},
keywords = {Development,Emergence,Explanation,History,Language,Modeling,Neural networks},
pages = {751--770},
title = {{Emergence in Cognitive Science}},
volume = {2},
year = {2010}
}
@article{Griffiths2010,
abstract = {Cognitive science aims to reverse-engineer the mind, and many of the engineering challenges the mind faces involve inductive inference. The probabilistic approach to modeling cognition begins with the goal of understanding these inductive problems in computational terms: what makes them difficult, and how they can be solved in principle. Mental processes are then modeled using algorithms for approximately implementing these ideal solutions, and neural processes are viewed as mechanisms for implementing these algorithms in biological hardware. This top-down approach is analogous to historical progressions of theory building in other natural sciences, moving from macro-level functional explanations of observable phenomena to micro-level mechanistic accounts. Typical connectionist models, by contrast, follow a bottom-up approach, starting from an abstract characterization of neural mechanisms and exploring what macro-level functional phenomena might emerge from those mechanisms. We suggest that the top-down approach is likely to yield more rapid progress towards understanding human inductive inference.},
author = {Griffiths, T L and Chater, N and Kemp, C and Perfors, A and Tenenbaum, J and Griffiths, T},
issn = {13646613},
journal = {Trends in},
pages = {357--364},
title = {{Probabilistic models of cognition: Exploring the laws of thought}},
volume = {14},
year = {2010}
}
@article{Ram2009,
abstract = {Growth mixture modeling (GMM) is a method for identifying multiple unobserved sub-populations, describing longitudinal change within each unobserved sub-population, and examining differences in change among unobserved sub-populations. We provide a practical primer that may be useful for researchers beginning to incorporate GMM analysis into their research. We briefly review basic elements of the standard latent basis growth curve model, introduce GMM as an extension of multiple-group growth modeling, and describe a four-step approach to conducting a GMM analysis. Example data from a cortisol stress-response paradigm are used to illustrate the suggested procedures. },
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ram, Nilam and Grimm, Kevin J.},
doi = {10.1177/0165025409343765},
eprint = {NIHMS150003},
file = {:home/andrew/Documents/grad/Papers/0165025409343765.pdf:pdf},
isbn = {0165025409},
issn = {01650254},
journal = {International Journal of Behavioral Development},
keywords = {Aging,Cortisol,Development,Dynamic process,Growth mixture modeling,Latent growth,Longitudinal methods},
number = {6},
pages = {565--576},
pmid = {23885133},
title = {{Methods and Measures: Growth mixture modeling: A method for identifying differences in longitudinal change among unobserved groups}},
volume = {33},
year = {2009}
}
@article{Botvinick2009,
abstract = {Research on human and animal behavior has long emphasized its hierarchical structure-the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
doi = {10.1016/j.cognition.2008.08.011},
file = {:home/andrew/Documents/grad/Papers/BotvinickEtAl09Hierarchically.pdf:pdf},
isbn = {1873-7838 (Electronic)$\backslash$n0010-0277 (Linking)},
issn = {00100277},
journal = {Cognition},
keywords = {Prefrontal cortex,Reinforcement learning},
number = {3},
pages = {262--280},
pmid = {18926527},
publisher = {Elsevier B.V.},
title = {{Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective}},
url = {http://dx.doi.org/10.1016/j.cognition.2008.08.011},
volume = {113},
year = {2009}
}
@article{Price2009,
abstract = {Examinations of the cognitive neuroscience of category learning frequently rely on probabilistic classification-learning tasks-namely, the weather prediction task (WPT)-to study the neural mechanisms of implicit learning. Accumulating evidence suggests that the task also depends on explicit-learning processes. The present investigation manipulated the WPT to assess the specific contributions of implicit- and explicit-learning processes to performance, with a particular focus on how the contributions of these processes change as the task progresses. In Experiment 1, a manipulation designed to disrupt implicit-learning processes had no effect on classification accuracy or the distribution of individual response strategies. In Experiment 2, by contrast, a manipulation designed to disrupt explicit-learning processes substantially reduced classification accuracy and reduced the number of participants who relied on a correct response strategy. The present findings suggest that WPT learning is not an effective tool for investigating nondeclarative learning processes.},
author = {Price, Amanda L.},
doi = {10.3758/MC.37.2.210},
file = {:home/andrew/Documents/grad/Papers/weatherprediction{\_}Explicit.pdf:pdf},
isbn = {0090-502X (Print)},
issn = {0090502X},
journal = {Memory and Cognition},
number = {2},
pages = {210--222},
pmid = {19223570},
title = {{Distinguishing the contributions of implicit and explicit processes to performance of the weather prediction task}},
volume = {37},
year = {2009}
}
@article{Mundy2009,
author = {Mundy, Peter and Sullivan, Lisa and Mastergeorge, Ann M},
doi = {10.1002/aur.61},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mundy, Sullivan, Mastergeorge - 2009 - A Parallel and Distributed-Processing Model of Joint Attention, Social Cognition and Autism.pdf:pdf},
journal = {Autism Research},
keywords = {early development,neural connectivity,social symptoms},
number = {1},
pages = {2--21},
title = {{A Parallel and Distributed-Processing Model of Joint Attention, Social Cognition and Autism}},
volume = {2},
year = {2009}
}
@article{Ostrovsky2009,
abstract = {How the visual system comes to bind diverse image regions into whole objects is not well understood. We recently had a unique opportunity to investigate this issue when we met three congenitally blind individuals in India. After providing them treatment, we studied the early stages of their visual skills. We found that prominent figural cues of grouping, such as good continuation and junction structure, were largely ineffective for image parsing. By contrast, motion cues were of profound significance in that they enabled intraobject integration and facilitated the development of object representations that permitted recognition in static images. Following 10 to 18 months of visual experience, the individuals' performance improved, and they were able to use the previously ineffective static figural cues to correctly parse many static scenes. These results suggest that motion information plays a fundamental role in organizing early visual experience and that parsing skills can be acquired even late in life.},
author = {Ostrovsky, Yuri and Meyers, Ethan and Ganesh, Suma and Mathur, Umang and Sinha, Pawan},
doi = {10.1111/j.1467-9280.2009.02471.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ostrovsky et al. - 2009 - Visual parsing after recovery from blindness.pdf:pdf},
isbn = {0956-7976},
issn = {09567976},
journal = {Psychological Science},
number = {12},
pages = {1484--1491},
pmid = {19891751},
title = {{Visual parsing after recovery from blindness}},
volume = {20},
year = {2009}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, R. and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}
@article{Bengio2009,
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illus- trates gradually more concepts, and gradu- ally more complex ones. Here, we formal- ize such training strategies in the context of machine learning, and call them “curricu- lum learning”. In the context of recent re- search studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neu- ral networks), we explore curriculum learn- ing in various set-ups. The experiments show that significant improvements in generaliza- tion can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
author = {Bengio, Yoshua and Louradour, J{\'{e}}r{\^{o}}me and Collobert, Ronan and Weston, Jason},
doi = {10.1145/1553374.1553380},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - 2009 - Curriculum learning.pdf:pdf},
isbn = {9781605585161},
issn = {0022-5193},
journal = {Proceedings of the 26th annual international conference on machine learning},
pages = {41--48},
pmid = {5414602},
title = {{Curriculum learning}},
year = {2009}
}
@article{Kaminski2009,
abstract = {What factors affect transfer of knowledge is a complex question. In recent research, the authors demonstrated that concreteness of the learning domain is one such factor (Kaminski, Sloutsky, {\&} Heckler, 2008). Even when prompted and given no time delay, participants who learned a concrete instantiation of a mathematical concept failed to transfer their knowledge to a novel analogous situation.},
author = {Kaminski, Jennifer a. and Sloutsky, Vladimir M. and Heckler, Andrew F.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaminski, Sloutsky, Heckler - 2009 - Concrete Instantiations of Mathematics A Double-Edged Sword.html:html},
isbn = {0021-8251},
journal = {Journal for Research in Mathematics Education},
pages = {90--93},
title = {{Concrete Instantiations of Mathematics: A Double-Edged Sword}},
volume = {40},
year = {2009}
}
@article{Jones2009,
abstract = {In this article, I respond to the recent work of Kaminski, Sloutsky, and Heckler (2008a). I advance two major concerns about their research and its applicability to learning mathematics: a confounding variable that arises from the mathematical differences between the generic examples and concrete examples poses a threat to the construct validity of the experiments, and the overgeneralization of the success of the treatment, given that the measure of success is a prompted near-transfer task.},
author = {Jones, Matthew G},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones - 2009 - Transfer, Abstraction, and Context.pdf:pdf},
isbn = {0021-8251},
issn = {0021-8251},
journal = {Journal for Research in Mathematics Education},
keywords = {Abstraction,Transfer},
number = {2},
pages = {80--89},
title = {{Transfer, Abstraction, and Context}},
volume = {40},
year = {2009}
}
@article{Grabner2009,
author = {Grabner, Roland H. and Ansari, Daniel and Koschutnig, Karl and Reishofer, Gernot and Ebner, Franz and Neuper, Christa},
doi = {10.1016/j.neuropsychologia.2008.10.013},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grabner et al. - 2009 - To retrieve or to calculate Left angular gyrus mediates the retrieval of arithmetic facts during problem solving.pdf:pdf},
issn = {00283932},
journal = {Neuropsychologia},
number = {2},
pages = {604--608},
title = {{To retrieve or to calculate? Left angular gyrus mediates the retrieval of arithmetic facts during problem solving}},
volume = {47},
year = {2009}
}
@article{Davis2009,
abstract = {Most studies investigating mental numerical processing involve adult participants and little is known about the functioning of these systems in children. The current study used functional magnetic resonance imaging (fMRI) to investigate the neural correlates of numeracy and the influence of age on these correlates with a group of adults and a group of third graders who had average to above average mathematical ability. Participants performed simple and complex versions of exact and approximate calculation tasks while in the magnet. Like adults, children activated a network of brain regions in the frontal and parietal lobes during the calculation tasks, and they recruited additional brain regions for the more complex versions of the tasks. However, direct comparisons between adults and children revealed significant differences in level of activation across all tasks. In particular, patterns of activation in the parietal lobe were significantly different as a function of age. Findings support previous claims that the parietal lobe becomes more specialized for arithmetic tasks with age.},
author = {Davis, Nicole and Cannistraci, Christopher J. and Rogers, Baxter P. and Gatenby, J. Christopher and Fuchs, Lynn S. and Anderson, Adam W. and Gore, John C.},
doi = {10.1016/j.mri.2009.05.010},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis et al. - 2009 - The neural correlates of calculation ability in children an fMRI study.pdf:pdf},
isbn = {1873-5894 (Electronic)$\backslash$r1873-5894 (Linking)},
issn = {0730725X},
journal = {Magnetic Resonance Imaging},
keywords = {Arithmetic,Mathematical skill,Numerical processing,School-age,fMRI},
number = {9},
pages = {1187--1197},
pmid = {19570639},
publisher = {Elsevier B.V.},
title = {{The neural correlates of calculation ability in children: an fMRI study}},
volume = {27},
year = {2009}
}
@article{Moser2008,
author = {Moser, Edvard I and Kropff, Emilio and Moser, May-britt},
doi = {10.1146/annurev.neuro.31.061307.090723},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moser, Kropff, Moser - 2008 - Place Cells, Grid Cells, and the Brain's Spatial Representation System.pdf:pdf},
journal = {Annual Review of Neuroscience},
keywords = {attractor,entorhinal cortex,hippocampus,memory,path integration},
pages = {69--89},
title = {{Place Cells, Grid Cells, and the Brain's Spatial Representation System}},
volume = {31},
year = {2008}
}
@article{Rogers2008a,
author = {Rogers, Timothy T and McClelland, James L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rogers, McClelland - 2008 - A simple model from a powerful framework that spans levels of analysis.pdf:pdf},
journal = {Behavioral and Brain Sciences},
pages = {729--750},
title = {{A simple model from a powerful framework that spans levels of analysis}},
volume = {31},
year = {2008}
}
@article{Penn2008,
abstract = {Over the last quarter century, the dominant tendency in comparative cognitive psychology has been to emphasize the similarities between human and nonhuman minds and to downplay the differences as "one of degree and not of kind" (Darwin 1871). In the present target article, we argue that Darwin was mistaken: the profound biological continuity between human and nonhuman animals masks an equally profound discontinuity between human and nonhuman minds. To wit, there is a significant discontinuity in the degree to which human and nonhuman animals are able to approximate the higher-order, systematic, relational capabilities of a physical symbol system (PSS) (Newell 1980). We show that this symbolic-relational discontinuity pervades nearly every domain of cognition and runs much deeper than even the spectacular scaffolding provided by language or culture alone can explain. We propose a representational-level specification as to where human and nonhuman animals' abilities to approximate a PSS are similar and where they differ. We conclude by suggesting that recent symbolic-connectionist models of cognition shed new light on the mechanisms that underlie the gap between human and nonhuman minds.},
author = {Penn, Derek C and Holyoak, Keith J and Povinelli, Daniel J},
doi = {10.1017/S0140525X08003543},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Penn, Holyoak, Povinelli - 2008 - Darwin's mistake explaining the discontinuity between human and nonhuman minds.pdf:pdf},
isbn = {0140-525X},
issn = {1469-1825},
journal = {The Behavioral and brain sciences},
keywords = {Animals,Brain,Brain: physiology,Cognition,Cognition: physiology,Evolution,Humans,Pan troglodytes,Pan troglodytes: physiology,Perception,Space Perception,Species Specificity,Symbolism,Wild,Wild: physiology},
number = {2},
pages = {109--30; discussion 130--178},
pmid = {18479531},
title = {{Darwin's mistake: explaining the discontinuity between human and nonhuman minds.}},
volume = {31},
year = {2008}
}
@article{Dweck2008,
abstract = {There is a growing body of evidence that students mindsets play a key role in their math and science achievement. Students who believe that intelligence or math and sci- ence ability is simply a fixed trait (a fixed mindset) are at a significant disadvantage com- pared to students who believe that their abilities can be developed (a growth mindset). Moreover, research is showing that these mindsets can play an important role in the relative underachievement of women and minorities in math and science. Below, I will present research showing that a) mindsets can predict math/science achievement over time; b) mindsets can contribute to math/science achievement discrepancies for women and minorities; c) interventions that change mindsets can boost achievement and reduce achievement discrepancies; and d) educators play a key role in shaping students mindsets.},
author = {Dweck, Carol S},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dweck - 2008 - Mindsets and Math Science Achievement.pdf:pdf},
journal = {The Opportunity Equation: Transforming Mathematics and Science Education for Citizenship and the Global Economy},
pages = {1--17},
title = {{Mindsets and Math / Science Achievement}},
year = {2008}
}
@book{Dweck2008a,
author = {Dweck, Carol S},
title = {{Mindset: The new psychology of success}},
year = {2008}
}
@article{Rogers2008,
author = {Rogers, Timothy T and McClelland, James L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rogers, McClelland - 2008 - Precis of Semantic Cognition A Parallel Distributed Processing Approach.pdf:pdf},
journal = {Behavioral and Brain Sciences},
keywords = {categorization,causal knowledge,concepts,connectionism,development,innateness,learning,memory,semantics},
pages = {689--749},
title = {{Precis of Semantic Cognition : A Parallel Distributed Processing Approach}},
volume = {31},
year = {2008}
}
@article{Sanborn2008,
abstract = {Many formal models of cognition implicitly use subjective probability distribu- tions to capture the assumptions of human learners. Most applications of these models determine these distributions indirectly. We propose a method for directly determining the assumptions of human learners by sampling from subjective prob- ability distributions. Using a correspondence between a model of human choice and Markov chain Monte Carlo (MCMC), we describe a method for sampling from the distributions over objects that people associate with different categories. In our task, subjects choose whether to accept or reject a proposed change to an object. The task is constructed so that these decisions follow an MCMC accep- tance rule, defining a Markov chain for which the stationary distribution is the category distribution. We test this procedure for both artificial categories acquired in the laboratory, and natural categories acquired from experience.},
author = {Sanborn, Adam N and Griffiths, Thomas L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanborn, Griffiths - 2008 - Markov Chain Monte Carlo with People.pdf:pdf},
journal = {Advances in neural information processing systems},
number = {47045},
pages = {1--8},
pmid = {5036717154477609753},
title = {{Markov Chain Monte Carlo with People}},
volume = {20},
year = {2008}
}
@article{Morishita2008,
abstract = {Neural circuits are shaped by experience in early postnatal life. The permanent loss of visual acuity (amblyopia) and anatomical remodeling within primary visual cortex following monocular deprivation is a classic example of critical period development from mouse to man. Recent work in rodents reveals a residual subthreshold potentiation of open eye response throughout life. Resetting excitatory-inhibitory balance or removing molecular 'brakes' on structural plasticity may unmask the potential for recovery of function in adulthood. Novel pharmacological or environmental interventions now hold great therapeutic promise based on a deeper understanding of critical period mechanisms. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Morishita, Hirofumi and Hensch, Takao K.},
doi = {10.1016/j.conb.2008.05.009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morishita, Hensch - 2008 - Critical period revisited impact on vision.pdf:pdf},
isbn = {0959-4388 (Print)$\backslash$n0959-4388 (Linking)},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {1},
pages = {101--107},
pmid = {18534841},
title = {{Critical period revisited: impact on vision}},
volume = {18},
year = {2008}
}
@article{Doumas2008,
abstract = {Relational thinking plays a central role in human cognition. However, it is not known how children and adults acquire relational concepts and come to represent them in a form that is useful for the purposes of relational thinking (i.e., as structures that can be dynamically bound to arguments). The authors present a theory of how a psychologically and neurally plausible cognitive architecture can discover relational concepts from examples and represent them as explicit structures (predicates) that can take arguments (i.e., predicate them). The theory is instantiated as a computer program called DORA (Discovery Of Relations by Analogy). DORA is used to simulate the discovery of novel properties and relations, as well as a body of empirical phenomena from the domain of relational learning and the development of relational representations in children and adults.},
author = {Doumas, Leonidas A. A. and Hummel, John E and Sandhofer, Catherine M},
doi = {10.1037/0033-295X.115.1.1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doumas, Hummel, Sandhofer - 2008 - A theory of the discovery and predication of relational concepts.pdf:pdf},
isbn = {0033-295X$\backslash$r1939-1471},
issn = {0033-295X},
journal = {Psychological review},
keywords = {cognitive develop-,learning relations,learning structured representations,relation discovery},
number = {1},
pages = {1--43},
pmid = {18211183},
title = {{A theory of the discovery and predication of relational concepts.}},
volume = {115},
year = {2008}
}
@article{Blair2008,
abstract = {ABSTRACT — This article examines the role of working memo- ry, attention shifting, and inhibitory control executive cogni- tive functions in the development of mathematics knowledge and ability in children. It suggests that an examination of the executive cognitive demand of mathematical thinking can complement procedural and conceptual knowledge-based approaches to understanding the ways in which children become profi cient in mathematics. Task analysis indicates that executive cognitive functions likely operate in concert with procedural and conceptual knowledge and in some instances might act as a unique infl uence on mathematics problem-solving ability. It is concluded that consideration of the executive cognitive demand of mathematics can contribute to research on best practices in mathematics education.},
author = {Blair, Clancy and Knipe, Hilary and Gamson, David},
doi = {10.1111/j.1751-228X.2008.00036.x},
isbn = {1751-228X},
issn = {17512271},
journal = {Mind, Brain, and Education},
number = {2},
pages = {80--89},
title = {{Is there a role for executive functions in the development of mathematics ability?}},
volume = {2},
year = {2008}
}
@misc{Kohen2008,
abstract = {The present study used Canadian National Longitudinal data to examine a model of the mechanisms through which the effects of neighborhood socioeconomic conditions impact young children's verbal and behavioral outcomes (N 5 3,528; M age 5 5.05 years, SD 5 0.86). Integrating elements of social disorganization theory and family stress models, and results from structural equation models suggest that both neighborhood and family mechanisms played an important role in the transmission of neighborhood socioeconomic effects. Neighborhood disadvantage manifested its effect via lower neighborhood cohesion, which was associated with maternal depression and family dysfunction. These processes were, in turn, related to less consistent, less stimulating, and more punitive parenting behaviors, and ultimately, poorer child outcomes.},
author = {Kohen, D. and Leventhal, T. and Dahinten, V.S. and McIntosh, C.},
booktitle = {Child Development},
doi = {10.1002/14651858.CD006061.pub2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohen et al. - 2008 - Pdf @ Onlinelibrary.Wiley.Com.html:html},
isbn = {92 832 0405 0},
issn = {00368326},
number = {1},
pages = {156--169},
pmid = {21633523},
title = {{Pdf @ Onlinelibrary.Wiley.Com}},
volume = {79},
year = {2008}
}
@article{Pashler2008,
abstract = {The term “learning styles” refers to the concept that individuals differ in regard to what mode of instruction or study is most effective for them. Proponents of learning-style assessment contend that optimal instruction requires diagnosing individuals' learning style and tailoring instruction accordingly. Assessments of learning style typically ask people to evaluate what sort of information presentation they prefer (e.g., words versus pictures versus speech) and/or what kind of mental activity they find most engaging or congenial (e.g., analysis versus listening), although assessment instruments are extremely diverse. The most common—but not the only—hypothesis about the instructional relevance of learning styles is the meshing hypothesis, according to which instruction is best provided in a format that matches the preferences of the learner (e.g., for a “visual learner,” emphasizing visual presentation of information).The learning-styles view has acquired great influence within the education field, and is frequently encountered at levels ranging from kindergarten to graduate school. There is a thriving industry devoted to publishing learning-styles tests and guidebooks for teachers, and many organizations offer professional development workshops for teachers and educators built around the concept of learning styles.The authors of the present review were charged with determining whether these practices are supported by scientific evidence. We concluded that any credible validation of learning-styles-based instruction requires robust documentation of a very particular type of experimental finding with several necessary criteria. First, students must be divided into groups on the basis of their learning styles, and then students from each group must be randomly assigned to receive one of multiple instructional methods. Next, students must then sit for a final test that is the same for all students. Finally, in order to demonstrate that optimal learning requires that students receive instruction tailored to their putative learning style, the experiment must reveal a specific type of interaction between learning style and instructional method: Students with one learning style achieve the best educational outcome when given an instructional method that differs from the instructional method producing the best outcome for students with a different learning style. In other words, the instructional method that proves most effective for students with one learning style is not the most effective method for students with a different learning style.Our review of the literature disclosed ample evidence that children and adults will, if asked, express preferences about how they prefer information to be presented to them. There is also plentiful evidence arguing that people differ in the degree to which they have some fairly specific aptitudes for different kinds of thinking and for processing different types of information. However, we found virtually no evidence for the interaction pattern mentioned above, which was judged to be a precondition for validating the educational applications of learning styles. Although the literature on learning styles is enormous, very few studies have even used an experimental methodology capable of testing the validity of learning styles applied to education. Moreover, of those that did use an appropriate method, several found results that flatly contradict the popular meshing hypothesis.We conclude therefore, that at present, there is no adequate evidence base to justify incorporating learning-styles assessments into general educational practice. Thus, limited education resources would better be devoted to adopting other educational practices that have a strong evidence base, of which there are an increasing number. However, given the lack of methodologically sound studies of learning styles, it would be an error to conclude that all possible versions of learning styles have been tested and found wanting; many have simply not been tested at all. Further research on the use of learning styles assessment in instruction may in some cases be warranted, but such research needs to be performed appropriately.},
author = {Pashler, Harold and McDaniel, Mark and Rohrer, Doug and Bjork, Robert},
doi = {10.1111/j.1539-6053.2009.01038.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pashler et al. - 2008 - Learning Styles Concepts and Evidence.pdf:pdf},
isbn = {1539-6053},
issn = {15291006},
journal = {Psychological Science in the Public Interest},
number = {3},
pages = {105--119},
pmid = {21197874},
title = {{Learning Styles: Concepts and Evidence}},
volume = {9},
year = {2008}
}
@article{Blair2008,
abstract = {ABSTRACT — This article examines the role of working memo- ry, attention shifting, and inhibitory control executive cogni- tive functions in the development of mathematics knowledge and ability in children. It suggests that an examination of the executive cognitive demand of mathematical thinking can complement procedural and conceptual knowledge-based approaches to understanding the ways in which children become profi cient in mathematics. Task analysis indicates that executive cognitive functions likely operate in concert with procedural and conceptual knowledge and in some instances might act as a unique infl uence on mathematics problem-solving ability. It is concluded that consideration of the executive cognitive demand of mathematics can contribute to research on best practices in mathematics education.},
author = {Blair, Clancy and Knipe, Hilary and Gamson, David},
doi = {10.1111/j.1751-228X.2008.00036.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blair, Knipe, Gamson - 2008 - Is there a role for executive functions in the development of mathematics ability.pdf:pdf},
isbn = {1751-228X},
issn = {17512271},
journal = {Mind, Brain, and Education},
number = {2},
pages = {80--89},
title = {{Is there a role for executive functions in the development of mathematics ability?}},
volume = {2},
year = {2008}
}
@article{OBoyle2008,
annote = {Such arrogance about aptitude},
author = {O'Boyle, Michael W.},
doi = {10.1080/02783190802199594},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Boyle - 2008 - Mathematically Gifted Children Developmental Brain Characteristics and Their Prognosis for Well-Being.pdf:pdf},
issn = {0278-3193},
journal = {Roeper Review},
number = {3},
pages = {181--186},
title = {{Mathematically Gifted Children: Developmental Brain Characteristics and Their Prognosis for Well-Being}},
volume = {30},
year = {2008}
}
@article{Varma2008,
abstract = {Background: There is increasing interest in applying neuroscience findings to topics in education. Purpose: This application requires a proper conceptualisation of the relation between cognition and brain function. This paper considers two such conceptualisations. The area focus understands each cognitive competency as the product of one (and only one) brain area. The network focus explains each cognitive competency as the product of collaborative processing among multiple brain areas. Sources of evidence: We first review neuroscience studies of mathematical reasoning–specifically arithmetic problem-solving and magnitude comparison–that exemplify the area focus and network focus. We then review neuroscience findings that illustrate the potential of the network focus for informing three topics in mathematics education: the development of mathematical reasoning, the effects of practice and instruction, and the derailment of mathematical reasoning in dyscalculia. Main argument: Although the area focus has historically dominated discussions in educational neuroscience, we argue that the network focus offers a complementary perspective on brain function that should not be ignored. Conclusions: We conclude by describing the current limitations of network-focus theorising and emerging neuroscience methods that promise to make such theorising more tractable in the future.},
author = {Varma, Sashank and Schwartz, Daniel L.},
doi = {10.1080/00131880802082633},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Varma, Schwartz - 2008 - How should educational neuroscience conceptualise the relation between cognition and brain function Mathematica.pdf:pdf},
isbn = {00131881},
issn = {0013-1881},
journal = {Educational Research},
number = {2},
pages = {149--161},
title = {{How should educational neuroscience conceptualise the relation between cognition and brain function? Mathematical reasoning as a network process}},
volume = {50},
year = {2008}
}
@article{Kucian2008,
abstract = {Neuroimaging findings in adults suggest exact and approximate number processing relying on distinct neural circuits. In the present study we are investigating whether this cortical specialization is already established in 9- and 12-year-old children. Using fMRI, brain activation was measured in 10 third- and 10 sixth-grade school children and 20 adults during trials of symbolic approximate (AP) and exact (EX) calculation, as well as non-symbolic magnitude comparison (MC) of objects. Children activated similar networks like adults, denoting an availability and a similar spatial extent of specified networks as early as third grade. However, brain areas related to number processing become further specialized with schooling. Children showed weaker activation in the intraparietal sulcus during all three tasks, in the left inferior frontal gyrus during EX and in occipital areas during MC. In contrast, activation in the anterior cingulate gyrus, a region associated with attentional effort and working memory load, was enhanced in children. Moreover, children revealed reduced or absent deactivation of regions involved in the so-called default network during symbolic calculation, suggesting a rather general developmental effect. No difference in brain activation patterns between AP and EX was found. Behavioral results indicated major differences between children and adults in AP and EX, but not in MC. Reaction time and accuracy rate were not correlated to brain activation in regions showing developmental changes suggesting rather effects of development than performance differences between children and adults. In conclusion, increasing expertise with age may lead to more automated processing of mental arithmetic, which is reflected by improved performance and by increased brain activation in regions related to number processing and decreased activation in supporting areas.},
author = {Kucian, Karin and von Aster, Michael and Loenneker, Thomas and Dietrich, Thomas and Martin, Ernst},
doi = {10.1080/87565640802101474},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kucian et al. - 2008 - Development of neural networks for exact and approximate calculation a FMRI study.pdf:pdf},
isbn = {1532-6942},
issn = {8756-5641},
journal = {Developmental neuropsychology},
number = {4},
pages = {447--473},
pmid = {18568899},
title = {{Development of neural networks for exact and approximate calculation: a FMRI study.}},
volume = {33},
year = {2008}
}
@article{Krueger2008,
abstract = {Only a subset of adults acquires specific advanced mathematical skills, such as integral calculus. The representation of more sophisticated mathematical concepts probably evolved from basic number systems; however its neuroanatomical basis is still unknown. Using fMRI, we investigated the neural basis of integral calculus while healthy participants were engaged in an integration verification task. Solving integrals activated a left-lateralized cortical network including the horizontal intraparietal sulcus, posterior superior parietal lobe, posterior cingulate gyrus, and dorsolateral prefrontal cortex. Our results indicate that solving of more abstract and sophisticated mathematical facts, such as calculus integrals, elicits a pattern of brain activation similar to the cortical network engaged in basic numeric comparison, quantity manipulation, and arithmetic problem solving. },
author = {Krueger, F and Spampinato, M V and Pardini, M and Pajevic, S and Wood, J N and Weiss, G H and Landgraf, S and Grafman, J},
doi = {10.1097/WNR.0b013e328303fd85},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krueger et al. - 2008 - Integral calculus problem solving an fMRI investigation.pdf:pdf},
isbn = {0959-4965},
issn = {0959-4965},
journal = {Neuroreport},
keywords = {arithmetic,dorsolateral prefrontal cortex,intraparietal sulcus,mathematics,superior parietal lobe},
number = {11},
pages = {1095--1099},
pmid = {18596607},
title = {{Integral calculus problem solving: an fMRI investigation}},
volume = {19},
year = {2008}
}
@article{Kaminski2008,
abstract = {Undergraduate students may benefit more from learning mathematics through a single abstract, symbolic representation than from learning multiple concrete examples.},
author = {Kaminski, Jennifer a. and Sloutsky, Vladimir M. and Heckler, Andrew F.},
doi = {10.1126/science.1154659},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaminski, Sloutsky, Heckler - 2008 - The Advantage of Abstract Examples in Learning Math.pdf:pdf},
isbn = {00368075 (ISSN)},
issn = {0036-8075, 1095-9203},
journal = {Science},
number = {5875},
pages = {454--455},
pmid = {18436760},
title = {{The Advantage of Abstract Examples in Learning Math}},
volume = {320},
year = {2008}
}
@article{Son2008,
author = {Son, Ji Y. and Smith, Linda B. and Goldstone, Robert L.},
doi = {10.1016/j.cognition.2008.05.002},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Son, Smith, Goldstone - 2008 - Simplicity and generalization Short-cutting abstraction in children's object categorizations.pdf:pdf},
issn = {00100277},
journal = {Cognition},
number = {3},
pages = {626--638},
title = {{Simplicity and generalization: Short-cutting abstraction in children's object categorizations}},
volume = {108},
year = {2008}
}
@incollection{Nathan2008,
abstract = {The need to understand and predict behaviour in complex settings such as the classroom and the workplace elevates the importance of the role of context and communication in building models of cognition. Embodied cognition is an emerging framework for under- standing intellectual behaviour in relation to the physical and social environment and to the perception- and action-based systems of the body. By reconsidering cognition with regard to interactions with the world, rather than in terms of the sequestered computa- tional nature of the mind, embodied cognition recasts many of the central issues of the study of thought and behaviour. One of the ways that cognition is seen as embodied is through the close relation of hand gestures with thinking and communication. In this chapter, I investigate how gestures enact symbols and thereby ground the meaning of abstract representations used in instructional settings.},
author = {Nathan, Mitchell J},
booktitle = {Symbols and embodiment: Debates on meaning and cognition},
doi = {10.1093/acprof:oso/9780199217274.003.0018},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nathan - 2008 - An embodied cognition perspective on symbols, gesture, and grounding instruction.pdf:pdf},
isbn = {9780191696060},
pages = {375--396},
title = {{An embodied cognition perspective on symbols, gesture, and grounding instruction}},
volume = {18},
year = {2008}
}
@article{Kaminski2008a,
abstract = {Undergraduate students may benefit more from learning mathematics through a single abstract, symbolic representation than from learning multiple concrete examples.},
author = {Kaminski, Jennifer a and Sloutsky, Vladimir M and Heckler, Andrew F},
doi = {10.1126/science.1154659},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaminski, Sloutsky, Heckler - 2008 - Learning theory. The advantage of abstract examples in learning math.pdf:pdf},
isbn = {00368075 (ISSN)},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
number = {5875},
pages = {454--455},
pmid = {18436760},
title = {{Learning theory. The advantage of abstract examples in learning math.}},
volume = {320},
year = {2008}
}
@article{Mccallum2008,
author = {Mccallum, William},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mccallum - 2008 - Commentary on Kaminski et al , The Advantage of Abstract Examples in Learning Math , An uncontrolled variable.pdf:pdf},
journal = {Group},
number = {April},
pages = {1--3},
title = {{Commentary on Kaminski et al , The Advantage of Abstract Examples in Learning Math , An uncontrolled variable}},
year = {2008}
}
@article{Shepard2008,
abstract = {Examples from Archimedes, Galileo, Newton, Einstein, and others suggest that fundamental laws of physics were-or, at least, could have been-discovered by experiments performed not in the physical world but only in the mind. Although problematic for a strict empiricist, the evolutionary emergence in humans of deeply internalized implicit knowledge of abstract principles of transformation and symmetry may have been crucial for humankind's step to rationality-including the discovery of universal principles of mathematics, physics, ethics, and an account of free will that is compatible with determinism.},
author = {Shepard, Roger N},
doi = {10.1080/03640210701801917},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shepard - 2008 - The step to rationality the efficacy of thought experiments in science, ethics, and free will.pdf:pdf},
issn = {0364-0213},
journal = {Cognitive science},
keywords = {agency,determinism,free will,imagined transformations,mental rotation,moral laws,physical laws,rationality,symmetry,the golden rule,thought experiments},
number = {1},
pages = {3--35},
pmid = {21635330},
title = {{The step to rationality: the efficacy of thought experiments in science, ethics, and free will.}},
volume = {32},
year = {2008}
}
@article{Ram2007,
abstract = {Growth curve modeling has become a mainstay in the study of development. In this article we review some of the flexibility provided by this technique for describing and testing hypotheses about: (1) intraindividual change across multiple occasions of measurement, and (2) interindividual differences in intraindividual change. Through empirical example we demonstrate how linear, quadratic, latent basis, exponential, and multiphase versions of the model can be specified using commonly available SEM/multilevel modeling software and illustrate and discuss how results are obtained and interpreted. Particularly, we underscore the “developmental theory” articulated by each model.},
author = {Ram, Nilam and Grimm, Kevin},
doi = {10.1177/0165025407077751},
file = {:home/andrew/Documents/grad/Papers/0165025407077751.pdf:pdf},
isbn = {0165-0254},
issn = {01650254},
journal = {International Journal of Behavioral Development},
keywords = {Developmental change,Growth curve modeling,Intraindividual change},
number = {4},
pages = {303--316},
title = {{Using simple and complex growth models to articulate developmental change: Matching theory to method}},
volume = {31},
year = {2007}
}
@article{Dicarlo2007,
author = {Dicarlo, James J and Cox, David D},
doi = {10.1016/j.tics.2007.06.010},
file = {:home/andrew/Documents/grad/Papers/dicarlo and cox 2007.pdf:pdf},
number = {8},
title = {{Untangling invariant object recognition}},
volume = {11},
year = {2007}
}
@article{Hok2007,
author = {Hok, Vincent and Hok, Vincent and Save, Etienne and Muller, Robert U and Poucet, Bruno},
doi = {10.1523/JNEUROSCI.2864-06.2007},
file = {:home/andrew/Documents/grad/Papers/Hoketal2007a.pdf:pdf},
journal = {The Journal of Neuroscience},
keywords = {goal coding,hippocampus,place cells,rat,spatial processing,unit recordings},
number = {January},
pages = {472--478},
title = {{Goal-related firing in hippocampal place cells Goal-Related Activity in Hippocampal Place Cells}},
volume = {27},
year = {2007}
}
@article{Rajendran2007,
author = {Rajendran, Gnanathusharan and Mitchell, Peter},
doi = {10.1016/j.dr.2007.02.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rajendran, Mitchell - 2007 - Cognitive theories of autism.pdf:pdf},
journal = {Developmental Review},
keywords = {asperger syndrome,autism,autistic spectrum disorders,cognitive theories,considers these theories by,describing the research which,dominated psychological research into,gave life to them,introduction and the history,of cognitive theories of,studies which,this article,three cognitive theories have},
number = {2},
pages = {224--260},
title = {{Cognitive theories of autism}},
volume = {27},
year = {2007}
}
@article{Corpus2007,
author = {Corpus, Jennifer Henderlong and Lepper, Mark R},
doi = {10.1080/01443410601159852},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Corpus, Lepper - 2007 - The Effects of Person Versus Performance Praise on Children's Motivation Gender and age as moderating factors.pdf:pdf},
isbn = {0144341060115},
journal = {Educational Psychology},
number = {4},
pages = {487--508},
title = {{The Effects of Person Versus Performance Praise on Children's Motivation: Gender and age as moderating factors}},
volume = {27},
year = {2007}
}
@article{Blackwell2007,
author = {Blackwell, Lisa S and Trzesniewski, Kali H and Dweck, Carol S},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blackwell, Trzesniewski, Dweck - 2007 - Implicit Theories of Intelligence Predict Achievement Across an Adolescent Transition A Longitud.pdf:pdf},
journal = {Child Development},
number = {1},
pages = {246--263},
title = {{Implicit Theories of Intelligence Predict Achievement Across an Adolescent Transition: A Longitudinal Study and an Intervention}},
volume = {78},
year = {2007}
}
@article{Day2007,
author = {Day, Samuel B and Gentner, Dedre},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day, Gentner - 2007 - Nonintentional analogical inference in text comprehension.pdf:pdf},
title = {{Nonintentional analogical inference in text comprehension}},
year = {2007}
}
@article{Maurer2007,
author = {Maurer, Daphne and Mondloch, Catherine J and Lewis, Terri L},
doi = {10.1111/j.1467-7687.2007.00562.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maurer, Mondloch, Lewis - 2007 - Sleeper effects.pdf:pdf},
pages = {40--47},
title = {{Sleeper effects}},
volume = {1},
year = {2007}
}
@article{Landy2007,
abstract = {Although a general sense of the magnitude, quantity, or numerosity of objects is common in both untrained people and animals, the abilities to deal exactly with large quantities and to reason precisely in complex but well-specified situations--to behave formally, that is--are skills unique to people trained in symbolic notations. These symbolic notations typically employ complex, hierarchically embedded structures, which all extant analyses assume are constructed by concatenative, rule-based processes. The primary goal of this article is to establish, using behavioral measures on naturalistic tasks, that some of the same cognitive resources involved in representing spatial relations and proximities are also involved in representing symbolic notations--in short, that formal notations are a kind of diagram. We examined self-generated productions in the domains of handwritten arithmetic expressions and typewritten statements in a formal logic. In both tasks, we found substantial evidence for spatial representational schemes even in these highly symbolic domains.},
author = {Landy, David and Goldtone, Robert L},
doi = {10.3758/BF03192935},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Landy, Goldtone - 2007 - Formal notations are diagrams evidence from a production task.pdf:pdf},
isbn = {0090-502X},
issn = {0090-502X},
journal = {Memory {\&} cognition},
number = {8},
pages = {2033--2040},
pmid = {18265618},
title = {{Formal notations are diagrams: evidence from a production task.}},
volume = {35},
year = {2007}
}
@article{Keuroghlian2007,
abstract = {Enormous progress has been made in our understanding of adaptive plasticity in the central auditory system. Experiments on a range of species demonstrate that, in adults, the animal must attend to (i.e., respond to) a stimulus in order for plasticity to be induced, and the plasticity that is induced is specific for the acoustic feature to which the animal has attended. The requirement that an adult animal must attend to a stimulus in order for adaptive plasticity to occur suggests an essential role of neuromodulatory systems in gating plasticity in adults. Indeed, neuromodulators, particularly acetylcholine (ACh), that are associated with the processes of attention, have been shown to enable adaptive plasticity in adults. In juvenile animals, attention may facilitate plasticity, but it is not always required: during sensitive periods, mere exposure of an animal to an atypical auditory environment can result in large functional changes in certain auditory circuits. Thus, in both the developing and mature auditory systems substantial experience-dependent plasticity can occur, but the conditions under which it occurs are far more stringent in adults. We review experimental results that demonstrate experience-dependent plasticity in the central auditory representations of sound frequency, level and temporal sequence, as well as in the representations of binaural localization cues in both developing and adult animals. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Keuroghlian, Alex S. and Knudsen, Eric I.},
doi = {10.1016/j.pneurobio.2007.03.005},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keuroghlian, Knudsen - 2007 - Adaptive auditory plasticity in developing and adult animals.pdf:pdf},
isbn = {0301-0082 (Print)$\backslash$r0301-0082 (Linking)},
issn = {03010082},
journal = {Progress in Neurobiology},
keywords = {Adult,Attention,Auditory system,Experience,Juvenile,Neuromodulators,Plasticity,Sensitive period},
number = {3},
pages = {109--121},
pmid = {17493738},
title = {{Adaptive auditory plasticity in developing and adult animals}},
volume = {82},
year = {2007}
}
@article{Cohen2007,
abstract = {The relationship between parents styles of talking about past events with their children and childrens recall of stressful events was explored. In this investigation, 2- to 5-year-old childrens recall of injuries requiring hospital emergency room treatment was assessed within a few days of the injury and again 2 years later, along with the way their parents reminisced with them about the event. Correlational analyses showed that age and parental reminiscing style were consistently related to child memory; regression analyses showed that although age was most important, parents who were more elaborative had children who recalled more during their initial interview about the harder- to-remember hospital event. Thus, an elaborative parental style may help childrens recall of even highly salient and stressful events.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Cohen, C. A. and Hegarty, M.},
doi = {10.1002/acp},
eprint = {NIHMS150003},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Hegarty - 2007 - Individual Differences in Use of External Visualizations to Perform an Internal Visualization Task.pdf:pdf},
isbn = {1591479304},
issn = {0013127X},
journal = {Applied Cognitive Psychology},
pages = {701--711},
pmid = {73986922},
title = {{Individual Differences in Use of External Visualizations to Perform an Internal Visualization Task}},
volume = {21},
year = {2007}
}
@article{Gomez2006,
abstract = {Infants engage in an extraordinary amount of learning during their waking hours even though much of their day is consumed by sleep. What role does sleep play in infant learning? Fifteen-month-olds were familiarized with an artificial language 4 hr prior to a lab visit. Learning the language involved relating initial and final words in auditory strings by remembering the exact word dependencies or by remembering an abstract relation be- tween initial and final words. One group napped during the interval between familiarization and test. Another group did not nap. Infants who napped appeared to re- member a more abstract relation, one they could apply to stimuli that were similar but not identical to those from familiarization. Infants whodid not nap showed amemory effect. Naps appear to promote a qualitative change in memory, one involving greater flexibility in learning.},
author = {Gomez, R L and Bootzin, Richard R. and Nadel, L},
doi = {10.1111/j.1467-9280.2006.01764.x},
file = {:home/andrew/Documents/grad/Papers/j.1467-9280.2006.01764.x.pdf:pdf},
issn = {0956-7976},
journal = {Psychological Science},
number = {8},
pages = {670--674},
pmid = {16913948},
title = {{Naps promote abstraction in language learning infants}},
volume = {17},
year = {2006}
}
@article{Mezzadri2006,
abstract = {We discuss how to generate random unitary matrices from the classical compact groups U(N), O(N) and USp(N) with probability distributions given by the respective invariant measures. The algorithm is straightforward to implement using standard linear algebra packages. This approach extends to the Dyson circular ensembles too. This article is based on a lecture given by the author at the summer school on Number Theory and Random Matrix Theory held at the University of Rochester in June 2006. The exposition is addressed to a general mathematical audience.},
archivePrefix = {arXiv},
arxivId = {math-ph/0609050},
author = {Mezzadri, Francesco},
eprint = {0609050},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mezzadri - 2006 - How to generate random matrices from the classical compact groups.pdf:pdf},
issn = {00029920},
primaryClass = {math-ph},
title = {{How to generate random matrices from the classical compact groups}},
url = {http://arxiv.org/abs/math-ph/0609050},
year = {2006}
}
@article{Mcnaughton2006,
author = {Mcnaughton, Bruce L and Battaglia, Francesco P and Jensen, Ole and Moser, Edvard I},
doi = {10.1038/nrn1932},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcnaughton et al. - 2006 - Path integration and the neural basis of the ‘ cognitive map '.pdf:pdf},
number = {August},
pages = {663--678},
title = {{Path integration and the neural basis of the ‘ cognitive map '}},
volume = {7},
year = {2006}
}
@article{Solstad2006,
author = {Solstad, Trygve and Moser, Edvard I and Einevoll, Gaute T},
doi = {10.1002/hipo},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Solstad, Moser, Einevoll - 2006 - From Grid Cells to Place Cells A Mathematical Model.pdf:pdf},
journal = {Hippocampus},
keywords = {1978,a,a widespread brain network,entorhinal cortex,for spatial representation and,grid cells,hippocampus,keefe and nadel,memory,navigation,o,place cells,spatial representation,the hippocampus is thought,to be part of},
pages = {1026--1031},
title = {{From Grid Cells to Place Cells: A Mathematical Model}},
volume = {1031},
year = {2006}
}
@book{Wasserman2006,
author = {Wasserman, Larry},
isbn = {0-387-25145-6},
publisher = {Springer Texts in Statistics},
title = {{All of Nonparametric Statistics}},
year = {2006}
}
@article{Gelman2006,
author = {Gelman, Andrew},
doi = {10.1198/004017005000000661},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman - 2006 - Multilevel (Hierarchical) Modeling What It Can and Cannot Do.pdf:pdf},
journal = {Technometrics},
keywords = {contextual effects,hierarchical model,multilevel regression},
number = {3},
pages = {432--435},
title = {{Multilevel (Hierarchical) Modeling: What It Can and Cannot Do}},
volume = {48},
year = {2006}
}
@article{Hannula2006,
author = {Hannula, Markku S.},
doi = {10.1007/s10649-005-9019-8},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hannula - 2006 - MOTIVATION IN MATHEMATICS GOALS REFLECTED IN EMOTIONS.pdf:pdf},
journal = {Educational studies in mathematics},
keywords = {affect,emotion,goal,mathematics learning,motivation,needs},
number = {2},
pages = {165--178},
title = {{MOTIVATION IN MATHEMATICS : GOALS REFLECTED IN EMOTIONS}},
volume = {63},
year = {2006}
}
@article{Ainsworth2006,
abstract = {Multiple (external) representations can provide unique benefits when people are learning complex new ideas. Unfortunately, many studies have shown this promise is not always achieved. The DeFT (Design, Functions, Tasks) framework for learning with multiple representations integrates research on learning, the cognitive science of representation and constructivist theories of education. It proposes that the effectiveness of multiple representations can best be understood by considering three fundamental aspects of learning: the design parameters that are unique to learning with multiple representations; the functions that multiple representations serve in supporting learning and the cognitive tasks that must be undertaken by a learner interacting with multiple representations. The utility of this framework is proposed to be in identifying a broad range of factors that influence learning, reconciling inconsistent experimental findings, revealing under-explored areas of multi-representational research and pointing forward to potential design heuristics for learning with multiple representations. ?? 2006 Elsevier Ltd. All rights reserved.},
author = {Ainsworth, Shaaron},
doi = {10.1016/j.learninstruc.2006.03.001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ainsworth - 2006 - DeFT A conceptual framework for considering learning with multiple representations.pdf:pdf},
isbn = {0959-4752 DO  - http://dx.doi.org/10.1016/j.learninstruc.2006.03.001},
issn = {09594752},
journal = {Learning and Instruction},
keywords = {Diagrams,Multimedia,Multiple representations,Pictures},
number = {3},
pages = {183--198},
title = {{DeFT: A conceptual framework for considering learning with multiple representations}},
volume = {16},
year = {2006}
}
@article{Sanborn2006,
annote = {Order effects from Bayesian cognitive modeling as results of inference algorithms with finite capacity, etc., see also Sanborn's other shit},
author = {Sanborn, Adam N and Griffiths, Thomas L and Navarro, Daniel J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanborn, Griffiths, Navarro - 2006 - A More Rational Model of Categorization.pdf:pdf},
journal = {Proceedings of the 28th Annual Conference of the Cognitive Science Society},
title = {{A More Rational Model of Categorization}},
year = {2006}
}
@article{Ostrovsky2006,
abstract = {Animal studies suggest that early visual deprivation can cause permanent functional blindness. However, few human data on this issue exist. Given enough time for recovery, can a person gain visual skills after several years of congenital blindness? In India, we recently had an unusual opportunity to work with an individual whose case history sheds light on this question. S.R.D. was born blind, and remained so until age 12. She then underwent surgery for the removal of dense congenital cataracts. We evaluated her performance on an extensive battery of visual tasks 20 years after surgery. We found that although S.R.D.'s acuity is compromised, she is proficient on mid- and high-level visual tasks. These results suggest that the human brain retains an impressive capacity for visual learning well into late childhood. They have implications for current conceptions of cortical plasticity and provide an argument for treating congenital blindness even in older children.},
author = {Ostrovsky, Yuri and Andalman, Aaron and Sinha, Pawan},
doi = {10.1111/j.1467-9280.2006.01827.x},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ostrovsky, Andalman, Sinha - 2006 - Vision following extended congenital blindness.pdf:pdf},
isbn = {0956-7976},
issn = {09567976},
journal = {Psychological Science},
number = {12},
pages = {1009--1014},
pmid = {17201779},
title = {{Vision following extended congenital blindness}},
volume = {17},
year = {2006}
}
@article{Foundalis2006,
abstract = {Phaeaco is a cognitive architecture for visual pattern recognition that starts at the ground level of receiving pixels as input, and works its way through creating abstract representations of geometric figures formed by those pixels. Phaeaco can tell how similar such figures are by using a psychologically plausible metric to compute a difference value among representations, and use that value to group figures together, if possible. Groups of figures are represented by statistical attributes (average, standard deviation, and other statistics), and serve as the basis for a formed and thereafter learned concept (e.g., triangle), stored in long-term memory. Phaeaco focuses on the Bongard problems, a set of puzzles in visual categorization, and applies its cognitive principles in its efforts to solve them, faring nearly as well as humans in the puzzles it manages to solve.},
author = {Foundalis, He},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Foundalis - 2006 - Phaeaco A Cognitive Architecture Inspired By Bongard's Problems.pdf:pdf},
journal = {Citeulike.Org},
number = {May},
pages = {1--461},
title = {{Phaeaco: A Cognitive Architecture Inspired By Bongard's Problems}},
year = {2006}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Osindero, Teh - 2006 - A fast learning algorithm for deep belief nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Animals,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology},
number = {7},
pages = {1527--54},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets.}},
volume = {18},
year = {2006}
}
@article{Hinton2006a,
abstract = {{\%}Z {\%}U {\%}+ {\%}{\^{}}},
author = {Hinton, Geoffrey and Nair, Vinod},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Nair - 2006 - Inferring motor programs from images of handwritten digits.pdf:pdf},
isbn = {9780262232531},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {515},
title = {{Inferring motor programs from images of handwritten digits}},
volume = {18},
year = {2006}
}
@article{Fangmeier2006,
author = {Fangmeier, T and Knauff, Markus and Ruff, Cc and Sloutsky, V},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fangmeier et al. - 2006 - FMRI-Evidence for a Three-Stage-Model of Deductive Reasoning.pdf:pdf},
journal = {Journal of Cognitive {\ldots}},
title = {{FMRI-Evidence for a Three-Stage-Model of Deductive Reasoning}},
year = {2006}
}
@article{Moses2006,
abstract = {Generalizations about neural function are often drawn from non-human animal models to human cognition, however, the assumption of cross-species conservation may sometimes be invalid. Humans may use different strategies mediated by alternative structures, or similar structures may operate differently within the context of the human brain. The transitive inference problem, considered a hallmark of logical reasoning, can be solved by non-human species via associative learning rather than logic. We tested whether humans use similar strategies to other species for transitive inference. Results are crucial for evaluating the validity of widely accepted assumptions of similar neural substrates underlying performance in humans and other animals. Here we show that successful transitive inference in humans is unrelated to use of associative learning strategies and is associated with ability to report the hierarchical relationship among stimuli. Our work stipulates that cross-species generalizations must be interpreted cautiously, since performance on the same task may be mediated by different strategies and/or neural systems. ?? 2006 Elsevier Ltd. All rights reserved.},
author = {Moses, Sandra N. and Villate, Christina and Ryan, Jennifer D.},
doi = {10.1016/j.neuropsychologia.2006.01.004},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moses, Villate, Ryan - 2006 - An investigation of learning strategy supporting transitive inference performance in humans compared to ot.pdf:pdf},
issn = {00283932},
journal = {Neuropsychologia},
keywords = {Associative learning,Awareness,Cross-species generalizations,Hierarchical relationship,Logic},
number = {8},
pages = {1370--1387},
pmid = {16503340},
title = {{An investigation of learning strategy supporting transitive inference performance in humans compared to other species}},
volume = {44},
year = {2006}
}
@article{Goldstone2005,
author = {Goldstone, Robert L and Son, Ji Y},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldstone, Son - 2005 - The Transfer of Scientific Principles Using Concrete and Idealized Simulations.pdf:pdf},
journal = {The Journal of the Learning Sciences},
number = {1},
pages = {69--110},
title = {{The Transfer of Scientific Principles Using Concrete and Idealized Simulations}},
volume = {14},
year = {2005}
}
@article{Scholz2005,
abstract = {MOTIVATION: Visualizing and analysing the potential non-linear structure of a dataset is becoming an important task in molecular biology. This is even more challenging when the data have missing values. RESULTS: Here, we propose an inverse model that performs non-linear principal component analysis (NLPCA) from incomplete datasets. Missing values are ignored while optimizing the model, but can be estimated afterwards. Results are shown for both artificial and experimental datasets. In contrast to linear methods, non-linear methods were able to give better missing value estimations for non-linear structured data.Application: We applied this technique to a time course of metabolite data from a cold stress experiment on the model plant Arabidopsis thaliana, and could approximate the mapping function from any time point to the metabolite responses. Thus, the inverse NLPCA provides greatly improved information for better understanding the complex response to cold stress. CONTACT: scholz@mpimp-golm.mpg.de.},
annote = {NULL},
author = {Scholz, Matthias and Kaplan, Fatma and Guy, Charles L. and Kopka, Joachim and Selbig, Joachim},
doi = {10.1093/bioinformatics/bti634},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scholz et al. - 2005 - Non-linear PCA A missing data approach.pdf:pdf},
isbn = {1367-4803},
issn = {13674803},
journal = {Bioinformatics},
number = {20},
pages = {3887--3895},
pmid = {16109748},
title = {{Non-linear PCA: A missing data approach}},
volume = {21},
year = {2005}
}
@article{Schwartz2005,
author = {Schwartz, Daniel L and Bransford, John D and Sears, David},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwartz, Bransford, Sears - 2005 - Efficiency and innvoation in transfer.pdf:pdf},
pages = {1--73},
title = {{Efficiency and innvoation in transfer}},
year = {2005}
}
@article{Nokes2005,
abstract = {Contemporary theories of learning postulate one or at most a small number of different learning mechanisms. However, people are capable of mastering a given task through qualitatively different learning paths such as learning by instruction and learning by doing. We hypothesize that the knowledge acquired through such alternative paths differs with respect to the level of abstraction and the balance between declarative and procedural knowledge. In a laboratory experiment we investigated what was learned about patterned letter sequences via either direct instruction in the relevant patterns or practice in solving letter-sequence extrapolation problems. Results showed that both types of learning led to mastery of the target task as measured by accuracy performance. However, behavioral differences emerged in how participants applied their knowledge. Participants given instruction showed more variability in the types of strategies they used to articulate their knowledge as well as longer solution times for generating the action implications of that knowledge as compared to the participants given practice. Results are discussed regarding the implications for transfer, generalization, and procedural application. Learning theories that claim generality should be tested against cross-scenario phenomena, not just parametric variations of a single learning scenario.},
author = {Nokes, Timothy J and Ohlsson, Stellan},
doi = {10.1207/s15516709cog0000_32},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nokes, Ohlsson - 2005 - Comparing multiple paths to mastery What is learned.pdf:pdf},
isbn = {0364-0213},
issn = {0364-0213},
journal = {Cognitive Science},
keywords = {human experimentation,instruction,problem solving,psychology,representation,skill acquisition and learning},
pages = {769--796},
pmid = {21702793},
title = {{Comparing multiple paths to mastery: What is learned?}},
volume = {29},
year = {2005}
}
@article{Wille2005,
abstract = {Formal Concept Analysis has been originally developed as a subfield of Applied Mathematics based on the mathematization of concept and concept hierarchy. Only after more than a decade of development, the connections to the philosophical logic of human thought became clearer and even later the connections to Piaget's cognitive structuralism which Thomas Bernhard Seiler convincingly elaborated to a comprehensive theory of concepts in his recent book [Se01]. It is the main concern of this paper to show the surprisingly rich correspondences between Seiler's multifarious aspects of concepts in the human mind and the structural properties and relationships of formal concepts in Formal Concept Analysis. These correspondences make understandable, what has been experienced in a great multitude of applications, that Formal Concept Analysis may function in the sense of transdisciplinary mathematics, i.e., it allows mathematical thought to aggregate with other ways of thinking and thereby to support human thought and action.},
author = {Wille, Rudolf},
doi = {10.1007/11528784_1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wille - 2005 - Formal Concept Analysis as Mathematical Theory of Concepts and Concept Hierarchies.pdf:pdf},
isbn = {978-3-540-31881-1},
journal = {Formal Concept Analysis - Foundations and Applications},
pages = {1--33},
title = {{Formal Concept Analysis as Mathematical Theory of Concepts and Concept Hierarchies}},
year = {2005}
}
@article{Kong2005,
abstract = {Recent functional neuroimaging studies have begun to clarify how the human brain performs the everyday activities that require mental calculation. We used fMRI to test the hypotheses that there are specific neural networks dedicated to performing an arithmetic operation (e.g. + or -) and to performing processes that support more complex calculations. We found that the right inferior parietal lobule, left precuneus and left superior parietal gyrus are relatively specific for performing subtraction; and bilateral medial frontal/cingulate cortex are relatively specific for supporting arithmetic procedure complexity. We also found that greater difficulty level was associated with activation in a brain network including left inferior intraparietal sulcus, left inferior frontal gyrus and bilateral cingulate. Our results suggest that the network activated by the simplest calculation serves as a common basis, to which more regions are recruited for more difficult problems or different arithmetic operations. ?? 2004 Elsevier B.V. All rights reserved.},
author = {Kong, Jian and Wang, Chunmao and Kwong, Kenneth and Vangel, Mark and Chua, Elizabeth and Gollub, Randy},
doi = {10.1016/j.cogbrainres.2004.09.011},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kong et al. - 2005 - The neural substrate of arithmetic operations and procedure complexity.pdf:pdf},
isbn = {0926-6410 (Print)},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Brain network,Calculation,Mental arithmetic,fMRI},
number = {3},
pages = {397--405},
pmid = {15722210},
title = {{The neural substrate of arithmetic operations and procedure complexity}},
volume = {22},
year = {2005}
}
@article{OBoyle2005,
abstract = {Mental rotation involves the creation and manipulation of internal images, with the later being particularly useful cognitive capacities when applied to high-level mathematical thinking and reasoning. Many neuroimaging studies have demonstrated mental rotation to be mediated primarily by the parietal lobes, particularly on the right side. Here, we use fMRI to show for the first time that when performing 3-dimensional mental rotations, mathematically gifted male adolescents engage a qualitatively different brain network than those of average math ability, one that involves bilateral activation of the parietal lobes and frontal cortex, along with heightened activation of the anterior cingulate. Reliance on the processing characteristics of this uniquely bilateral system and the interplay of these anterior/posterior regions may be contributors to their mathematical precocity. ?? 2005 Elsevier B.V. All rights reserved.},
author = {O'Boyle, Michael W. and Cunnington, Ross and Silk, Timothy J. and Vaughan, David and Jackson, Graeme and Syngeniotis, Ari and Egan, Gary F.},
doi = {10.1016/j.cogbrainres.2005.08.004},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Boyle et al. - 2005 - Mathematically gifted male adolescents activate a unique brain network during mental rotation.pdf:pdf},
isbn = {0926-6410 (Print)$\backslash$r0926-6410 (Linking)},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Mathematical giftedness,Mental rotation,Sketch,Visuospatial processing,fMRI},
mendeley-tags = {Sketch},
number = {2},
pages = {583--587},
pmid = {16150579},
title = {{Mathematically gifted male adolescents activate a unique brain network during mental rotation}},
volume = {25},
year = {2005}
}
@article{Ioannidis2005,
author = {Ioannidis, John P A},
doi = {10.1371/journal.pmed.0020124},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:pdf},
number = {8},
title = {{Why Most Published Research Findings Are False}},
volume = {2},
year = {2005}
}
@book{Rogers2004,
author = {Rogers, Timothy T and McClelland, James L.},
publisher = {MIT Press},
title = {{Semantic Cognition: A Parallel Distributed Processing Approach}},
year = {2004}
}
@article{Kalish2004,
author = {Kalish, Michael L and Lewandowsky, Stephan and Kruschke, John K},
doi = {10.1037/0033-295X.111.4.1072},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalish, Lewandowsky, Kruschke - 2004 - Population of Linear Experts Knowledge Partitioning and Function Learning.pdf:pdf},
number = {4},
pages = {1072--1099},
title = {{Population of Linear Experts : Knowledge Partitioning and Function Learning}},
volume = {111},
year = {2004}
}
@book{Gardenfors2004,
author = {G{\"{a}}rdenfors, Peter},
booktitle = {Conceptual Spaces},
doi = {10.1007/978-1-4020-9877-2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/G{\"{a}}rdenfors - 2004 - Conceptual spaces.pdf:pdf},
isbn = {9781402098772},
title = {{Conceptual spaces}},
year = {2004}
}
@article{Kuhlmann2004,
author = {Kuhlmann, Gregory and Stone, Peter and Mooney, Raymond and Shavlik, Jude},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuhlmann et al. - 2004 - Guiding a Reinforcement Learner with Natural Language Advice Initial Results in RoboCup Soccer.pdf:pdf},
number = {July},
pages = {30--35},
title = {{Guiding a Reinforcement Learner with Natural Language Advice : Initial Results in RoboCup Soccer}},
year = {2004}
}
@article{Singh2004,
abstract = {Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous en- tities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing arti- ficial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy.},
author = {Singh, S. and Barto, A.G. and Chentanez, N.},
doi = {10.1109/TAMD.2010.2051031},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh, Barto, Chentanez - 2004 - Intrinsically motivated reinforcement learning.pdf:pdf},
isbn = {0262195348, 9780262195348},
issn = {1943-0604},
journal = {18th Annual Conference on Neural Information Processing Systems (NIPS)},
number = {2},
pages = {1281--1288},
title = {{Intrinsically motivated reinforcement learning}},
volume = {17},
year = {2004}
}
@article{Hegarty,
author = {Hegarty, M.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hegarty - 2004 - Diagrams in the Mind and in the World Relations between Internal and External.pdf:pdf},
title = {{Diagrams in the Mind and in the World: Relations between Internal and External}},
year = {2004}
}
@article{Kawashima2004,
abstract = {The purpose of this study was to examine brain areas involved in simple arithmetic, and to compare these areas between adults and children. Eight children (four girls and four boys; age, 9-14 years) and eight adults (four women and four men; age, 40-49 years) were subjected to this study. Functional magnetic resonance imaging (fMRI) was performed during mental calculation of addition, subtraction, and multiplication of single digits. In each group, the left middle frontal, bilateral inferior temporal and bilateral lateral occipital cortices were activated during each task. The adult group showed activation of the right frontal cortex during addition and multiplication tasks, but the children group did not. Activation of the intraparietal cortex was observed in the adult group during each task. Although, activation patterns were slightly different among tasks, as well as between groups, only a small number of areas showed statistically significant differences. The results indicate that cortical networks involved in simple arithmetic are similar among arithmetic operations, and may not show significant changes in the structure during the second decade of life. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Kawashima, Ryuta and Taira, Masato and Okita, Katsuo and Inoue, Kentaro and Tajima, Nobumoto and Yoshida, Hajime and Sasaki, Takeo and Sugiura, Motoaki and Watanabe, Job and Fukuda, Hiroshi},
doi = {10.1016/j.cogbrainres.2003.10.009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawashima et al. - 2004 - A functional MRI study of simple arithmetic—a comparison between children and adults.pdf:pdf;:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawashima et al. - 2004 - A functional MRI study of simple arithmetic - A comparison between children and adults.html:html},
isbn = {1053-8119},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Adults,Children,Functional MRI study,adults,children,functional mri study},
number = {3},
pages = {227--233},
title = {{A functional MRI study of simple arithmetic—a comparison between children and adults}},
volume = {18},
year = {2004}
}
@article{Kawashima2004a,
author = {Kawashima, Ryuta and Taira, Masato and Okita, Katsuo and Inoue, Kentaro and Tajima, Nobumoto and Yoshida, Hajime and Sasaki, Takeo and Sugiura, Motoaki and Watanabe, Job and Fukuda, Hiroshi},
doi = {10.1016/j.cogbrainres.2003.10.009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawashima et al. - 2004 - A functional MRI study of simple arithmetic—a comparison between children and adults.pdf:pdf},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {adults,children,functional mri study},
number = {3},
pages = {227--233},
title = {{A functional MRI study of simple arithmetic—a comparison between children and adults}},
volume = {18},
year = {2004}
}
@article{Kawashima2004,
abstract = {The purpose of this study was to examine brain areas involved in simple arithmetic, and to compare these areas between adults and children. Eight children (four girls and four boys; age, 9-14 years) and eight adults (four women and four men; age, 40-49 years) were subjected to this study. Functional magnetic resonance imaging (fMRI) was performed during mental calculation of addition, subtraction, and multiplication of single digits. In each group, the left middle frontal, bilateral inferior temporal and bilateral lateral occipital cortices were activated during each task. The adult group showed activation of the right frontal cortex during addition and multiplication tasks, but the children group did not. Activation of the intraparietal cortex was observed in the adult group during each task. Although, activation patterns were slightly different among tasks, as well as between groups, only a small number of areas showed statistically significant differences. The results indicate that cortical networks involved in simple arithmetic are similar among arithmetic operations, and may not show significant changes in the structure during the second decade of life. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Kawashima, Ryuta and Taira, Masato and Okita, Katsuo and Inoue, Kentaro and Tajima, Nobumoto and Yoshida, Hajime and Sasaki, Takeo and Sugiura, Motoaki and Watanabe, Job and Fukuda, Hiroshi},
doi = {10.1016/j.cogbrainres.2003.10.009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawashima et al. - 2004 - A functional MRI study of simple arithmetic - A comparison between children and adults.html:html},
isbn = {1053-8119},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Adults,Children,Functional MRI study},
number = {3},
pages = {225--231},
title = {{A functional MRI study of simple arithmetic - A comparison between children and adults}},
volume = {18},
year = {2004}
}
@article{Qin2004,
abstract = {In a brain imaging study of children learning algebra, it is shown that the same regions are active in children solving equations as are active in experienced adults solving equations. As with adults, practice in symbol manipulation produces a reduced activation in prefrontal cortex area. However, unlike adults, practice seems also to produce a decrease in a parietal area that is holding an image of the equation. This finding suggests that adolescents' brain responses are more plastic and change more with practice. These results are integrated in a cognitive model that predicts both the behavioral and brain imaging results.},
author = {Qin, Yulin and Carter, Cameron S and Silk, Eli M and Stenger, V Andrew and Fissell, Kate and Goode, Adam and Anderson, John R},
doi = {10.1073/pnas.0401227101},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qin et al. - 2004 - The change of the brain activation patterns as children learn algebra equation solving.pdf:pdf},
isbn = {0027-8424 (Print)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
pages = {5686--5691},
pmid = {15064407},
title = {{The change of the brain activation patterns as children learn algebra equation solving.}},
volume = {101},
year = {2004}
}
@article{Logothetis2004,
author = {Logothetis, Nikos K. and Wandell, Brian a.},
doi = {10.1146/annurev.physiol.66.082602.092845},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Logothetis, Wandell - 2004 - Interpreting the BOLD Signal.pdf:pdf},
issn = {0066-4278},
journal = {Annual Review of Physiology},
keywords = {blood-oxygen-level-dependent,broad community of scientists,fmri,has brought together a,imaging,interested in measuring the,local field,multiunit activity,of functional magnetic resonance,potential,s abstract the development,visual cortex},
number = {1},
pages = {735--769},
title = {{Interpreting the BOLD Signal}},
volume = {66},
year = {2004}
}
@article{Love2004,
author = {Love, Bradley C. and Medin, Douglas L. and Gureckis, Todd M.},
doi = {10.1037/0033-295X.111.2.309},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Love, Medin, Gureckis - 2004 - SUSTAIN A Network Model of Category Learning.pdf:pdf},
issn = {1939-1471},
journal = {Psychological Review},
number = {2},
pages = {309--332},
title = {{SUSTAIN: A Network Model of Category Learning.}},
volume = {111},
year = {2004}
}
@article{Weber2004,
abstract = {It is widely accepted by mathematics educators and mathematicians that most proof-oriented university mathematics courses are taught in a "definition-theorem-proof" format. However, there are relatively few empirical studies on what takes place during this instruction, why this instruction is used, and how it affects students' learning. In this paper, I investigate these issues by examining a case study of one professor using this type of instruction in an introductory real analysis course. I first describe the professor's actions in the classroom and argue that these actions are the result of the professor's beliefs about mathematics, students, and education, as well as his knowledge of the material being covered. I then illustrate how the professor's teaching style influenced the way that his students attempted to learn the material. Finally, I discuss the implications that the reported data have on mathematics education research. ?? 2004 Elsevier Inc. All rights reserved.},
author = {Weber, Keith},
doi = {10.1016/j.jmathb.2004.03.001},
isbn = {0732-3123},
issn = {07323123},
journal = {Journal of Mathematical Behavior},
keywords = {Advanced mathematical thinking,Collegiate mathematics education,Lecture,Mathematics education,Proof,Real analysis,Teaching},
pages = {115--133},
title = {{Traditional instruction in advanced mathematics courses: A case study of one professor's lectures and proofs in an introductory real analysis course}},
volume = {23},
year = {2004}
}
@article{Tadepalli2004,
abstract = {Relational reinforcement learning (RRL) is both a young and an old eld. In this paper, we trace the history of the eld to related disciplines, outline some current work and promising new directions, and survey the research issues and opportunities that lie ahead.},
author = {Tadepalli, Prasad and Givan, Robert and Driessens, Kurt},
doi = {10.1109/ICAL.2009.5262787},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tadepalli, Givan, Driessens - 2004 - Relational reinforcement learning An overview.pdf:pdf},
issn = {00747696},
journal = {ICML workshop on Relational Reinforcement Learning},
pages = {1--9},
title = {{Relational reinforcement learning: An overview}},
volume = {4},
year = {2004}
}
@article{Evans2003,
abstract = {Researchers in thinking and reasoning have proposed recently that there are two distinct cognitive systems underlying reasoning. System 1 is old in evolutionary terms and shared with other animals: it comprises a set of autonomous subsystems that include both innate input modules and domain-specific knowledge acquired by a domain-general learning mechanism. System 2 is evolutionarily recent and distinctively human: it permits abstract reasoning and hypothetical thinking, but is constrained by working memory capacity and correlated with measures of general intelligence. These theories essentially posit two minds in one brain with a range of experimental psychological evidence showing that the two systems compete for control of our inferences and actions.},
author = {Evans, Jonathan St B T},
doi = {10.1016/j.tics.2003.08.012},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans - 2003 - In two minds Dual-process accounts of reasoning.pdf:pdf},
isbn = {13646613},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {10},
pages = {454--459},
pmid = {14550493},
publisher = {Elsevier Ltd},
title = {{In two minds: Dual-process accounts of reasoning}},
volume = {7},
year = {2003}
}
@article{Nathan2003,
author = {Nathan, Mitchell J and Petrosino, Anthony},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nathan, Petrosino - 2003 - Expert Blind Spot Among Pre-Service Teachers.pdf:pdf},
journal = {American Educational Research Journal},
number = {4},
pages = {905--928},
title = {{Expert Blind Spot Among Pre-Service Teachers}},
volume = {40},
year = {2003}
}
@incollection{Gentner2003,
author = {Gentner, Dedre},
booktitle = {Language in mind: Advances in the study of language and thought.},
pages = {195--235},
title = {{Why We're So Smart}},
year = {2003}
}
@article{McClelland2003,
abstract = {How do we know what properties something has, and which of its properties should be generalized to other objects? How is the knowledge underlying these abilities acquired, and how is it affected by brain disorders? Our approach to these issues is based on the idea that cognitive processes arise from the interactions of neurons through synaptic connections. The knowledge in such interactive and distributed processing systems is stored in the strengths of the connections and is acquired gradually through experience. Degradation of semantic knowledge occurs through degradation of the patterns of neural activity that probe the knowledge stored in the connections. Simulation models based on these ideas capture semantic cognitive processes and their development and disintegration, encompassing domain-specific patterns of generalization in young children, and the restructuring of conceptual knowledge as a function of experience.},
author = {McClelland, J.L. and Rogers, T.T.},
doi = {10.1038/nrn1076},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland, Rogers - 2003 - The parallel distributed processing approach to semantic cognition.pdf:pdf},
isbn = {1471003X},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
keywords = {Animals,Cerebral Cortex,Cerebral Cortex: anatomy {\&} histology,Cerebral Cortex: physiology,Cognition,Cognition: physiology,Humans,Learning,Learning Disorders,Learning Disorders: physiopathology,Learning: physiology,Models, Neurological,Nerve Net,Nerve Net: anatomy {\&} histology,Nerve Net: physiology,Neural Pathways,Neural Pathways: anatomy {\&} histology,Neural Pathways: physiology,Semantics},
number = {4},
pages = {310--322},
pmid = {12671647},
title = {{The parallel distributed processing approach to semantic cognition}},
volume = {4},
year = {2003}
}
@article{Dehaene2003,
abstract = {Did evolution endow the human brain with a predisposition to represent and acquire knowledge about numbers? Although the parietal lobe has been suggested as a potential substrate for a domain-specific representation of quantities, it is also engaged in verbal, spatial, and attentional functions that may contribute to calculation. To clarify the organisation of number-related processes in the parietal lobe, we examine the three-dimensional intersection of fMRI activations during various numerical tasks, and also review the corresponding neuropsychological evidence. On this basis, we propose a tentative tripartite organisation. The horizontal segment of the intraparietal sulcus (HIPS) appears as a plausible candidate for domain specificity: It is systematically activated whenever numbers are manipulated, independently of number notation, and with increasing activation as the task puts greater emphasis on quantity processing. Depending on task demands, we speculate that this core quantity system, analogous to an internal "number line," can be supplemented by two other circuits. A left angular gyrus area, in connection with other left-hemispheric perisylvian areas, supports the manipulation of numbers in verbal form. Finally, a bilateral posterior superior parietal system supports attentional orientation on the mental number line, just like on any other spatial dimension.},
author = {Dehaene, Stanislas and Piazza, Manuela and Pinel, Philippe and Cohen, Laurent},
doi = {10.1080/02643290244000239},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dehaene et al. - 2003 - Three parietal circuits for number processing.pdf:pdf},
isbn = {0264329024400},
issn = {0264-3294},
journal = {Cognitive neuropsychology},
number = {July 2015},
pages = {487--506},
pmid = {20957581},
title = {{Three parietal circuits for number processing.}},
volume = {20},
year = {2003}
}
@article{Delazer2003,
author = {Delazer, M and Domahs, F and Bartha, L and Brenneis, C and Lochy, a and Trieb, T and Benke, T},
doi = {10.1016/j.cogbrainres.2003.09.005},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Delazer et al. - 2003 - Learning complex arithmetic—an fMRI study.pdf:pdf},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {angular gyrus,arithmetic,fmri,learning},
number = {1},
pages = {76--88},
title = {{Learning complex arithmetic—an fMRI study}},
volume = {18},
year = {2003}
}
@article{Bartlett2002,
author = {Bartlett, Peter L},
file = {:home/andrew/Documents/grad/Papers/bartlett02a.pdf:pdf},
keywords = {data-dependent complexity,error bounds,maxi-,rademacher averages},
pages = {463--482},
title = {{Rademacher and Gaussian Complexities : Risk Bounds and Structural Results}},
volume = {3},
year = {2002}
}
@article{Ng2002,
author = {Ng, Andrew Y and Jordan, Michael I},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng, Jordan - 2002 - On Discriminative vs. Generative classifiers A comparison of logistic regression and naive Bayes.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {841--848},
title = {{On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes}},
year = {2002}
}
@incollection{Cleeremans2002,
author = {Cleeremans, Axel and Jim{\'{e}}nez, Luis},
booktitle = {Implicit Learning and Consciousness: An Empirical, Philosophical and Computational Consensus in the Making (Frontiers of Cognitive Science)},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cleeremans, Jim{\'{e}}nez - 2002 - Implicit learning and consciousness A graded, dynamic perspective.pdf:pdf},
title = {{Implicit learning and consciousness: A graded, dynamic perspective}},
year = {2002}
}
@article{Lewis2002,
abstract = {The question of whether age-of-acquisition (AoA), frequency, and repetition priming effects occur at a common stage or at different stages of processing is addressed. Two single-stage accounts (i.e., cumulative frequency and a neural-network simulation) are considered in regard to their predictions concerning the interactions between AoA and frequency with aging and priming effects. A repetition-priming face-classification task was conducted on both older and younger participants to test these predictions. Consistent with the predictions of the neural-network simulation, AoA had an effect on reaction times that could not be explained by cumulative frequency alone. Also, as predicted by the simulation, the size of the priming effect was determined by the cumulative frequency of the item. It is discussed how this evidence is supportive of the notion that AoA , frequency, and priming all have effects at a common and single stage during face processing.},
author = {Lewis, Michael B and Chadwick, Andrea J and Ellis, Hadyn D},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lewis, Chadwick, Ellis - 2002 - Exploring a neural-network account of age-of-acquisition effects using repetition priming of faces.pdf:pdf},
isbn = {0090-502X (Print)$\backslash$r0090-502X (Linking)},
issn = {0090-502X},
journal = {Memory {\&} Cognition},
keywords = {80 and over,Adolescent,Adult,Age Factors,Aged,Aging,Face,Female,Humans,Learning,Male,Middle Aged,Nerve Net,Nerve Net: physiology,Periodicity,Photic Stimulation,Reaction Time,Recognition (Psychology)},
number = {8},
pages = {1228--1237},
pmid = {12661854},
title = {{Exploring a neural-network account of age-of-acquisition effects using repetition priming of faces.}},
volume = {30},
year = {2002}
}
@article{Barnett2002,
abstract = {Despite a century's worth of research, arguments surrounding the question of whether far transfer occurs have made little progress toward resolution. The authors argue the reason for this confusion is a failure to specify various dimensions along which transfer can occur, resulting in comparisons of "apples and oranges." They provide a framework that describes 9 relevant dimensions and show that the literature can productively be classified along these dimensions, with each study situated at the intersection of various dimensions. Estimation of a single effect size for far transfer is misguided in view of this complexity. The past 100 years of research shows that evidence for transfer under some conditions is substantial, but critical conditions for many key questions are untested.},
author = {Barnett, Susan M and Ceci, Stephen J},
doi = {10.1037/0033-2909.128.4.612},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barnett, Ceci - 2002 - When and where do we apply what we learn A taxonomy for far transfer.pdf:pdf},
isbn = {0033-2909},
issn = {0033-2909},
journal = {Psychological bulletin},
number = {4},
pages = {612--637},
pmid = {12081085},
title = {{When and where do we apply what we learn? A taxonomy for far transfer.}},
volume = {128},
year = {2002}
}
@article{Menon2002,
author = {Menon, Vinod and Mackenzie, Katherine and Rivera, Susan Michelle and Reiss, Allan Leonard},
doi = {10.1002/hbm.10035},
issn = {1065-9471},
journal = {Human Brain Mapping},
keywords = {angular gyrus,arithmetic,fmri,interference,n400,prefrontal cortex,stroop},
number = {2},
pages = {119--130},
title = {{Prefrontal cortex involvement in processing incorrect arithmetic equations: Evidence from event-related fMRI}},
volume = {16},
year = {2002}
}
@book{Lang2002,
author = {Lang, Serge},
booktitle = {Graduate Texts in Mathematics-New York-},
edition = {Rev. 3rd},
isbn = {038795385X},
pages = {914},
publisher = {Springer Graduate Texts in Mathematics},
title = {{Algebra (revised third edition)}},
year = {2002}
}
@article{Menon2002,
author = {Menon, Vinod and Mackenzie, Katherine and Rivera, Susan Michelle and Reiss, Allan Leonard},
doi = {10.1002/hbm.10035},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Menon et al. - 2002 - Prefrontal cortex involvement in processing incorrect arithmetic equations Evidence from event-related fMRI.pdf:pdf},
issn = {1065-9471},
journal = {Human Brain Mapping},
keywords = {angular gyrus,arithmetic,fmri,interference,n400,prefrontal cortex,stroop},
number = {2},
pages = {119--130},
title = {{Prefrontal cortex involvement in processing incorrect arithmetic equations: Evidence from event-related fMRI}},
volume = {16},
year = {2002}
}
@incollection{Wilson2001,
author = {Wilson, William H and Halford, Graeme S and Gray, Brett and Phillips, Steven},
booktitle = {The analogical mind},
editor = {Gentner, Dedre and Holyoak, Keith J. and Kokinov, B. N.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson et al. - 2001 - The STAR-2 Model for Mapping Hierarchically Structure Analogs.pdf:pdf},
pages = {125--60},
title = {{The STAR-2 Model for Mapping Hierarchically Structure Analogs}},
year = {2001}
}
@article{Tunney2001,
abstract = {Participants can transfer grammatical knowledge acquired implicitly in 1 vocabulary to new sequences instantiated in both the same and a novel vocabulary. Two principal theories have been advanced to account for these effects. One suggests that sequential dependencies form the basis for cross-domain transfer (e.g., Z. Dienes, G. T. M. Altmann, {\&} S. J. Gao, 1999). Another argues that a form of episodic memory known as abstract analogy is sufficient (e.g., L. R. Brooks {\&} J. R. Vokey, 1991). Three experiments reveal the contributions of the 2. In Experiment 1 sequential dependencies form the only basis for transfer. Experiment 2 demonstrates that this process is impaired by a change in the distributional properties of the language. Experiment 3 demonstrates that abstract analogy of repetition structure is relatively immune to such a change. These findings inform theories of artificial grammar learning and the transfer of grammatical knowledge.},
author = {Tunney, R J and Altmann, Gerry T. M.},
doi = {10.1037//0278-7393.27.3.614},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tunney, Altmann - 2001 - Two modes of transfer in artificial grammar learning.pdf:pdf},
isbn = {0278-7393},
issn = {0278-7393},
journal = {Journal of experimental psychology. Learning, memory, and cognition},
keywords = {Adult,Female,Humans,Learning,Linguistics,Male,Models,Psychological,Serial Learning,Transfer (Psychology),Vocabulary},
number = {3},
pages = {614--39},
pmid = {11394670},
title = {{Two modes of transfer in artificial grammar learning.}},
volume = {27},
year = {2001}
}
@article{Deci2001,
author = {Deci, Edward L and Koestner, Richard and Ryan, Richard M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deci, Koestner, Ryan - 2001 - Extrinsic Rewards and Intrinsic Motivation in Education Reconsidered Once Again.pdf:pdf},
journal = {Review of Educational Research},
number = {1},
pages = {1--27},
title = {{Extrinsic Rewards and Intrinsic Motivation in Education: Reconsidered Once Again}},
volume = {71},
year = {2001}
}
@article{Gobet2001,
abstract = {Pioneering work in the 1940s and 1950s suggested that the concept of ‘chunking' might be important in many processes of perception, learning and cognition in humans and animals. We summarize here the major sources of evidence for chunking mechanisms, and consider how such mechanisms have been implemented in computational models of the learning process. We distinguish two forms of chunking: the first deliberate, under strategic control, and goal-oriented; the second automatic, continuous, and linked to perceptual processes. Recent work with discrimination-network computational models of long- and short-term memory (EPAM/CHREST) has produced a diverse range of applications of perceptual chunking. We focus on recent successes in verbal learning, expert memory, language acquisition and learning multiple representations, to illustrate the implementation and use of chunking mechanisms within contemporary models of human learning.},
author = {Gobet, Fernand. and Lane, Peter C R. and Croker, Steve. and Cheng, Peter C-H. and Jones, Gary. and Oliver, Iain. and Pine, Julian M.},
doi = {10.1016/S1364-6613(00)01662-4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gobet et al. - 2001 - Chunking Mechanisms in Human Learning.pdf:pdf},
isbn = {1364-6613},
issn = {1879-307X},
journal = {Trends in Cognitive Sciences},
number = {6},
pages = {236--243},
pmid = {11390294},
title = {{Chunking Mechanisms in Human Learning}},
volume = {5},
year = {2001}
}
@article{Pesenti2001,
abstract = {Calculating prodigies are individuals who are exceptional at quickly and accurately solving complex mental calculations. With positron emission tomography (PET), we investigated the neural bases of the cognitive abilities of an expert calculator and a group of non-experts, contrasting complex mental calculation to memory retrieval of arithmetic facts. We demonstrated that calculation expertise was not due to increased activity of processes that exist in non-experts; rather, the expert and the non-experts used different brain areas for calculation. We found that the expert could switch between short-term effort-requiring storage strategies and highly efficient episodic memory encoding and retrieval, a process that was sustained by right prefrontal and medial temporal areas.},
annote = {Uncorrected p-values},
author = {Pesenti, M and Zago, L and Crivello, F and Mellet, E and Samson, D and Duroux, B and Seron, X and Mazoyer, B and Tzourio-Mazoyer, N},
doi = {10.1038/82831},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pesenti et al. - 2001 - Mental calculation in a prodigy is sustained by right prefrontal and medial temporal areas.pdf:pdf},
isbn = {1097-6256 (Print)$\backslash$r1097-6256 (Linking)},
issn = {1097-6256},
journal = {Nature neuroscience},
pages = {103--107},
pmid = {11135652},
title = {{Mental calculation in a prodigy is sustained by right prefrontal and medial temporal areas.}},
volume = {4},
year = {2001}
}
@article{Prabhakaran2001,
abstract = {Brain activation was examined using functional magnetic resonance imaging during mathematical problem solving in 7 young healthy participants. Problems were selected from the Necessary Arithmetic Operations Test (NAOT; R. B. Ekstrom, J. W. French, H. H. Harman, {\&} D. Dermen, 1976). Participants solved 3 types of problems: 2-operation problems requiring mathematical reasoning and text processing, 1-operation problems requiring text processing but minimal mathematical reasoning, and 0-operation problems requiring minimal text processing and controlling sensorimotor demands of the NAOT problems. Two-operation problems yielded major activations in bilateral frontal regions similar to those found in other problem-solving tasks, indicating that the processes mediated by these regions subserve many forms of reasoning. Findings suggest a dissociation in mathematical problem solving between reasoning, mediated by frontal cortex, and text processing, mediated by temporal cortex.},
author = {Prabhakaran, V and Rypma, B and Gabrieli, J D},
doi = {10.1037/0894-4105.15.1.115},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prabhakaran, Rypma, Gabrieli - 2001 - Neural substrates of mathematical reasoning a functional magnetic resonance imaging study of neoco.pdf:pdf},
isbn = {0894-4105 (Print)},
issn = {0894-4105},
journal = {Neuropsychology},
number = {1},
pages = {115--127},
pmid = {11216882},
title = {{Neural substrates of mathematical reasoning: a functional magnetic resonance imaging study of neocortical activation during performance of the necessary arithmetic operations test.}},
volume = {15},
year = {2001}
}
@article{Weber2001,
abstract = {The ability to construct proofs is an important skill for all mathematicians. Despite its importance, students have great difficulty with this task. In this paper, I first demonstrate that undergraduates often are aware of and able to apply the facts required to prove a statement but still fail to prove it. They thus fail to construct a proof because they could not use the syntactic knowledge that they had. By comparing doctoral students and undergraduates constructing proofs in abstract algebra, I have hypothesized four types of `strategic knowledge' – knowledge of how to choose which facts and theorems to apply – which the doctoral students appeared to possess and undergraduates did not. The doctoral students appeared to know the powerful proof techniques in abstract algebra, which theorems are most important, when particular facts and theorems are likely to be useful, and when one should or should not try and prove theorems using symbol manipulation.},
author = {Weber, Keith},
doi = {10.1023/A:1015535614355},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weber - 2001 - Student difficulty in constructing proofs The need for strategic knowledge.pdf:pdf},
issn = {0013-1954, 1573-0816},
journal = {Educational Studies in Mathematics},
keywords = {abstract algebra,group theory,homomorphism,mathematics education},
number = {1},
pages = {101--119},
title = {{Student difficulty in constructing proofs: The need for strategic knowledge}},
volume = {48},
year = {2001}
}
@article{Gorea2000,
author = {Gorea, Andrei and Sagi, Dov},
file = {:home/andrew/Documents/grad/Papers/12380.full.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {22},
title = {{Failure to handle more than one internal representation in visual detection tasks}},
volume = {97},
year = {2000}
}
@article{Loughlin2000,
author = {Loughlin, Claire O and Thagard, Paul},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Loughlin, Thagard - 2000 - Autism and Coherence A Computational Model.pdf:pdf},
journal = {Mind {\&} Language},
number = {4},
pages = {375--392},
title = {{Autism and Coherence: A Computational Model}},
volume = {15},
year = {2000}
}
@article{Thompson2000,
author = {Thompson, Roger K.R. and Oden, David L.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thompson, Oden - 2000 - Categorical Perception and Conceptual Judgments by Nonhuman Primates The Paleological Monkey and the Analogical.pdf:pdf},
journal = {Cognitive Science},
number = {3},
pages = {363--396},
title = {{Categorical Perception and Conceptual Judgments by Nonhuman Primates : The Paleological Monkey and the Analogical Ape}},
volume = {24},
year = {2000}
}
@article{Chinn2000,
abstract = {A systematic review may encompass both odds ratios and mean differences in continuous outcomes. A separate meta-analysis of each type of outcome results in loss of information and may be misleading. It is shown that a ln(odds ratio) can be converted to effect size by dividing by 1.81. The validity of effect size, the estimate of interest divided by the residual standard deviation, depends on comparable variation across studies. If researchers routinely report residual standard deviation, any subsequent review can combine both odds ratios and effect sizes in a single meta-analysis when this is justified.},
author = {Chinn, Susan},
doi = {10.1002/1097-0258(20001130)19:22<3127::AID-SIM784>3.0.CO;2-M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chinn - 2000 - A simple method for converting an odds ratio to effect size for use in meta-analysis.pdf:pdf},
isbn = {0277-6715 (Print)$\backslash$r0277-6715 (Linking)},
issn = {0277-6715},
journal = {Statistics in Medicine},
number = {22},
pages = {3127--3131},
pmid = {11113947},
title = {{A simple method for converting an odds ratio to effect size for use in meta-analysis}},
volume = {19},
year = {2000}
}
@article{Fukumizu2000,
abstract = {This paper proposes new methods for generating input locations actively in gathering training data, aiming at solving problems unique to multilayer perceptrons. One of the problems is that optimum input locations, which are calculated deterministically, sometimes distribute densely around the same point and cause local minima in backpropagation training. Two probabilistic active learning methods, which utilize the statistical variance of locations, are proposed to solve this problem. One is parametric active learning and the other is multipoint-search active learning. Another serious problem in applying active learning to multilayer perceptrons is that a Fisher information matrix can be singular, while many methods, including the proposed ones, assume its regularity. A technique of pruning redundant hidden units is proposed to keep the Fisher information matrix regular. Combined with this technique, active learning can be applied stably to multilayer perceptrons. The effectiveness of the proposed methods is demonstrated through computer simulations on simple artificial problems and a real-world problem of color conversion.},
author = {Fukumizu, Kenji},
doi = {10.1109/72.822506},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fukumizu - 2000 - Statistical active learning in multilayer perceptrons.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {active learning,fisher,information matrix,multilayer perceptron,pruning},
number = {1},
pages = {17--26},
pmid = {18249735},
title = {{Statistical active learning in multilayer perceptrons}},
volume = {11},
year = {2000}
}
@article{Alahakoon2000,
abstract = {The growing self-organizing map (GSOM) has been presented as an extended version of the self-organizing map (SOM), which has significant advantages for knowledge discovery applications. In this paper, the GSOM algorithm is presented in detail and the effect of a spread factor, which can be used to measure and control the spread of the GSOM, is investigated. The spread factor is independent of the dimensionality of the data and as such can be used as a controlling measure for generating maps with different dimensionality, which can then be compared and analyzed with better accuracy. The spread factor is also presented as a method of achieving hierarchical clustering of a data set with the GSOM. Such hierarchical clustering allows the data analyst to identify significant and interesting clusters at a higher level of the hierarchy, and as such continue with finer clustering of only the interesting clusters. Therefore, only a small map is created in the beginning with a low spread factor, which can be generated for even a very large data set. Further analysis is conducted on selected sections of the data and as such of smaller volume. Therefore, this method facilitates the analysis of even very large data sets.},
author = {Alahakoon, D and Halgamuge, S K and Srinivasan, B},
doi = {10.1109/72.846732},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alahakoon, Halgamuge, Srinivasan - 2000 - Dynamic Self-Organizing Maps with Controlled Growth for Knoledge Discovery.pdf:pdf},
issn = {1045-9227},
journal = {IEEE Trans. on Neural Networks},
number = {3},
pages = {601--14},
pmid = {18249788},
title = {{Dynamic Self-Organizing Maps with Controlled Growth for Knoledge Discovery}},
volume = {11},
year = {2000}
}
@article{Atkinson2000,
abstract = {Worked examples are instructional devices that provide an expert's problem solution for a learner to study. Worked-examples research is a cognitive-experimental program that has relevance to classroom in- struction and the broader educational research community. A frame- work for organizing the findings of this research is proposed, leading to instructional design principles. For instance, one instructional de- sign principle suggests that effective examples have highly integrated components. They employ multiple modalities in presentation and em- phasize conceptual structure by labeling or segmenting. At the lesson level, effective instruction employs multiple examples for each concep- tual problem type, varies example formats within problem type, and employs surface features to signal deep structure. Also, examples should be presented in close proximity to matched practice problems. More- over, learners can be encouraged through direct training or by the structure of the worked example to actively self-explain examples. Worked examples are associated with early stages of skill develop- ment, but the design principles are relevant to constructivist research and teaching. The},
author = {Atkinson, Robert K. and Derry, Sharon J. and Renkl, Alexander and Wortham, Donald},
doi = {10.3102/00346543070002181},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Atkinson et al. - 2000 - Learning from examples Instructional principles from the worked examples research.pdf:pdf},
isbn = {0034654307000},
issn = {0034-6543},
journal = {Review of Educational Research},
number = {2},
pages = {181--214},
title = {{Learning from examples: Instructional principles from the worked examples research}},
volume = {70},
year = {2000}
}
@article{Dienes1999,
abstract = {This paper shows how a neural network can model the way people who have acquired knowledge of an artificial grammar in one perceptual domain (e.g., sequences of tones differing in pitch) can apply the knowledge to a quite different perceptual domain (e.g., sequences of letters). it is shown that a version of the Simple Recurrent Network (SRN) can transfer its knowledge of artificial grammars across domains without feedback. The performance of the model is sensitive to at least some of the same variables that affect subjects' performance-for example, the model is responsive to both the grammaticality of test sequences and their similarity to training sequences, to the cover task used during training, and to whether training is on bigrams or larger sequences.},
author = {Dienes, Zolt{\'{a}}n and Altmann, Gerry T. M. and Gao, Shi-Ji},
doi = {10.1207/s15516709cog2301},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dienes, Altmann, Gao - 1999 - Mapping across Domains Without Feedback A Neural Network Model of Transfer of Implicit Knowledge.pdf:pdf},
issn = {0364-0213},
journal = {Cognitive Science},
number = {1},
pages = {53--82},
title = {{Mapping across Domains Without Feedback: A Neural Network Model of Transfer of Implicit Knowledge}},
volume = {23},
year = {1999}
}
@article{McClelland1999,
author = {McClelland, James L and Plaut, David C},
doi = {10.1016/S1364-6613(99)01320-0},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland, Plaut - 1999 - Does generalization in infant learning implicate abstract algebra-like rules.pdf:pdf},
isbn = {1879-307X (Electronic)$\backslash$r1364-6613 (Linking)},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {5},
pages = {166--168},
pmid = {10322471},
title = {{Does generalization in infant learning implicate abstract algebra-like rules?}},
volume = {3},
year = {1999}
}
@article{Pajares1999,
author = {Pajares, Frank and Graham, Laura},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pajares, Graham - 1999 - Self-Efficacy , Motivation Constructs , and Mathematics Performance of Entering Middle School Students.pdf:pdf},
pages = {124--139},
title = {{Self-Efficacy , Motivation Constructs , and Mathematics Performance of Entering Middle School Students}},
volume = {139},
year = {1999}
}
@article{Bransford1999,
author = {Bransford, John D and Schwartz, Daniel L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bransford, Schwartz - 1999 - Rethinking Transfer A Simple Proposal With Multiple Implications.pdf:pdf},
journal = {Review of Research in Education},
number = {1},
pages = {61--100},
title = {{Rethinking Transfer : A Simple Proposal With Multiple Implications}},
volume = {24},
year = {1999}
}
@article{Dehaene1999,
abstract = {Does the human capacity for mathematical intuition depend on linguistic competence or on visuo-spatial representations? A series of behavioral and brain-imaging experiments provides evidence for both sources. Exact arithmetic is acquired in a language-specific format, transfers poorly to a different language or to novel facts, and recruits networks involved in word-association processes. In contrast, approximate arithmetic shows language independence, relies on a sense of numerical magnitudes, and recruits bilateral areas of the parietal lobes involved in visuo-spatial processing. Mathematical intuition may emerge from the interplay of these brain systems.},
author = {Dehaene, S. and Spelke, E. and Pinel, P. and Stanescu, R. and Tsivkin, S.},
doi = {10.1126/science.284.5416.970},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dehaene et al. - 1999 - Sources of Mathematical Thinking Behavioral and Brain-Imaging Evidence.pdf:pdf},
isbn = {00368075 10959203},
issn = {0036-8075, 1095-9203},
journal = {Science},
number = {5416},
pages = {970--974},
pmid = {10320379},
title = {{Sources of Mathematical Thinking: Behavioral and Brain-Imaging Evidence}},
volume = {284},
year = {1999}
}
@article{Hazzan1999,
annote = {Abstraction:
1) Quality of relationships between object of thought and person
- Familiarity, etc.
2) process-object duality/conversion
-E.g. from structural understanding of group as how to perform operation to group as object
-E.g. commutativity as something that happens when *I* do things with group, vs. abstract property 
-Difference between explicit and implicit knowledge in terms of process vs object
3) degree of complexity of object of thought
-E.g. replacing set with an exemplar from it, reasoning about cyclic groups generally in terms of a specific example},
author = {Hazzan, Orit},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hazzan - 1999 - Reducing Abstraction Level When Learning Abstract Algebra Concepts.pdf:pdf},
journal = {Educational Studies in Mathematics},
pages = {71--90},
title = {{Reducing Abstraction Level When Learning Abstract Algebra Concepts}},
volume = {40},
year = {1999}
}
@article{Li1998,
author = {Li, Ren-cang},
file = {:home/andrew/Documents/grad/Papers/relpthyI.pdf:pdf},
isbn = {0895479896298},
journal = {SIAM J. Matrix Anal. Appl.},
keywords = {15a18,15a42,65f15,65f35,65g99,ams subject classifications,eigenvector,graded matrix,multiplicative perturbation,pii,relative gap,relative perturbation theory,s0895479896298506,singular vector,structured sylvester equation},
number = {4},
pages = {956--982},
title = {{Relative Perturbation Theory: Eigenvalue and Singular Value Variations}},
volume = {19},
year = {1998}
}
@article{Siegler1998,
author = {Siegler, R and Stern, E},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siegler, Stern - 1998 - Conscious and unconscious strategy discovery A microgenetic analysis.pdf:pdf},
journal = {Journal of Experimental Psychology: General},
number = {4},
pages = {377--397},
title = {{Conscious and unconscious strategy discovery: A microgenetic analysis}},
volume = {127},
year = {1998}
}
@article{Brainard1998,
abstract = {Previous studies have identified sensitive periods for the developing barn owl during which visual experience has a powerful influence on the calibration of sound localization behavior. Here we investigated neural correlates of these sensitive periods by assessing developmental changes in the capacity of visual experience to alter the map of auditory space in the optic tectum of the barn owl. We used two manipulations. (1) We equipped owls with prismatic spectacles that optically displaced the visual field by 23 degrees to the left or right, and (2) we restored normal vision to prism-reared owls that had been raised wearing prisms. In agreement with previous behavioral experiments, we found that the capacity of abnormal visual experience to shift the tectal auditory space map was restricted to an early sensitive period. However, this period extended until later in life (approximately 200 d) than described previously in behavioral studies (approximately 70 d). Furthermore, unlike the previous behavioral studies that found that the capacity to recover normal sound localization after restoration of normal vision was lost at approximately 200 d of age, we found that the capacity to recover a normal auditory space map was never lost. Finally, we were able to reconcile the behaviorally and neurophysiologically defined sensitive periods by taking into account differences in the richness of the environment in the two sets of experiments. We repeated the behavioral experiments and found that when owls were housed in a rich environment, the capacity to adjust sound localization away from normal extended to later in life, whereas the capacity to recover to normal was never lost. Conversely, when owls were housed in an impoverished environment, the capacity to recover a normal auditory space map was restricted to a period ending at approximately 200 d of age. The results demonstrate that the timing and even the existence of sensitive periods for plasticity of a neural circuit and associated behavior can depend on multiple factors, including (1) the nature of the adjustment demanded of the system and (2) the richness of the sensory and social environment in which the plasticity is studied.},
author = {Brainard, Michael S. and Knudsen, Eric I.},
doi = {papers://47831562-1F78-4B52-B52E-78BF7F97A700/Paper/p11},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brainard, Knudsen - 1998 - Sensitive periods for visual calibration of the auditory space map in the barn owl optic tectum.pdf:pdf},
isbn = {0270-6474},
issn = {0270-6474},
journal = {The Journal of Neuroscience},
keywords = {auditory map,auditory plastic-,barn owl,critical period,ity,knudsen and knud-,of the auditory stimulus,optic tectum,sensitive period,sound localization,space map,superior colliculus,through the prisms,tyto alba},
number = {10},
pages = {3929--3942},
pmid = {9570820},
title = {{Sensitive periods for visual calibration of the auditory space map in the barn owl optic tectum.}},
volume = {18},
year = {1998}
}
@article{Bottou1998,
abstract = {An abstract is not available.},
author = {Bottou, L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bottou - 1998 - Online learning and stochastic approximations.pdf:pdf},
isbn = {978-0521117913},
journal = {On-line learning in neural networks},
pages = {1--34},
title = {{Online learning and stochastic approximations}},
year = {1998}
}
@article{Vijayakumar1998,
author = {Vijayakumar, Sethu and Sugiyama, Masashi and Ogawa, Hidemitsu},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vijayakumar, Sugiyama, Ogawa - 1998 - Training Data Selection for Optimal Generalization with Noise Variance Reduction in Neural Network.pdf:pdf},
title = {{Training Data Selection for Optimal Generalization with Noise Variance Reduction in Neural Networks Functional analytic framework for NN learning}},
year = {1998}
}
@article{Lesser1998,
author = {Lesser, Larry},
doi = {10.1111/j.1467-9639.1998.tb00750.x},
issn = {0141-982X},
journal = {Teaching Statistics},
number = {1},
pages = {10--12},
title = {{Countering Indifference Using Counterintuitive Examples}},
volume = {20},
year = {1998}
}
@article{Hummel1997,
abstract = {This article describes an integrated theory of analogical access and mapping, instantiated in a computational model called LISA (Learning and Inference with Schemas and Analogies). LISA represents predicates and objects as distributed patterns of activation that are dynamically bound into propositional structures, thereby achieving both the flexibility of a connectionist system and the structure sensitivity of a symbolic system. The model treats access and mapping as types of guided pattern classification, differing only in that mapping is augmented by a capacity to learn new correspondences. The resulting model simulates a wide range of empirical findings concerning human analogical access and mapping. LISA also has a number of inherent limitations, including capacity limits, that arise in human reasoning and suggests a specific computational account of these limitations. Extensions of this approach also account for analogical inference and schema induction.},
archivePrefix = {arXiv},
arxivId = {1112.4045},
author = {Hummel, John E. and Holyoak, Keith J.},
doi = {10.1037/0033-295X.104.3.427},
eprint = {1112.4045},
isbn = {0033-295X$\backslash$r1939-1471},
issn = {0033295X},
journal = {Psychological Review},
number = {3},
pages = {427--466},
pmid = {1000104248},
title = {{Distributed representations of structure: A theory of analogical access and mapping.}},
volume = {104},
year = {1997}
}
@book{Hiebert1997,
annote = {NULL},
author = {Hiebert, James and Carpenter, Thomas P. and Fennena, Elizabeth and Fuson, Karen C. and Wearne, Diana and Murray, Hanlie and Olivier, Alwyn and Human, Piet and Lindquist, Mary M.},
title = {{Making Sense: Teaching and learning mathematics with understanding}},
year = {1997}
}
@article{Greeno1997,
abstract = {Anderson, Reder, and Simon (1996) contested four propositions that they incorrectly called "claims of situated learning." This response argues that the important differences between situative and cognitive perspectives are not addressed by discussion of these imputed claims. Instead, there are significant differences in the framing assumptions of the two perspectives. I clarify these differences by inferring questions to which Anderson et al.'s discussion provided answers, by identifying presuppositions of those questions made by Anderson et al., and by stating the different presuppositions and questions that I believe are consistent with the situative perspective. The evidence given by Anderson et al. is compatible with the framing assumptions of situativity; therefore, deciding between the perspectives will involve broader considerations than those presented in their article. These considerations include expectations about which framework offers the better prospect for developing a unified scientific account of activity considered from both social and individual points of view, and which framework supports research that will inform discussions of educational practice more productively. The cognitive perspective takes the theory of individual cognition as its basis and builds toward a broader theory by incrementally developing analyses of additional components that are considered as contexts. The situative perspective takes the theory of social and ecological interaction as its basis and builds toward a more comprehensive theory by developing increasingly detailed analyses of information structures in the contents of people's interactions. While I believe that the situative framework is more promising, the best strategy for the field is for both perspectives to be developed energetically.},
author = {Greeno, J. G.},
doi = {10.3102/0013189X026001005},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greeno - 1997 - On Claims That Answer the Wrong Questions.pdf:pdf},
isbn = {0013189X},
issn = {0013-189X},
journal = {Educational Researcher},
number = {1},
pages = {5--17},
title = {{On Claims That Answer the Wrong Questions}},
volume = {26},
year = {1997}
}
@article{Thompson1997,
author = {Thompson, Roger K.R. and Oden, David L. and Boysen, Sarah T.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thompson, Oden, Boysen - 1997 - Language-Naive Chimpanzees ( Pan troglodytes ) Judge Relations Between Relations in a Conceptual Matchin.pdf:pdf},
journal = {Journal of Experimental Psychology: Animal Behavior Processes},
number = {1},
pages = {31--43},
title = {{Language-Naive Chimpanzees ( Pan troglodytes ) Judge Relations Between Relations in a Conceptual Matching-to-Sample Task}},
volume = {23},
year = {1997}
}
@article{Zhang1997,
abstract = {This article proposes a theoretical framework for external representation based problem solving. The Tic-Tat-Toe and its isomorphs are used to illustrate the procedures of the framework as a methodology and test the predictions of the framework as a functional model. Experimental results show that the behavior in the Tic-Tat-Toe is determined by the directly available information in external and internal representations in terms of perceptual and cognitive biases, regardless of whether the biases are consistent with, inconsistent with, or irrelevant to the task. It is shown that external representations are not merely inputs and stimuli to the internal mind and that they have much more important functions than mere memory aids. A representational determinism is suggested-the form of a representation determines what information can be perceived, what processes can be activated, and what structures can be discovered from the specific representation.},
author = {Zhang, Jiajie},
doi = {10.1207/s15516709cog2102_3},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
number = {2},
pages = {179--217},
pmid = {9712194170},
title = {{The Nature Problem of External in Solving Representations}},
volume = {21},
year = {1997}
}
@article{Rohde1997,
abstract = {Contra Elman (e.g., 1991), starting iwth simplified inputs is not necessary for training recurrent neural networks to learn pseudo-natural languages.},
author = {Rohde, Dlt and Plaut, Dc},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rohde, Plaut - 1997 - Simple recurrent networks and natural language How important is starting small.pdf:pdf},
journal = {Proceedings of the 19th annual conference of the Cognitive Science Society},
pages = {656--661},
title = {{Simple recurrent networks and natural language: How important is starting small}},
year = {1997}
}
@article{Lin1997,
author = {Lin, F-R. and Shaw, M J},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Shaw - 1997 - Active training of backpropagation neural networks using the learning by experimentation methodology.pdf:pdf},
issn = {02545330},
journal = {Annals of Operations Research},
keywords = {active learning,learning by experimentation methodology,neural networks},
pages = {129--145},
title = {{Active training of backpropagation neural networks using the learning by experimentation methodology}},
volume = {75},
year = {1997}
}
@article{Dienes1997,
abstract = {Explored the extent to which unconscious knowledge could be applied flexibly in 2 experiments. In Exp 1, using 48 Ss, transfer was investigated in a case where the mapping between the domains was transparent so any transfer decrement must arise because the knowledge was perceptually-bound. The domains were colors and names of colors. In Exp 2, using 40 Ss, the relationship between consciousness and transfer was tested. In the test phase for each experiment, Ss were informed that there was a complex set of rules that governed the order of the items in each string, and were told to classify the strings. Exp 2 used letters and Ss were asked for a confidence judgement on a scale from 50–100. Exp 1 demonstrated that knowledge of artificial grammars was actually relatively inflexible in its application, in that there was limited transfer between 2 domains with obvious mappings. Results for Exp 2 indicate that the knowledge had an unconscious component in both the same and different domains. It is concluded that grammar learning is a useful paradigm for exploring the links between consciousness and flexibility. The training and test items are appended.},
author = {Dienes, Zolt{\'{a}}n and Altmann, Gerry T. M.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dienes, Altmann - 1997 - Transfer of implicit knowledge across domains How implicit and how abstract.pdf:pdf},
journal = {How implicit is implicit learning?},
pages = {107--123},
title = {{Transfer of implicit knowledge across domains: How implicit and how abstract?}},
year = {1997}
}
@article{Zhang1997,
abstract = {This article proposes a theoretical framework for external representation based problem solving. The Tic-Tat-Toe and its isomorphs are used to illustrate the procedures of the framework as a methodology and test the predictions of the framework as a functional model. Experimental results show that the behavior in the Tic-Tat-Toe is determined by the directly available information in external and internal representations in terms of perceptual and cognitive biases, regardless of whether the biases are consistent with, inconsistent with, or irrelevant to the task. It is shown that external representations are not merely inputs and stimuli to the internal mind and that they have much more important functions than mere memory aids. A representational determinism is suggested-the form of a representation determines what information can be perceived, what processes can be activated, and what structures can be discovered from the specific representation.},
author = {Zhang, Jiajie},
doi = {10.1207/s15516709cog2102_3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 1997 - The Nature Problem of External in Solving Representations.pdf:pdf},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
number = {2},
pages = {179--217},
pmid = {9712194170},
title = {{The Nature Problem of External in Solving Representations}},
volume = {21},
year = {1997}
}
@article{Gibson1996,
author = {Gibson, Faison P},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gibson - 1996 - The Sugar Production Factory--A Dynamic Decision Task.pdf:pdf},
keywords = {dynamic decision making},
pages = {49--60},
title = {{The Sugar Production Factory--A Dynamic Decision Task}},
volume = {1},
year = {1996}
}
@article{Anderson1996,
abstract = {In the Adaptive Character of Thought (ACT-R) theory, complex cognition arises from an interaction of procedural and declarative knowledge. Procedural knowledge is represented in units called production rules, and declarative knowledge is represented in units called chunks. The individual units are created by simple encodings of objects in the environment (chunks) or simple encodings of transformations in the environment (production rules). A great many such knowledge units underlie human cognition. From this large database, the appropriate units are selected for a particular context by activation processes that are tuned to the statistical structure of the environment. According to the ACT-R theory, the power of human cognition depends on the amount of knowledge encoded and the effective deployment of the encoded knowledge.},
author = {Anderson, John R.},
doi = {10.1037//0003-066X.51.4.355},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson - 1996 - A Simple Theory of Complex Cognition.pdf:pdf},
isbn = {1935-990X},
issn = {0003066X},
journal = {American Psychologist},
number = {4},
pages = {355--365},
pmid = {155},
title = {{A Simple Theory of Complex Cognition}},
volume = {51},
year = {1996}
}
@article{Gross-Tsur1996,
abstract = {One hundred and forty-three 11-year-old children with development dyscalculia, from a cohort of 3029 students, were studied to determine demographic features and prevalence of this primary cognitive disorder. They were evaluated for gender, IQ, linguistic and perceptual skills, symptoms of attention-deficit hyperactivity disorder (ADHD), socio-economic status and associated learned disabilities. The IQs of the 140 children (75 girls and 65 boys) retained in the study group (three were excluded because of low IQs) ranged from 80 to 129 (mean 98.2, SD 9.9). 26 per cent of the children had symptoms of ADHD, and 17 per cent had dyslexia. Their socio-economic status was significantly lower than that of the rest of the cohort, and 42 per cent had first-degree relatives with learning disabilities. The prevalence of dyscalculia in the original cohort was 6.5 per cent, similar to that of dyslexia and ADHD. However, unlike these other learning disabilities, dyscalculia affected the two sexes in about the same proportions.},
author = {Gross-Tsur, V and Manor, O and Shalev, R S},
doi = {10.1007/s007870070009},
isbn = {1469-8749},
issn = {0012-1622},
journal = {Developmental medicine and child neurology},
keywords = {developmental dyscalculia,ð prevalence ð prognosis},
number = {1},
pages = {25--33},
pmid = {8606013},
title = {{Developmental dyscalculia: prevalence and demographic features.}},
volume = {38},
year = {1996}
}
@article{Bartlett1996,
author = {Bartlett, Pl and Long, Pm and Williamson, Rc},
doi = {10.1006/jcss.1996.0033},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bartlett, Long, Williamson - 1996 - Fat-shattering and the learnability of real-valued functions.pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
keywords = {1996,434 452,rnal of computer and,system sciences 52},
number = {0033},
pages = {434--452},
title = {{Fat-shattering and the learnability of real-valued functions}},
volume = {52},
year = {1996}
}
@article{Gross-Tsur1996,
abstract = {One hundred and forty-three 11-year-old children with development dyscalculia, from a cohort of 3029 students, were studied to determine demographic features and prevalence of this primary cognitive disorder. They were evaluated for gender, IQ, linguistic and perceptual skills, symptoms of attention-deficit hyperactivity disorder (ADHD), socio-economic status and associated learned disabilities. The IQs of the 140 children (75 girls and 65 boys) retained in the study group (three were excluded because of low IQs) ranged from 80 to 129 (mean 98.2, SD 9.9). 26 per cent of the children had symptoms of ADHD, and 17 per cent had dyslexia. Their socio-economic status was significantly lower than that of the rest of the cohort, and 42 per cent had first-degree relatives with learning disabilities. The prevalence of dyscalculia in the original cohort was 6.5 per cent, similar to that of dyslexia and ADHD. However, unlike these other learning disabilities, dyscalculia affected the two sexes in about the same proportions.},
author = {Gross-Tsur, V and Manor, O and Shalev, R S},
doi = {10.1007/s007870070009},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gross-Tsur, Manor, Shalev - 1996 - Developmental dyscalculia prevalence and demographic features.pdf:pdf},
isbn = {1469-8749},
issn = {0012-1622},
journal = {Developmental medicine and child neurology},
keywords = {developmental dyscalculia,ð prevalence ð prognosis},
number = {1},
pages = {25--33},
pmid = {8606013},
title = {{Developmental dyscalculia: prevalence and demographic features.}},
volume = {38},
year = {1996}
}
@article{McClelland1995,
abstract = {Damage to the hippocampal system disrupts recent memory but leaves remote memory intact. The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocortical changes. Models that learn via changes to connections help explain this organization. These models discover the structure in ensembles of items if learning of each item is gradual and interleaved with learning about other items. This suggests that the neocortex learns slowly to discover the structure in ensembles of experiences. The hippocampal system permits rapid learning of new items without disrupting this structure, and reinstatement of new memories interleaves them with others to integrate them into structured neocortical memory systems.},
author = {McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
doi = {10.1037/0033-295X.102.3.419},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McClelland, McNaughton, O'Reilly - 1995 - Why there are complementary learning systems in the hippocampus and neocortex Insights from th.pdf:pdf},
isbn = {0033-295X (Print)$\backslash$r0033-295X (Linking)},
issn = {1939-1471},
journal = {Psychological Review},
number = {3},
pages = {419--457},
pmid = {7624455},
title = {{Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.102.3.419},
volume = {102},
year = {1995}
}
@article{Mao1995,
abstract = {A number of networks and learning algorithms which provide$\backslash$nnew or alternative tools for feature extraction and data$\backslash$nprojection is proposed. The networks include a network$\backslash$n(SAMANN) for Sammon's nonlinear projection, a linear$\backslash$ndiscriminant analysis (LDA) network, a nonlinear$\backslash$ndiscriminant analysis (NDA) network, and a network for$\backslash$nnonlinear projection (NP-SOM) based on Kohonen's$\backslash$nself-organizing map. Five representative neural networks$\backslash$nfor feature extraction and data projection based on a$\backslash$nvisual judgement of two-dimensional projection maps and$\backslash$nquantitative criteria on data sets with various properties$\backslash$nare evaluated.},
annote = {NULL},
author = {Mao, Jianchang and Jain, Anil K.},
doi = {10.1109/72.363467},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao, Jain - 1995 - Artificial neural networks for feature extraction and multivariate data projection.pdf:pdf},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {296--317},
pmid = {18263314},
title = {{Artificial neural networks for feature extraction and multivariate data projection}},
volume = {6},
year = {1995}
}
@article{Schiefele1995,
author = {Schiefele, Ulrich and Csikszentmihalyi, Mihaly},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schiefele, Csikszentmihalyi - 1995 - Motivation and Ability as Factors in Mathematics Experience and Achievement.pdf:pdf},
journal = {Journal for Research in Matematics Education},
number = {2},
pages = {163--181},
title = {{Motivation and Ability as Factors in Mathematics Experience and Achievement}},
volume = {26},
year = {1995}
}
@article{Kass1995,
author = {Kass, Robert E. and Raftery, Adrian E.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kass, Raftery - 1995 - Bayes Factors.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {430},
pages = {773--795},
title = {{Bayes Factors}},
volume = {90},
year = {1995}
}
@article{Krogh1995,
abstract = {Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme. 1 INTRODUCTION It is well known that a combination of many different predictors can improve predictions. In the neural networks community "ensembles" of neural networks h...},
author = {Krogh, Anders and Vedelsby, Jesper},
doi = {10.1.1.37.8876},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krogh, Vedelsby - 1995 - Neural Network Ensembles, Cross Validation, and Active Learning.pdf:pdf},
isbn = {0262201046},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 7},
pages = {231--238},
title = {{Neural Network Ensembles, Cross Validation, and Active Learning}},
year = {1995}
}
@article{Tesauro1995,
author = {Tesauro, G},
doi = {http://doi.acm.org/10.1145/203330.203343},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tesauro - 1995 - Temporal difference learning and TD-Gammon.pdf:pdf},
issn = {0001-0782},
journal = {Commun. ACM},
number = {3},
pages = {58--68},
title = {{Temporal difference learning and TD-Gammon}},
volume = {38},
year = {1995}
}
@article{Leron1995,
abstract = {The authors discuss an alternative method for teaching abstract algebra, involving computer investigations and small group as well as large group discussions. The article is structured as a discussion between a traditional instructor and the authors and includes several examples of classroom interactions as well as worksheets used in computer investigations.},
author = {Leron, Uri and Dubinsky, Ed},
doi = {10.2307/2975010},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leron, Dubinsky - 1995 - An Abstract Algebra Story.pdf:pdf},
issn = {0002-9890},
journal = {American Mathematical Monthly},
keywords = {Abstract Algebra,Construct Paper,MTHED 527,Undergraduate education},
number = {3},
pages = {227--242},
title = {{An Abstract Algebra Story}},
volume = {102},
year = {1995}
}
@article{Happel1994,
author = {Happel, B and Murre, J M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Happel, Murre - 1994 - Design and Evolution of Modular Neural Network Architectures.html:html},
journal = {Neural Networks},
number = {6/7},
pages = {985--1004},
title = {{Design and Evolution of Modular Neural Network Architectures}},
volume = {7},
year = {1994}
}
@article{DiSessa1993,
annote = {p. 134-135 "[This suggests that] expert intuitions might develop for reasons of increasing coherence of the knowledge system (Systematicity D, mutual plausibility) even though such intuitions may not be instructed or even have any direct instrumental role in problem solving."

p. 189 - "in mathematics, definitions are supposed categorically to determine the set of examples of a class. But if students possess structurally limited knowledge systems, judgments will frequently be made on the basis of different and inarticulate knowledge, even if definitions are overtly end},
author = {DiSessa, Andrea A.},
file = {:home/andrew/Documents/grad/Papers/3233725.pdf:pdf},
journal = {Cognition and Instruction},
number = {2/3},
pages = {105--225},
title = {{Toward an Epistemology of Physics}},
url = {http://www.jstor.org/stable/3233725},
volume = {10},
year = {1993}
}
@article{Marcus1993,
abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
author = {Marcus, Mitchell P and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
doi = {10.1162/coli.2010.36.1.36100},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcus, Santorini, Marcinkiewicz - 1993 - Building a large annotated corpus of English The Penn Treebank.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
keywords = {POS-Tagging},
number = {2},
pages = {313--330},
title = {{Building a large annotated corpus of English: The Penn Treebank}},
volume = {19},
year = {1993}
}
@article{Rumelhart1993,
abstract = {(From the chapter) a bewildering variety of connectionist applications has cropped up throughout the cognitive sciences and engineering one of the central issues in all of these models is the representation of knowledge in the connectionist network getting a coherent picture of "what goes on" inside a network as it develops, manipulates, and alters the representation of the knowledge it processes is vital for our understanding of connectionist information processing, and likely for our understanding of the minds these systems model explore the sorts of representations that connectionist systems employ and the crucial role learning plays in constructing them distributed versus localist representations learning representations in connectionist networks autoencoders representing semantic networks in connectionist systems connectionist representations and human judgments of similarity (PsycINFO Database Record (c) 2003 APA},
author = {Rumelhart, David E and Todd, Peter M},
isbn = {0262132842},
issn = {0-262-13284-2},
journal = {Attention and performance XIV: Synergies in experimental psychology, artificial intelligence, and cognitive neuroscience},
pages = {3--30},
title = {{Learning and connectionist representations}},
year = {1993}
}
@article{Ericsson1993,
abstract = {The theoretical framework presented in this article explains expert performance as the end result of individuals' prolonged efforts to improve performance while negotiating motivational and external constraints. In most domains of expertise, individuals begin in their childhood a regimen of effortful activities (deliberate practice) designed to optimize improvement. Individual differences, even among elite performers, are closely related to assessed amounts of deliberate practice. Many characteristics once believed to reflect innate talent are actually the result of intense practice extended for a minimum of 10 yrs. Analysis of expert performance provides unique evidence on the potential and limits of extreme environmental adaptation and learning.},
archivePrefix = {arXiv},
arxivId = {http://doi.apa.org/psycinfo/1993-40718-001},
author = {Ericsson, K. Anders and Krampe, Ralf T. and Tesch-R{\"{o}}mer, Clemens},
doi = {10.1037/0033-295X.100.3.363},
eprint = {/doi.apa.org/psycinfo/1993-40718-001},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ericsson, Krampe, Tesch-R{\"{o}}mer - 1993 - The role of deliberate practice in the acquisition of expert performance.pdf:pdf},
isbn = {1939-1471(Electronic);0033-295X(Print)},
issn = {1939-1471},
journal = {Psychological Review},
number = {3},
pages = {363--406},
pmid = {2140},
primaryClass = {http:},
title = {{The role of deliberate practice in the acquisition of expert performance.}},
volume = {100},
year = {1993}
}
@incollection{Detterman1993,
annote = {"First, significant transfer is probably rare and accounts for very little human behavior. Studies that claim transfer often tell subjects to transfer, or use a `trick' to call the subjects attention to the similarity of the two problems. Such studies cannot be taken as evidence for transfer. We generally do what we have learned to do and no more. The lesson learned from studies of transfer is that, if you want people to learn something, teach it to them. Don't teach them something else and expect them to figure out what you really want them to do."

On studies showing successful transfer "The experimeter's manipulations have all the subtlety of the famer's baseball bat."},
author = {Detterman, Douglas K.},
booktitle = {Transfer on Trial: Intelligence, Cognition, and Instruction},
pages = {1--24},
title = {{The Case for the Prosecution: Transfer as an Epiphenomenon}},
year = {1993}
}
@article{Elman1993,
abstract = {It is a striking fact that in humans the greatest learnmg occurs precisely at that point in time - childhood - when the most dramatic maturational changes also occur. This report describes possible synergistic interactions between maturational change and the ability to learn a complex domain (language), as investigated in connectionist networks. The networks are trained to process complex sentences involving relative clauses, number agreement, and several types of verb argument structure. Training fails in the case of networks which are fully formed and ‘adultlike' in their capacity. Training succeeds only when networks begin with limited working memory and gradually ‘mature' to the adult state. This result suggests that rather than being a limitation, developmental restrictions on resources may constitute a necessary prerequisite for mastering certain complex domains. Specifically, successful learning may depend on starting small.},
author = {Altmann, G. T M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Altmann - 1993 - Learning and development in neural networks the importance of starting small.pdf:pdf},
isbn = {0010-0277},
journal = {Cognition},
keywords = {Neural networks},
number = {2},
pages = {71--99},
title = {{Learning and development in neural networks: the importance of starting small}},
volume = {48},
year = {1993}
}
@article{Movellan1993,
abstract = {In this article we present symmetric diffusion networks, a family of networks that instantiate the principles of continuous, stochastic, adaptive and interactive propagation of information.  Using methods from Markovian diffusion theory, we formalize the activation dynamics of these networks and then show that they can be trained to reproduce entire multivariate probability distributions on their outputs using the contrastive Hebbian learning rule (CHL).  We show that CHL performs gradient descent on an error function that captures differences between desired and obtained continuous multivariate probability distributions.  This allows the learning algorithm to go beyond expected values of output units and to approximate complete probability distributions on continuous multivariate activation spaces.  We argue that learning continuous distributions is an important task underlying a variety of real life situations which were beyond the scope of previous connectionist networks.  Deterministic networks, like back propagation, cannot learn the task because they are limited to learning average values of independent output units.  Previous stochastic connectionist networks could learn probability distribtions but they were limited to discrete variables.  Simulations show that symmetric diffustion networks can be trained with the CHL rule to approximate discrete and continuous probability distributions of various types.},
author = {Movellan, Javier R. and McClelland, James L.},
doi = {10.1207/s15516709cog1704_1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Movellan, McClelland - 1993 - Learning Continuous Probability Distributions with Symmetric Diffusion Networks.pdf:pdf},
isbn = {0364-0213},
issn = {03640213},
journal = {Cognitive Science},
number = {4},
pages = {463--496},
title = {{Learning Continuous Probability Distributions with Symmetric Diffusion Networks}},
volume = {17},
year = {1993}
}
@article{Sutton1992,
abstract = {We introduce the first temporal-difference learning algorithm that is stable with linear function approximation and off-policy training, for any finite Markov de-cision process, behavior policy, and target policy, and whose complexity scales linearly in the number of parameters. We consider an i.i.d. policy-evaluation set-ting in which the data need not come from on-policy experience. The gradient temporal-difference (GTD) algorithm estimates the expected update vector of the TD(0) algorithm and performs stochastic gradient descent on its L 2 norm. We prove that this algorithm is stable and convergent under the usual stochastic ap-proximation conditions to the same least-squares solution as found by the LSTD, but without LSTD's quadratic computational complexity. GTD is online and in-cremental, and does not involve multiplying by products of likelihood ratios as in importance-sampling methods.},
author = {Sutton, Richard S and Szepesv, Csaba},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Szepesv - 1992 - A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation.pdf:pdf},
journal = {Computing},
pages = {1--8},
title = {{A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation}},
year = {1992}
}
@incollection{Bisanz1992,
author = {Bisanz, Jeffrey and LeFevre, Jo-Anne},
booktitle = {The Nature and Origins of Mathematical Skills},
pages = {113--136},
title = {{Understanding Elementary Mathematics}},
year = {1992}
}
@misc{Kruschke1992,
abstract = {ALCOVE (attention learning covering map) is a connectionist model of category learning that incorporates an exemplar-based representation (Medin {\&} Schaffer, 1978; Nosofsky, 1986) with error-driven learning (Gluck {\&} Bower, 1988; Rumelhart, Hinton, {\&} Williams, 1986). Alcove selectively attends to relevant stimulus dimensions, is sensitive to correlated dimensions, can account for a form of base-rate neglect, does not suffer catastrophic forgetting, and can exhibit 3-stage (U-shaped) learning of high-frequency exceptions to rules, whereas such effects are not easily accounted for by models using other combinations of representation and learning method.},
author = {Kruschke, J K},
booktitle = {Psychological review},
doi = {10.1037/0033-295X.99.1.22},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kruschke - 1992 - ALCOVE an exemplar-based connectionist model of category learning.pdf:pdf},
isbn = {0033-295X$\backslash$n1939-1471},
issn = {0033-295X},
keywords = {Attention,Discrimination Learning,Generalization (Psychology),Humans,Mental Recall,Models,Neural Networks (Computer),Pattern Recognition,Psychological,Reinforcement (Psychology),Visual},
number = {1},
pages = {22--44},
pmid = {1546117},
title = {{ALCOVE: an exemplar-based connectionist model of category learning.}},
volume = {99},
year = {1992}
}
@article{Berry1991,
author = {Berry, Diane C.},
doi = {10.1080/14640749108400961},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berry - 1991 - The role of action in implicit learning.pdf:pdf},
journal = {The Quarterly Journal of Experimental Psychology},
number = {4},
pages = {881--906},
title = {{The role of action in implicit learning}},
volume = {43},
year = {1991}
}
@article{Frith1991,
author = {Frith, Uta and Morton, John and Leslie, Alan M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frith, Morton, Leslie - 1991 - The cognitive basis of a biological disorder autism.pdf:pdf},
journal = {Trends in Neurosciences},
number = {10},
pages = {433--438},
title = {{The cognitive basis of a biological disorder: autism}},
volume = {14},
year = {1991}
}
@incollection{Wilensky1991,
address = {Westport, CT, US},
author = {Wilensky, Uri},
booktitle = {Constructionism},
editor = {Harel, Idit and Papert, Seymour},
publisher = {Ablex Publishing},
title = {{Abstract Meditations on the Concrete and Concrete Implications for Mathematics Education}},
year = {1991}
}
@article{Krogh1991,
author = {Krogh, Anders},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krogh - 1991 - A Simple Weight Decay Can Improve Generalization.pdf:pdf},
journal = {Advances in Neural Information Processing Systems(Proceedings of NIPS)},
pages = {950--957},
title = {{A Simple Weight Decay Can Improve Generalization}},
volume = {4},
year = {1991}
}
@article{Kramer1991,
annote = {NULL},
author = {Kramer, M},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kramer - 1991 - Nonlinear principal component analysis using autoassociative neural networks.pdf:pdf},
journal = {AIChE journal},
keywords = {{\&}abstraction {\&}accounted {\&}application {\&}architecture},
number = {2},
pages = {233--243},
title = {{Nonlinear principal component analysis using autoassociative neural networks}},
volume = {37},
year = {1991}
}
@article{Jacobs1991,
abstract = {We describe a multi-network, or modular, connectionist architecture that captures that fact that many tasks have structure at a level of granularity intermediate to that assumed by local and global function approximation schemes. The main innovation of the architecture is that it combines associative and competitive learning in order to learn task decompositions. A task decomposition is discovered by forcing the networks comprising the architecture to compete to learn the training patterns. As a result of the competition, different networks learn different training patterns and, thus, learn to partition the input space. The performance of the architecture on a "what" and "where" vision task and on a multi-payload robotics task are presented.},
author = {Jacobs, Ra and Jordan, Mi},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobs, Jordan - 1991 - A competitive modular connectionist architecture.pdf:pdf},
isbn = {1-55860-184-8},
journal = {Advances in neural information processing systems},
pages = {1--7},
title = {{A competitive modular connectionist architecture}},
year = {1991}
}
@article{Gottlieb1991,
author = {Gottlieb, Gilbert and Horton, Margaret and Rodriguiz, Ramona and Iii, Wayne Kelly},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gottlieb et al. - 1991 - Experiential Canalization of Behavioral Development Results.pdf:pdf},
number = {1},
pages = {35--39},
title = {{Experiential Canalization of Behavioral Development : Results}},
volume = {27},
year = {1991}
}
@article{Dubinsky1991,
abstract = {Dubinsky describes a generalization of Piaget's notion of reflective$\backslash$nabstraction to the context of advanced mathematical thinking. The$\backslash$nresult is a description of learning in terms of encapsulating processes$\backslash$nand interiorizing actions. These abstractions are organized in a$\backslash$nschema for a particular concept, and Dubinsky offers "genetic decomposition"$\backslash$nas a manner for describing schema. These decompositions may be useful$\backslash$nfor planning and reflecting on instructional sequences. See also$\backslash$nSfard and Linchevski (1994) and Asial et. al. (1996).},
author = {Dubinsky, Ed},
doi = {10.1007/0-306-47203-1_7},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dubinsky - 1991 - Reflective abstraction in advanced mathematical thinking.pdf:pdf},
isbn = {0792314565},
issn = {1098-6065},
journal = {Advanced mathematical thinking},
pages = {95--121},
title = {{Reflective abstraction in advanced mathematical thinking}},
year = {1991}
}
@incollection{Stewart1990,
abstract = {abstract The singular value decomposition has a number of applications in dig-ital signal processing. However, the the decomposition must be com-puted from a matrix consisting of both signal and noise. It is therefore important to be able to assess the eeects of the noise on the singular values and singular vectors | a problem in classical perturbation the-ory. In this paper we survey the perturbation theory of the singular value decomposition. Abstract The singular value decomposition has a number of applications in digital signal processing. However, the the decomposition must be computed from a matrix consisting of both signal and noise. It is therefore important to be able to assess the eeects of the noise on the singular values and singular vectors | a problem in classical perturbation theory. In this paper we survey the perturbation theory of the singular value decomposition.},
author = {Stewart, G W and Stewart, G W},
booktitle = {SVD AND SIGNAL PROCESSING, II: ALGORITHMS, ANALYSIS AND APPLICATIONS},
file = {:home/andrew/Documents/grad/Papers/CS-TR-2539.pdf:pdf},
number = {September},
title = {{Perturbation Theory for the Singular Value Decomposition}},
url = {https://pdfs.semanticscholar.org/0a2b/0c431bd68c6a67926f44a07a0d575f2957f1.pdf{\%}0Ahttp://users.math.msu.edu/users/markiwen/Teaching/MTH995/Papers/SVD{\_}Stewart.pdf},
year = {1990}
}
@article{Becker1990,
author = {Becker, Betsy J},
journal = {American Educational Research Journal},
pages = {65--87},
title = {{Item characteristics and gender differences on the SAT-M for mathematically able youths}},
volume = {27},
year = {1990}
}
@article{Koedinger1990,
abstract = {We present a new model of skilled performance in geometry proof problem solving called the Diagram Configuration model (DC). While previous models plan proofs in a step-by-step fashion, we observed that experts plan at a more abstract level: They focus on the key steps and skip the less important ones. DC models this abstract planning behavior by parsing geometry problem diagrams into perceptual chunks, called diagram configurations, which cue relevant schematic knowledge. We provide verbal protocol evidence that DC's schemas correspond with the step-skipping inferences experts make in their initial planning. We compare DC with other models of geometry expertise and then, in the final section, we discuss more general implications of our research. DC's reasoning has important similarities with Larkin's (1988) display-based reasoning approach and Johnson-Laird's (1983) mental model approach. DC's perceptually based schemas are a step towards a unified explanation of (1) experts' superior problem-solving effectiveness, (2) experts' superior problem-state memory, and (3) experts' ability, in certain domains, to solve relatively simple problems by pure forward inferencing. We also argue that the particular and efficient knowledge organization of DC challenges current theories of skill acquisition as it presents an end-state of learning that is difficult to explain within such theories. Finally, we discuss the implications of DC for geometry instruction. {\textcopyright} 1990.},
author = {Koedinger, Kenneth R. and Anderson, John R.},
doi = {10.1207/s15516709cog1404_2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koedinger, Anderson - 1990 - Abstract Planning and Perceptual Chunks Elements of Expertise in Geometry.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
pages = {511--550},
title = {{Abstract Planning and Perceptual Chunks: Elements of Expertise in Geometry}},
volume = {14},
year = {1990}
}
@article{Baldi1989,
abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed. {\textcopyright} 1989.},
author = {Baldi, Pierre and Hornik, Kurt},
doi = {10.1016/0893-6080(89)90014-2},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldi, Hornik - 1989 - Neural networks and principal component analysis Learning from examples without local minima.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
number = {1},
pages = {53--58},
title = {{Neural networks and principal component analysis: Learning from examples without local minima}},
volume = {2},
year = {1989}
}
@article{Holyoak1989,
author = {Holyoak, Keith J},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holyoak - 1989 - Analogical Mapping by Constraint Satisfaction.pdf:pdf},
title = {{Analogical Mapping by Constraint Satisfaction}},
volume = {5},
year = {1989}
}
@article{Falkenhainer1989,
author = {Falkenhainer, Brian and Forbus, Kenneth D and Gentner, Dedre},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Falkenhainer, Forbus, Gentner - 1989 - The Structure-Mapping Engine Algorithm and Examples.pdf:pdf},
journal = {Artificial Intelligence},
number = {1},
pages = {1--63},
title = {{The Structure-Mapping Engine : Algorithm and Examples}},
volume = {41},
year = {1989}
}
@article{Cosmides1989,
abstract = {In order to successfully engage in social exchange-cooperation between two or more individuals for mutual benefit-humans must be able to solve a number of complex computational problems, and do so with special efficiency. Following Marr (1982), Cosmides (1985) and Cosmides and Tooby (1989) used evolutionary principles to develop a computational theory of these adaptive problems. Specific hypotheses concerning the structure of the algorithms that govern how humans reason about social exchange were derived from this computational theory. This article presents a series of experiments designed to test these hypotheses, using the Wason selection task, a test of logical reasoning. Part I reports experiments testing social exchange theory against the availability theories of reasoning; Part II reports experiments testing it against Cheng and Holyoak's (1985) permission schema theory. The experimental design included eight critical tests designed to choose between social exchange theory and these other two families of theories; the results of all eight tests support social exchange theory. The hypothesis that the human mind includes cognitive processes specialized for reasoning about social exchange predicts the content effects. ?? 1989.},
author = {Cosmides, Leda},
doi = {10.1016/0010-0277(89)90023-1},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
number = {3},
pages = {187--276},
pmid = {2743748},
title = {{The logic of social exchange: Has natural selection shaped how humans reason? Studies with the Wason selection task}},
volume = {31},
year = {1989}
}
@article{Cosmides1989,
abstract = {In order to successfully engage in social exchange-cooperation between two or more individuals for mutual benefit-humans must be able to solve a number of complex computational problems, and do so with special efficiency. Following Marr (1982), Cosmides (1985) and Cosmides and Tooby (1989) used evolutionary principles to develop a computational theory of these adaptive problems. Specific hypotheses concerning the structure of the algorithms that govern how humans reason about social exchange were derived from this computational theory. This article presents a series of experiments designed to test these hypotheses, using the Wason selection task, a test of logical reasoning. Part I reports experiments testing social exchange theory against the availability theories of reasoning; Part II reports experiments testing it against Cheng and Holyoak's (1985) permission schema theory. The experimental design included eight critical tests designed to choose between social exchange theory and these other two families of theories; the results of all eight tests support social exchange theory. The hypothesis that the human mind includes cognitive processes specialized for reasoning about social exchange predicts the content effects. ?? 1989.},
author = {Cosmides, Leda},
doi = {10.1016/0010-0277(89)90023-1},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cosmides - 1989 - The logic of social exchange Has natural selection shaped how humans reason Studies with the Wason selection task.pdf:pdf},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
number = {3},
pages = {187--276},
pmid = {2743748},
title = {{The logic of social exchange: Has natural selection shaped how humans reason? Studies with the Wason selection task}},
volume = {31},
year = {1989}
}
@article{Fodor1988,
abstract = {This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a 'language of thought': i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the 'systematicity' of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or 'abstract neurological') structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation. ?? 1988.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fodor, Jerry A. and Pylyshyn, Zenon W.},
doi = {10.1016/0010-0277(88)90031-5},
eprint = {arXiv:1011.1669v3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fodor, Pylyshyn - 1988 - Connectionism and cognitive architecture A critical analysis.pdf:pdf},
isbn = {9788578110796},
issn = {00100277},
journal = {Cognition},
number = {1-2},
pages = {3--71},
pmid = {25246403},
title = {{Connectionism and cognitive architecture: A critical analysis}},
volume = {28},
year = {1988}
}
@article{Novick1988,
author = {Novick, Laura R},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Novick - 1988 - Analogical Transfer , Problem Similarity , and Expertise.pdf:pdf},
number = {3},
pages = {510--520},
title = {{Analogical Transfer , Problem Similarity , and Expertise}},
volume = {14},
year = {1988}
}
@incollection{Greeno1987,
author = {Greeno, J. and Riley, M.},
booktitle = {Metacognition, motivation, and understanding},
pages = {289--313},
title = {{Processes and development of understanding}},
year = {1987}
}
@article{Golub1987,
abstract = {The Eckart-Young-Mirsky theorem solves the problem of approximating a matrix by one of lower rank. However, the approximation generally differs from the original in all its elements. In this paper it is shown how to obtain a best approximation of lower rank in which a specified set of columns of the matrix remains fixed. The paper concludes with some applications of the generalization. {\textcopyright} 1987.},
author = {Golub, G. H. and Hoffman, Alan and Stewart, G. W.},
doi = {10.1016/0024-3795(87)90114-5},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Golub, Hoffman, Stewart - 1987 - A generalization of the Eckart-Young-Mirsky matrix approximation theorem.pdf:pdf},
isbn = {0024-3795},
issn = {00243795},
journal = {Linear Algebra and Its Applications},
number = {C},
pages = {317--327},
title = {{A generalization of the Eckart-Young-Mirsky matrix approximation theorem}},
volume = {88-89},
year = {1987}
}
@article{West1987,
abstract = {All organisms inherit parents' genes, but many also inherit parents, peers, and the places they inhabit as well. We suggest the term ontogenetic niche to signify the ecological and social legacies that accompany genes. A formal name is needed to give the idea of the inherited environment equal status with its conceptual cognates; nature and nurture. We argue here that increased recognition of the inherited environment facilitates unification efforts within the developmental sciences by emphasizing the affinity, rather than opposability, of ontogenetic processes.},
author = {West, M J and King, a P},
doi = {10.1002/dev.420200508},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/West, King - 1987 - Settling nature and nurture into an ontogenetic niche.pdf:pdf},
isbn = {0012-1630 (Print)$\backslash$n0012-1630 (Linking)},
issn = {0012-1630},
journal = {Developmental psychobiology},
number = {April},
pages = {549--562},
pmid = {3678619},
title = {{Settling nature and nurture into an ontogenetic niche.}},
volume = {20},
year = {1987}
}
@article{Gong1986,
author = {Gong, Gail},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gong - 1986 - Cross-Validation, the Jackknife, and the Bootstrap Excess Error Estimation in Forward Logistic Regression.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {383},
pages = {108--113},
title = {{Cross-Validation, the Jackknife, and the Bootstrap: Excess Error Estimation in Forward Logistic Regression}},
volume = {81},
year = {1986}
}
@article{Campbell1986,
author = {Campbell, Nancy K and Hackett, Gail},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell, Hackett - 1986 - The Effects of Mathematics Task Performance on Math Self-Efficacy and Task Interest.pdf:pdf},
journal = {Journal of Vocational Behavior},
pages = {149--162},
title = {{The Effects of Mathematics Task Performance on Math Self-Efficacy and Task Interest}},
volume = {162},
year = {1986}
}
@misc{Hinton1986,
abstract = {Concepts can be represented by distributed patterns of activity in networks of neuron-like units. One advantage of this kind of representation is that it leads to automatic generalization. When the weighjts in the network are changed to incorporate new knowledgwe about one concept, the changes affect the knowledge associated with other concepts that are represented by similar activity patterns. There have been numerous demonstrations of sensible generalization which have depended on the experimenter choosing appropriately similar patterns for diferent concepts. This paper shows how the network can be made to choose the patterns itself when shown a set of propositions that use the concepts. It chooses patterns which make explicit the underlying features that are only implicit in the propositions it is shown.},
author = {Hinton, Geoffrey},
booktitle = {Proceedings of the Eighth Annual Conference of the Cognitive Science Society},
doi = {10.1109/69.917563},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton - 1986 - Learning distributed representations of concepts.pdf:pdf},
isbn = {0-262-68053-X},
issn = {10414347},
pages = {1--12},
pmid = {21943171},
title = {{Learning distributed representations of concepts}},
year = {1986}
}
@article{Burger1986,
abstract = {This study provides a description of the van Hiele levels of reasoning in geometry according to responses to clinical interview tasks concerning triangles and quadrilaterals. The subjects were 13 students from Grades 1 through 12 plus a university mathematics major. The tasks included drawing shapes, identifying and defining shapes, sorting shapes, determining a mystery shape, establishing properties of parallelograms, and comparing components of a mathematical system. The students' behavior on the tasks was consistent with the van Hieles' original general description of the levels, although the discreteness of levels, particularly of analysis and abstraction, was not confirmed. The use of formal deduction among students who were taking or had taken secondary school geometry was nearly absent, consistent with earlier observations by Usiskin (1982).},
author = {Burger, Wf and Shaughnessy, Jm},
doi = {10.2307/749317},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burger, Shaughnessy - 1986 - Characterizing the van Hiele Levels of Development in Geometry.pdf:pdf},
issn = {0021-8251},
journal = {Journal for research in mathematics {\ldots}},
number = {1},
pages = {31--48},
title = {{Characterizing the van Hiele Levels of Development in Geometry}},
volume = {17},
year = {1986}
}
@article{W.Burger1986,
author = {{W. Burger}, J. Michael Shaughnessy},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/W. Burger - 1986 - Characterizing the van Hiele Levels of Development in Geometry.pdf:pdf},
isbn = {0898597595},
journal = {Journal for Research in Matematics Education},
number = {1},
pages = {31--48},
title = {{Characterizing the van Hiele Levels of Development in Geometry}},
volume = {17},
year = {1986}
}
@article{Waltz1985,
abstract = {This is a description of research in developing a natural language processing system with modular knowledge sources but strongly interactive processing. The system offers insights into a variety of linguistic phenomena and allows easy testing of a variety of hypotheses. Language interpretation takes place on a activation network which is dynamically created from input, recent context, and long-term knowledge. Initially ambiguous and unstable, the network settles on a single interpretation, using a parallel, analog relaxation process. We also describe a parallel model for the representation of context and of the priming of concepts. Examples illustrating contextual influence on meaning interpretation and "semantic garden path" sentence processing, among other issues, are included. ?? 1985.},
author = {Waltz, David L. and Pollack, Jordan B.},
doi = {10.1016/S0364-0213(85)80009-4},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Waltz, Pollack - 1985 - Massively parallel parsing A strongly interactive model of natural language interpretation.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
number = {1},
pages = {51--74},
title = {{Massively parallel parsing: A strongly interactive model of natural language interpretation}},
volume = {9},
year = {1985}
}
@article{Kotovsky1985,
abstract = {This paper analyzes the causes for large differences in difficulty of various isomorphic versions of the Tower of Hanoi problem. Some forms of the problem take 16 times as long to solve, on average, as other versions. Since isomorphism rules out size of task domain as a determinant of relative difficulty, these experiments seek and find causes for the differences in features of the problem representation. Analysis of verbal protocols and the temporal patterning of moves allows the problem-solving behavior to be divided into exploratory and final-path phases. Entry into the final-path phase depends on acquisition of the ability to plan pairs of moves, an achievement made difficult by the working memory load it entails. This memory load can be reduced by automating the rules governing moves, either through problem exploration or training. Once automation has occurred, the solution is obtained very rapidly. Memory load is also proposed as the locus of other differences in difficulty found between various problem representations. ?? 1985.},
author = {Kotovsky, K. and Hayes, J. R. and Simon, H. A.},
doi = {10.1016/0010-0285(85)90009-X},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kotovsky, Hayes, Simon - 1985 - Why are some problems hard Evidence from Tower of Hanoi.pdf:pdf},
isbn = {0010-0285},
issn = {00100285},
journal = {Cognitive Psychology},
number = {2},
pages = {248--294},
title = {{Why are some problems hard? Evidence from Tower of Hanoi}},
volume = {17},
year = {1985}
}
@techreport{Fuys1984,
author = {Fuys, David},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fuys - 1984 - English Translation of Selected Writings of Dina van Hiele-Geldof and Pierre M. van Hiele.pdf:pdf},
title = {{English Translation of Selected Writings of Dina van Hiele-Geldof and Pierre M. van Hiele.}},
year = {1984}
}
@misc{Jackson1984,
author = {Jackson, Douglas N.},
title = {{Personality Research Manual}},
year = {1984}
}
@article{Knudsen1984,
abstract = {We studied the ability of barn owls to recover accurate sound localization after being raised with one ear occluded. Most of the owls had ear plugs inserted before they reached adult size, and therefore they never experienced normal adult localization cues until their ear plugs were removed. Upon removal of their ear plugs, these owls exhibited large systematic sound localization errors. The rate at which they recovered accurate localization decreased with the age of the bird at the time of plug removal, and recovery essentially ceased when owls reached 38 to 42 weeks of age. We interpret this age as the end of a critical period for the consolidation of associations between auditory cues and locations in space. Owls that had experienced adult localization cues for a short period of time before ear plugging recovered normal accuracy rapidly, even if they remained plugged well past the end of the critical period. This suggests that a brief exposure to normal adult cues early in the critical period is sufficient to enable the recovery of localization accuracy much later in life.},
author = {Knudsen, F and Knudsen, P.F and Esterly, S.D.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Knudsen, Knudsen, Esterly - 1984 - A critical period for the recovery of sound localization accuracy following monaural occlusion in the.pdf:pdf},
isbn = {0270-6474 (Print)},
issn = {0270-6474; 0270-6474},
journal = {Journal of Neuroscience},
number = {4},
pages = {1012--1020},
pmid = {6716128},
title = {{A critical period for the recovery of sound localization accuracy following monaural occlusion in the barn owl}},
volume = {4},
year = {1984}
}
@article{Mayberry1983,
author = {Mayberry, Joanne},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayberry - 1983 - The Van Hiele Levels of Geometric Thought in Undergraduate Preservice Teachers.pdf:pdf},
journal = {Journal for Research in Matematics Education},
number = {1},
pages = {58--69},
title = {{The Van Hiele Levels of Geometric Thought in Undergraduate Preservice Teachers}},
volume = {14},
year = {1983}
}
@article{Gick1983,
abstract = {An analysis of the process of analogical thinking predicts that analogies will be noticed on the basis of semantic retrieval cues and that the induction of a general schema from concrete analogs will facilitate analogical transfer. These predictions were tested in experiments in which subjects first read one or more stories illustrating problems and their solutions and then attempted to solve a disparate but analogous transfer problem. The studies in Part I attempted to foster the abstraction of a problem schema from a single story analog by means of summarization instructions, a verbal statement of the underlying principle, or a diagrammatic representation of it. None of these devices achieved a notable degree of sucess. In contrast, the experiments in Part II demonstrated that if two prior analogs were given, subjects often derived a problem schema as an incidental product of describing the similarities of the analogs. The quality of the induced schema was highly predictive of subsequent transfer performance. Furthermore, the verbal statements and diagrams that had failed to facilitate transfer from one analog proved highly beneficial when paired with two. The function of examples in learning was discussed in light of the present study. {\textcopyright} 1983.},
author = {Gick, Mary L. and Holyoak, Keith J.},
doi = {10.1016/0010-0285(83)90002-6},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gick, Holyoak - 1983 - Schema induction and analogical transfer.pdf:pdf},
isbn = {0010-0285},
issn = {00100285},
journal = {Cognitive Psychology},
number = {1},
pages = {1--38},
title = {{Schema induction and analogical transfer}},
volume = {15},
year = {1983}
}
@article{Sweller1983,
abstract = {Previous studies have found that the strategies used by expert and novice problem solvers differ. Novices tend to use means-ends analysis, which involves working backward from the goal, whereas experts prefer to work forward from the givens of a problem. Experiment 1 was designed to study the course of development of expertise using a subset of kinematics problems. After solving many problems, subjects demonstrated the switch from a means-ends to a forward-chaining strategy. This was associated with the conventional concomitants of expertise such as a decrease in the number of moves required for solution. In addition, the speed at which expertise developed varied for different categories of problems. Subjects appeared to categorize problems according to the order in which equations would be required, with these categories being discovered at nonuniform rates. This was assumed to be due to the differential rate of acquisition of schemas associated with different categories of problems. Experiments 2 and 3, again using kinematics problems, tested the hypothesis that the means-ends strategies used by novices retarded the acquisition of appropriate'schemas. It was suggested that under a means-ends strategy, moves are controlled by the problem goal, which reduces the information obtained by problem solvers concerning problem structure. The use of nonspecific rather than specific goals was found to enhance the acquisition of expertise as measured by the use of a forward-oriented strategy, the number of moves required for solution, and the number of equations written without substitutions. Experiments 4 and 5, using geometry problems, duplicated the enhanced rate of strategy alteration found with reduced goal specificity. The results of Experiments 6 and 7, again using geometry problems, indicated that reduced goal specificity also enhanced the rate at which problem solvers induced appropriate problem categories. It was concluded that in circumstances in which the primary reason for presenting problems is to assist problem solvers in acquiring knowledge concerning problem structure, the use of conventional problems solved by means-ends analysis may not be maximally efficient.},
author = {Sweller, John and Mawer, Robert F and Ward, Mark R},
doi = {10.1037/0096-3445.112.4.639},
issn = {0096-3445},
journal = {Journal of Experimental Psychology: General},
number = {4},
pages = {639--661},
pmid = {1120},
title = {{Development of expertise in mathematical problem solving.}},
volume = {112},
year = {1983}
}
@article{Kochen1983,
annote = {1) People suck at uncued analogical transfer
2) Multiple examples help, and if transfer is to be far diverse examples are important.
3) Adding a statement about a general principle had little effect after one example, but after two it worked much better.},
author = {Kochen, Manfred and Krantz, David and Hunt, Earl and Carroll, Tim and Frankovich, Teresa and Arbor, Ann},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kochen et al. - 1983 - Schema Induction and Analogical.pdf:pdf},
title = {{Schema Induction and Analogical}},
volume = {38},
year = {1983}
}
@article{Sweller1983,
abstract = {Previous studies have found that the strategies used by expert and novice problem solvers differ. Novices tend to use means-ends analysis, which involves working backward from the goal, whereas experts prefer to work forward from the givens of a problem. Experiment 1 was designed to study the course of development of expertise using a subset of kinematics problems. After solving many problems, subjects demonstrated the switch from a means-ends to a forward-chaining strategy. This was associated with the conventional concomitants of expertise such as a decrease in the number of moves required for solution. In addition, the speed at which expertise developed varied for different categories of problems. Subjects appeared to categorize problems according to the order in which equations would be required, with these categories being discovered at nonuniform rates. This was assumed to be due to the differential rate of acquisition of schemas associated with different categories of problems. Experiments 2 and 3, again using kinematics problems, tested the hypothesis that the means-ends strategies used by novices retarded the acquisition of appropriate'schemas. It was suggested that under a means-ends strategy, moves are controlled by the problem goal, which reduces the information obtained by problem solvers concerning problem structure. The use of nonspecific rather than specific goals was found to enhance the acquisition of expertise as measured by the use of a forward-oriented strategy, the number of moves required for solution, and the number of equations written without substitutions. Experiments 4 and 5, using geometry problems, duplicated the enhanced rate of strategy alteration found with reduced goal specificity. The results of Experiments 6 and 7, again using geometry problems, indicated that reduced goal specificity also enhanced the rate at which problem solvers induced appropriate problem categories. It was concluded that in circumstances in which the primary reason for presenting problems is to assist problem solvers in acquiring knowledge concerning problem structure, the use of conventional problems solved by means-ends analysis may not be maximally efficient.},
author = {Sweller, John and Mawer, Robert F and Ward, Mark R},
doi = {10.1037/0096-3445.112.4.639},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sweller, Mawer, Ward - 1983 - Development of expertise in mathematical problem solving.pdf:pdf},
issn = {0096-3445},
journal = {Journal of Experimental Psychology: General},
number = {4},
pages = {639--661},
pmid = {1120},
title = {{Development of expertise in mathematical problem solving.}},
volume = {112},
year = {1983}
}
@article{Sweller1983,
abstract = {Previous studies have found that the strategies used by expert and novice problem solvers differ. Novices tend to use means-ends analysis, which involves working backward from the goal, whereas experts prefer to work forward from the givens of a problem. Experiment 1 was designed to study the course of development of expertise using a subset of kinematics problems. After solving many problems, subjects demonstrated the switch from a means-ends to a forward-chaining strategy. This was associated with the conventional concomitants of expertise such as a decrease in the number of moves required for solution. In addition, the speed at which expertise developed varied for different categories of problems. Subjects appeared to categorize problems according to the order in which equations would be required, with these categories being discovered at nonuniform rates. This was assumed to be due to the differential rate of acquisition of schemas associated with different categories of problems. Experiments 2 and 3, again using kinematics problems, tested the hypothesis that the means-ends strategies used by novices retarded the acquisition of appropriate'schemas. It was suggested that under a means-ends strategy, moves are controlled by the problem goal, which reduces the information obtained by problem solvers concerning problem structure. The use of nonspecific rather than specific goals was found to enhance the acquisition of expertise as measured by the use of a forward-oriented strategy, the number of moves required for solution, and the number of equations written without substitutions. Experiments 4 and 5, using geometry problems, duplicated the enhanced rate of strategy alteration found with reduced goal specificity. The results of Experiments 6 and 7, again using geometry problems, indicated that reduced goal specificity also enhanced the rate at which problem solvers induced appropriate problem categories. It was concluded that in circumstances in which the primary reason for presenting problems is to assist problem solvers in acquiring knowledge concerning problem structure, the use of conventional problems solved by means-ends analysis may not be maximally efficient.},
author = {Sweller, John and Mawer, Robert F. and Ward, Mark R.},
doi = {10.1037/0096-3445.112.4.639},
issn = {0096-3445},
journal = {Journal of Experimental Psychology: General},
number = {4},
pages = {639--661},
pmid = {1120},
title = {{Development of expertise in mathematical problem solving.}},
volume = {112},
year = {1983}
}
@misc{Sweller1983a,
abstract = {Previous studies have found that the strategies used by expert and novice problem solvers differ. Novices tend to use means-ends analysis, which involves working backward from the goal, whereas experts prefer to work forward from the givens of a problem. Experiment 1 was designed to study the course of development of expertise using a subset of kinematics problems. After solving many problems, subjects demonstrated the switch from a means-ends to a forward-chaining strategy. This was associated with the conventional concomitants of expertise such as a decrease in the number of moves required for solution. In addition, the speed at which expertise developed varied for different categories of problems. Subjects appeared to categorize problems according to the order in which equations would be required, with these categories being discovered at nonuniform rates. This was assumed to be due to the differential rate of acquisition of schemas associated with different categories of problems. Experiments 2 and 3, again using kinematics problems, tested the hypothesis that the means-ends strategies used by novices retarded the acquisition of appropriate'schemas. It was suggested that under a means-ends strategy, moves are controlled by the problem goal, which reduces the information obtained by problem solvers concerning problem structure. The use of nonspecific rather than specific goals was found to enhance the acquisition of expertise as measured by the use of a forward-oriented strategy, the number of moves required for solution, and the number of equations written without substitutions. Experiments 4 and 5, using geometry problems, duplicated the enhanced rate of strategy alteration found with reduced goal specificity. The results of Experiments 6 and 7, again using geometry problems, indicated that reduced goal specificity also enhanced the rate at which problem solvers induced appropriate problem categories. It was concluded that in circumstances in which the primary reason for presenting problems is to assist problem solvers in acquiring knowledge concerning problem structure, the use of conventional problems solved by means-ends analysis may not be maximally efficient.},
author = {Sweller, John and Mawer, Robert F. and Ward, Mark R.},
booktitle = {Journal of Experimental Psychology: General},
doi = {10.1037/0096-3445.112.4.639},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sweller, Mawer, Ward - 1983 - Development of expertise in mathematical problem solving.pdf:pdf},
issn = {0096-3445},
number = {4},
pages = {639--661},
pmid = {1120},
title = {{Development of expertise in mathematical problem solving.}},
volume = {112},
year = {1983}
}
@article{Sweller1982,
abstract = {Means–ends analysis is a mechanism that is assumed to operate when people solve transformation problems. Its use is affected by the extent to which the goal is clearly specified to the problem solver as a problem state and by the extent to which learning occurs during a problem-solving episode. Five maze-tracing experiments were conducted with 116 undergraduates in which the finish point of the maze could be presented either as a specific location or in more general terms. The latter prevented the use of conventional means–ends analysis. Results indicate that on the particular maze configuration used, the nonspecific goal resulted in fewer errors and more rapid learning of the structure of the problem. Under conditions that facilitated the use of means–ends analyses, knowledge of the goal location rendered the problem insoluble. General results were replicated with the use of numerical problems. Implications for the generality of means–ends analysis as a problem-solving mechanism are discussed. (11 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Sweller, John and Levine, Marvin},
doi = {10.1037/0278-7393.8.5.463},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sweller, Levine - 1982 - Effects of goal specificity on means-ends analysis and learning.pdf:pdf},
isbn = {1939-1285(Electronic);0278-7393(Print)},
issn = {0278-7393},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
number = {5},
pages = {463--474},
title = {{Effects of goal specificity on means-ends analysis and learning.}},
volume = {8},
year = {1982}
}
@article{Robertson1981,
abstract = {This article will review the methods currently available to the clinician and research worker for measuring the concentration of ionized calcium in various body fluids including whole blood, serum, plasma, urine, cerebrospinal fluid, milk, and synovial fluid. The methods to be reviewed are based on procedures involving bioluminescence, colorimetry and ion-selective electrodes. Emphasis will be given to the precision and, wherever possible, accuracy of each technique. Possible sources of error and interfering agents will be identified. Attention will be given to the recommended conditions for measuring ionized calcium in each body fluid. An assessment will be made of the theoretical and practical importance of measuring ionized calcium rather than total calcium and of its value in clinical medicine.},
author = {Robertson, W G and Marshall, R W},
doi = {10.3109/10408368109105869},
issn = {1040-8363},
journal = {Critical reviews in clinical laboratory sciences},
keywords = {Auto-immune,PAD4,Physiology},
mendeley-tags = {Auto-immune,PAD4,Physiology},
pages = {85--125},
pmid = {7026165},
title = {{Ionized calcium in body fluids.}},
volume = {15},
year = {1981}
}
@misc{Campbell1980,
author = {Campbell, G. and Geller, S.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell, Geller - 1980 - Balanced Latin Squares.pdf:pdf},
title = {{Balanced Latin Squares}},
year = {1980}
}
@article{Gick1980,
author = {Gick, Mary L and Holyoak, Keith J.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gick, Holyoak - 1980 - Analogical Problem Solving.pdf:pdf},
journal = {Cognitive P},
pages = {306--355},
title = {{Analogical Problem Solving}},
volume = {12},
year = {1980}
}
@article{Ericsson1980,
abstract = {Behavior and experience are organized around the enjoyment and pursuit of incentives. During the time that an incentive is behaviorally salient, an organism is especially responsive to incentive-related cues. This sustained sensitivity requires postulating a continuing state (denoted by a construct, current concern) with a definite onset (commitment) and offset (consummation or disengagement). Disengagement follows frustration, accompanies the behavioral process of extinction, and involves an incentive-disengagement cycle of invigoration, aggression, depression, and recovery. Depression is thus a normal part of disengagement that may be either adaptive or maladaptive for the individual but is probably adaptive for the species. The theory offers implications for motivation; etiology, symptomatology, and treatment of depression; drug use; and other social problem areas.},
author = {Ericsson, K. Anders and Simon, Herbert A.},
doi = {10.1037/0033-295X.87.3.215},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ericsson, Simon - 1980 - Verbal Reports as Data.pdf:pdf},
isbn = {0033-295X},
issn = {0033-295X},
journal = {Psychological Review},
number = {3},
pages = {215--251},
pmid = {12759483},
title = {{Verbal Reports as Data}},
volume = {87},
year = {1980}
}
@article{Fischer1980,
abstract = {Skill theory provides tools for predicting developmental sequences and synchronies in any domain at any point in development by integrating behavioral and cognitive-developmental concepts. Cognitive development is explained by skill structures called "levels," together with transformation rules relating these levels to each other. The transformation rules specify the developmental steps by which a skill moves gradually from one level of complexity to the next. At every step in these developmental sequences, the individual controls a particular skill. Skills are gradually transformed from sensory-motor actions to representations and then to abstractions. The transformations produce continuous behavioral changes; but across the entire profile of a person's skills and within highly practiced task domains, a stagelike shift in skills occurs as the person develops to an optimal level. The theory suggests a common framework for integrating developmental analyses of cognitive, social, language, and perceptual-motor skills and certain behavioral changes in learning and problem solving. (6 p ref) (PsycINFO Database Record (c) 2010 APA )},
author = {Fischer, K W},
doi = {10.1037/0033-295x.87.6.477},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer - 1980 - A theory of cognitive development The control and construction of hierarchies of skills.pdf:pdf},
isbn = {0033-295X$\backslash$r1939-1471},
issn = {0033-295X},
journal = {Psychological Review},
keywords = {2140 History {\&} Systems,2820 Cognitive {\&} Perceptual,Ability,Cognitive Development,Development,Literature Review,Theories,skill theory as predictor of cognitive development},
number = {6},
pages = {477--531},
title = {{A theory of cognitive development: The control and construction of hierarchies of skills}},
volume = {87},
year = {1980}
}
@article{Schwarz1978,
author = {Schwarz, Gideon E.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwarz - 1978 - Estimating the Dimension of a Model.pdf:pdf},
journal = {Annals of Statistics},
number = {2},
title = {{Estimating the Dimension of a Model}},
volume = {6},
year = {1978}
}
@article{Langendoen1973,
author = {Langendoen, D. T.},
file = {:home/andrew/Documents/grad/Papers/1421865.pdf:pdf},
journal = {The American Journal of Psychology1},
number = {1},
pages = {207--212},
title = {{Review of "Phrase and Paraphrase : Some Innovative Uses of Language" by Lila R . Gleitman and Henry Gleitman}},
volume = {86},
year = {1973}
}
@article{Anderson1972,
author = {Anderson, P W},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson - 1972 - More Is Different.pdf:pdf},
journal = {Science},
number = {4047},
pages = {393--396},
title = {{More Is Different}},
volume = {177},
year = {1972}
}
@article{Michell1969,
abstract = {A method has been developed for measuring the rate of phagocytosis rather than the quantity of particles ingested per cell when the process is virtually complete. The method, which is simpler and more rapid than those described previously, utilizes cellular monolayers, radioactive particles, and short incubation times. Under the conditions described, the rate of uptake of particles by either guinea-pig peritoneal or human blood leukocytes was proportional to both cell concentration and the time of incubation, and was independent of changes in the concentration of particles during the measurement. The particles were retained by the cells for at least 90 min. The most suitable particles so far used have been (32)P-labeled Salmonella typhimurium, and acetyl-(14)C- or methyl-(14)C-labeled starch particles. The oxidation of (14)C-labeled glucose has been studied under the same conditions that were used for the assays of phagocytosis: the greatest increase in formation of (14)CO(2) from glucose-1-(14)C occurred a few minutes after the most rapid period of phagocytosis.},
author = {Michell, R H and Pancake, S J and Noseworthy, J and Karnovsky, M L},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Michell et al. - 1969 - Measurement of rates of phagocytosis the use of cellular monolayers.pdf:pdf},
issn = {0021-9525},
journal = {The Journal of cell biology},
keywords = {Animals,Antimetabolites,Antimetabolites: pharmacology,Auto-immune,Carbon Isotopes,Guinea Pigs,Humans,Leukocytes,Leukocytes: physiology,Methods,PAD4,Phagocytosis,Phagocytosis: drug effects,Phosphorus Isotopes,Physiology,Salmonella typhimurium,Starch},
mendeley-tags = {Auto-immune,PAD4,Physiology},
month = {jan},
number = {1},
pages = {216--24},
pmid = {4881437},
title = {{Measurement of rates of phagocytosis: the use of cellular monolayers.}},
volume = {40},
year = {1969}
}
@article{Reber1967,
author = {Reber, Arthur s.},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reber - 1967 - Implicit Learning of Artificial Grammars.pdf:pdf},
journal = {Journal of Verbal Learning and Verbal Behavior},
number = {6},
pages = {855--863},
title = {{Implicit Learning of Artificial Grammars}},
volume = {6},
year = {1967}
}
@misc{Shepard1961,
abstract = {The present study explores some of the factors that determine how difficult a classification will be to learn or remember. The experiment to be reported was designed primarily to answer two questions: How does the difficulty of learning vary from one type of classification to another? Is something specific learned abut the structure of a classification that will transfer positively to the subsequent learning of a new classification of that same type? The experimental procedure conformed to the usual paired-associate paradigm except that only two responses were used. That is, the eight stimuli were presented, one at a time, in a continuing random sequence and an association between each stimulus and one of two alternative classificatory responses was built up by the method of anticipation. Six female freshmen at Fairleigh Dickinson University served for 15 hours each in this first experiment. These subjects ( S s) were selected to be as uniform as possible with respect to their college entrance examination scores. A combined empirical and theoretical investigation of the difficulties of different kinds of classifications was undertaken using both learning and memory tasks. Sets of stimuli of a variety of kinds were used but, in each set, there were eight stimuli each of which took on one of two possible values on each of three different dimensions. Results showed that of the 70 possible classifications of the eight stimuli into two equal groups, there are only six basic types. The different classifications belonging to any one of these types have the same structure; they differ only with respect to which of the three dimensions is assigned to which of the three roles in the classification, and with respect to which of the two classificatory responses is assigned to which group of four stimuli.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Shepard, Roger N and Hovland, Carl I and Jenkins, Herbert M},
booktitle = {Psychological Monographs: General and Applied},
doi = {10.1037/h0093825},
eprint = {arXiv:1011.1669v3},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shepard, Hovland, Jenkins - 1961 - Learning and memorization of classifications.pdf:pdf},
isbn = {9788578110796},
issn = {0096-9753},
number = {13},
pages = {1--42},
pmid = {418},
title = {{Learning and memorization of classifications.}},
volume = {75},
year = {1961}
}
@article{Newell1961,
abstract = {A theory of problem solving expressed as a computer program permits simulation of thinking processes. From Psyc Abstracts 36:04:4CM11N. (PsycINFO Database Record (c) 2003 APA},
author = {Newell, a and Simon, H a},
doi = {10.2307/1708146},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Newell, Simon - 1961 - Computer simulation of human thinking.pdf:pdf},
isbn = {2819611222},
issn = {00368075},
journal = {Science (New York, N.Y.)},
keywords = {AUTOMATIC DATA PROCESSING,MENTAL PROCESSES},
number = {3495},
pages = {2011--2017},
pmid = {14479322},
title = {{Computer simulation of human thinking.}},
volume = {134},
year = {1961}
}
@article{Mirsky1960,
author = {Mirsky, L.},
doi = {10.1093/qmath/11.1.50},
file = {:home/andrew/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mirsky - 1960 - Symmetric gauge functions and unitarily invariant norms.pdf:pdf},
issn = {0033-5606},
journal = {The Quarterly Journal of Mathematics},
number = {1},
pages = {50--59},
title = {{Symmetric gauge functions and unitarily invariant norms}},
volume = {11},
year = {1960}
}
