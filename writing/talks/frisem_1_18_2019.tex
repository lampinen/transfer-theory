\documentclass{beamer}
\setbeamerfont{subsection in toc}{size=\footnotesize}
\usepackage{pgfpages}
%\setbeameroption{show notes on second screen=left} %enable for notes
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
%\usepackage{transparent}
\usepackage{hyperref}
\lstset{language=python,frame=single}
\usepackage{verbatim}
%\usepackage{apacite}
\usepackage[longnamesfirst]{natbib}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{relsize}
\usepackage{appendixnumberbeamer}
\usepackage{xparse}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{tikz}
\usetikzlibrary{matrix,backgrounds}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}

\tikzset{onslide/.code args={<#1>#2}{%
  \only<#1>{\pgfkeysalso{#2}} 
}}

\tikzstyle{block} = [rectangle, draw, fill=red!20!blue!10, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{netnode} = [circle, draw, very thick, inner sep=0pt, minimum size=0.5cm] 
\tikzstyle{relunode} = [rectangle, draw, very thick, inner sep=0pt, minimum size=0.5cm] 
    
\tikzstyle{line} = [draw, line width=1.5pt, -latex']

\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

\pgfdeclarelayer{myback}
\pgfsetlayers{myback,background,main}

\usetheme[numbering=fraction]{metropolis}
\newcommand{\semitransp}[2][35]{\color{fg!#1}#2}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\renewcommand*\footnoterule{}
%%\AtBeginSection[]
%%{
%%  \begin{frame}
%%    \frametitle{Table of Contents}
%%    \tableofcontents[currentsection]
%%  \end{frame}
%%}

\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{{\bf\overline{#1}}}
\newcommand{\bh}[1]{{\bf\hat{#1}}}

\newcommand{\trainerr}{\mathcal{\varepsilon}_\text{train}}
\newcommand{\generr}{\mathcal{\varepsilon}_\text{test}}

\newcommand{\toptn}{{t}^\text{opt}_\text{gradient}}
\newcommand{\eoptn}{\mathcal{\varepsilon}^\text{opt}_\text{gradient}}
\newcommand{\eoptnn}{\mathcal{\varepsilon}^\text{opt}_\text{non-gradient}}

\newcommand{\wa}{{\bf{W}^{21}}}
\newcommand{\wb}{{\bf{W}^{32}}}
\newcommand{\wk}{{\bf{W}^{k(k-1)}}}
\newcommand{\ddt}{\frac{d}{dt}}
\newcommand{\ovn}{\overline{N}}
\newcommand{\rank}{\text{rank}}

\begin{document}

\title{Understanding generalization and transfer in deep linear neural networks}
\author{Andrew Lampinen}
\date{FriSem, 1/18/2019}
\frame{\titlepage}

\begin{frame}{Generalization in deep networks: ``Creativity'' from AlphaGo}
\begin{figure}
\includegraphics[width=0.5\textwidth]{figures/alphago_move_37.png}
\end{figure}
\note{This is a move that nobody has seen before, during the game commentators called it ``probably a mistake,'' but later decided it was ``beautiful.''}
\end{frame}

\begin{frame}{Generalization in humans: Past tense over-regularization}
\begin{figure}
\includegraphics[width=0.8\textwidth]{figures/past_tense.png}
\end{figure}
\note{There are many interesting phenomena like this, but there's a general pattern of learning broad features followed by progressive differentiation of lower level structure.}
\end{frame}

\begin{frame}[standout]
How, why, and when do neural networks (and humans) generalize what they have learned?
\end{frame}

\begin{frame}{Outline}
\vspace{1em}
\tableofcontents
\end{frame}

\section{The theory}

\subsection{Deep linear networks}
\begin{frame}{Setting: Deep linear networks}
\begin{figure}
\centering
\begin{tikzpicture}[auto, overlay, remember picture]
\begin{scope}[shift={(0, -0.5)}]
\node at (0, -2.75) (input) {\only<3-5>{\LARGE \textbf{Go}}};
\node at (0, 2.75) (output) {\only<5>{\LARGE \textbf{Goed}}};

\node [netnode, onslide={<3-5>{fill=red!20}}] at (-1.5,-2) (i1) {};
\node [netnode, onslide={<3-5>{fill=blue!80}}] at (-0.5,-2) (i2) {};
\node [netnode, onslide={<3-5>{fill=red!75}}] at (0.5,-2) (i3) {};
\node [netnode, onslide={<3-5>{fill=blue!30}}] at (1.5,-2) (i4) {};

\node [netnode, onslide={<4-5>{fill=blue!20}}] at (-1,0) (h111) {};
\node [netnode, onslide={<4-5>{fill=red}}] at (0,0) (h112) {};
\node [netnode, onslide={<4-5>{fill=red!30}}] at (1,0) (h113) {};

\path [draw, very thick] (i1) to (h111);
\path [draw, very thick] (i1) to (h112);
\path [draw, very thick] (i1) to (h113);
\path [draw, very thick] (i2) to (h111);
\path [draw, very thick] (i2) to (h112);
\path [draw, very thick] (i2) to (h113);
\path [draw, very thick] (i3) to (h111);
\path [draw, very thick] (i3) to (h112);
\path [draw, very thick] (i3) to (h113);
\path [draw, very thick] (i4) to (h111);
\path [draw, very thick] (i4) to (h112);
\path [draw, very thick] (i4) to (h113);

\only<-5>{ 
\node [netnode, onslide={<5>{fill=red!50}}] at (-2,2) (h120) {};
\node [netnode, onslide={<5>{fill=red!10}}] at (-1,2) (h121) {};
\node [netnode, onslide={<5>{fill=blue!75}}] at (0,2) (h122) {};
\node [netnode, onslide={<5>{fill=blue!30}}] at (1,2) (h123) {};
\node [netnode, onslide={<5>{fill=blue!10}}] at (2,2) (h124) {};

\path [draw, very thick] (h111) to (h120);
\path [draw, very thick] (h111) to (h121);
\path [draw, very thick] (h111) to (h122);
\path [draw, very thick] (h111) to (h123);
\path [draw, very thick] (h111) to (h124);
\path [draw, very thick] (h112) to (h120);
\path [draw, very thick] (h112) to (h121);
\path [draw, very thick] (h112) to (h122);
\path [draw, very thick] (h112) to (h123);
\path [draw, very thick] (h112) to (h124);
\path [draw, very thick] (h113) to (h120);
\path [draw, very thick] (h113) to (h121);
\path [draw, very thick] (h113) to (h122);
\path [draw, very thick] (h113) to (h123);
\path [draw, very thick] (h113) to (h124);
}
\only<6->{

\node at (0, 0.85) (dots) {\LARGE$\bm{\vdots}$};

\node [netnode] at (-1,1.5) (h211) {};
\node [netnode] at (0,1.5) (h212) {};
\node [netnode] at (1,1.5) (h213) {};

\node [netnode] at (-2,3.5) (h120) {};
\node [netnode] at (-1,3.5) (h121) {};
\node [netnode] at (0,3.5) (h122) {};
\node [netnode] at (1,3.5) (h123) {};
\node [netnode] at (2,3.5) (h124) {};

\path [draw, very thick] (h211) to (h120);
\path [draw, very thick] (h211) to (h121);
\path [draw, very thick] (h211) to (h122);
\path [draw, very thick] (h211) to (h123);
\path [draw, very thick] (h211) to (h124);
\path [draw, very thick] (h212) to (h120);
\path [draw, very thick] (h212) to (h121);
\path [draw, very thick] (h212) to (h122);
\path [draw, very thick] (h212) to (h123);
\path [draw, very thick] (h212) to (h124);
\path [draw, very thick] (h213) to (h120);
\path [draw, very thick] (h213) to (h121);
\path [draw, very thick] (h213) to (h122);
\path [draw, very thick] (h213) to (h123);
\path [draw, very thick] (h213) to (h124);


}


%% annotations
\only<-5>{
\node at (-2.75, -1) {\LARGE$\wa$};
\node at (-2.75, 1) {\LARGE$\wb$};
}
\only<6->{
\node at (-2.75, -1) {\LARGE$\wa$};
\node at (-2.75, 2.25) {\LARGE$\wk$};
}
\only<2-5>{
\node at (2.4, -2) {\LARGE$N_1$};
\node at (2, 0) {\LARGE$N_2$};
\node at (2.9, 2) {\LARGE$N_3$};
}
\end{scope}
\end{tikzpicture}
\end{figure}
\end{frame}

\begin{frame}{Why linear?}
Linear networks are much easier to understand mathematically. They lack the full representational capacity of nonlinear networks, but exhibit similar (and nonlinear) learning dynamics.
\begin{figure}
\uncover<2->{
\only<-2>{
\includegraphics[height=0.6\textheight]{figures/progressive_differentiation.png}
}
\only<3>{
\includegraphics[height=0.6\textheight]{../../plots/paper/randomized_fig_B.png}
}
}
\end{figure}
\vspace{-1em}
\uncover<2->{\footnotesize \citep{Saxe2013, Saxe2018, Lampinen2019}}
\end{frame}

\subsection{Learning dynamics}
\begin{frame}{Learning dynamics}
\begin{columns}
\begin{column}{0.55\textwidth}
\only<-3>{\vspace{0.9em}}
\begin{itemize}[<+->] \itemsep 1em
\item Training data $\{(\bh{x}^\mu, \bh{y}^\mu)\}$, e.g.
    $\{(\text{talk}, \text{talked}), (\text{go}, \text{went}), \cdots\}$
\item Loss $\sum_{\mu}||\bh{y}^\mu - \wb\wa \bh{x}^\mu||^2$
\item Batch gradient descent
    \only<-3>{$\Delta \wa  =  \lambda \sum_{\mu} \wb^T \left( \bh{y}^\mu   - \wb \wa {\bh x}^\mu   \right){\bh x}^\mu{}^T$\\[0.5em]
	      $\Delta \wb  =  \lambda \sum_{\mu} \left( \bh{y}^\mu  - \wb \wa  {\bh x}^\mu \right) {\bh x}^\mu{}^T \wa^T$}
    \only<4->{$\tau \ddt \wa  =  \wb^T \left( {\bf \Sigma}^{31} - \wb \wa {\bf \Sigma}^{11} \right)$\\[0.5em]
	      $\tau \ddt \wb = \left( {\bf \Sigma}^{31} - \wb \wa {\bf \Sigma}^{11} \right) \wa^T$}

\end{itemize}
\end{column}
\begin{column}{0.45\textwidth}
\centering
$\bf{y}^\mu$
\includegraphics[width=\textwidth]{figures/reference_network.png}
$\bh{x}^\mu$
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Structure in the data}
\vspace{2em}
\begin{columns}
\begin{column}{0.5\textwidth}
Input-Input Correlations:
${\bf\Sigma}^{11} \equiv \sum_{\mu=1}^{\overline{N}_1} {\bh x}^\mu {\bh x}^\mu{}^T \uncover<2->{= {\bf I}}$
\end{column}
\begin{column}{0.5\textwidth}
\uncover<3->{
Input-Output Correlations:
${\bf \Sigma}^{31} \equiv \sum_{\mu=1}^{\overline{N_1}} \bh{y}^\mu {\bh x}^\mu{}^T$% = \bb{W} + {\bf Z}{\bh X}^T$
}
\end{column}
\end{columns}
\begin{figure}
\centering
\uncover<4->{
\includegraphics[width=\textwidth]{figures/SVD_from_saxe2018.png}
}
\end{figure}
\uncover<4->{\footnotesize \citep{Saxe2013}, figure from \citep{Saxe2018}}
\end{frame}

\begin{frame}{Learning dynamics are driven by ${\bf\Sigma}_{31}$}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/SVD_from_saxe2018.png}
\end{figure}
\begin{columns}
\begin{column}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/SVD_learn_from_saxe2018.png}
\end{column}
\begin{column}{0.5\textwidth}
\centering
\uncover<2->{
$s(t,\hat s)=\frac{\hat s e^{2\hat st/\tau}}{e^{2\hat st/\tau}-1+\hat s/\epsilon}$\\[0.5em]
}
\uncover<3->{
$t(s,\hat s) = \frac{\tau}{2\hat s} 
   \ln{\frac{{\hat s}/\epsilon -1}{{\hat s}/s -1}}$
}
\end{column}
\end{columns}
{\footnotesize \citep{Saxe2013}, figure from \citep{Saxe2018}}
\end{frame}

\begin{frame}[standout]
Principal components of the training data are learned independently; stronger components are learned earlier.
\end{frame}

\subsection{Distortion of data by noise}
\begin{frame}{How the signal emerges from the noise}

\end{frame}

\subsection{Generalization dynamics}
\begin{frame}{Putting it all together: generalization dynamics}

\end{frame}


\section{Applications of the theory}

\subsection{The trouble with structure-agnostic generalization bounds}
\begin{frame}{Why do standard generalization bounds suck?}
\end{frame}

\subsection{Randomized data}
\begin{frame}{Randomized data is learned slower}
\end{frame}

\subsection{Multiple tasks \& transfer}
\begin{frame}{Multi-task learning}
\end{frame}

\subsection{Structure sensitive regularization}
\begin{frame}{Is gradient descent optimal?}
\end{frame}

\begin{frame}[allowframebreaks]
\bibliographystyle{plainnat}
\blfootnote{\bibliography{generalization}}
\end{frame}


\end{document}


