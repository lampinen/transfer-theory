\documentclass[final]{beamer}
\usepackage[size=a0]{beamerposter}
\mode<presentation>{\usetheme{lampinen}}
\usepackage{float}
\usepackage{url}           
%\usepackage{booktabs}      
\usepackage{amsfonts}      
%\usepackage{blkarray}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{nicefrac}      
%\usepackage{microtype}   
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage[numbers]{natbib}
\usetikzlibrary{matrix,backgrounds}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations.pathreplacing,angles,quotes}


\tikzstyle{netnode} = [circle, draw, thick, inner sep=0pt, minimum size=0.5cm]
\tikzstyle{relunode} = [rectangle, draw, thick, inner sep=0pt, minimum size=0.5cm]

\tikzstyle{line} = [draw, line width=0.5pt, -latex']

\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{{\bf\overline{#1}}}
\newcommand{\bh}[1]{{\bf\hat{#1}}}
\newcommand{\sg}[1]{{\color{red} #1}}
\newcommand{\eopt}[1]{\varepsilon^\text{opt}_{#1}}


\newcommand{\trainerr}{\mathcal{\varepsilon}_\text{train}}
\newcommand{\generr}{\mathcal{\varepsilon}_\text{test}}

\newcommand{\toptn}{{t}^\text{opt}_\text{neural}}
\newcommand{\eoptn}{\mathcal{\varepsilon}^\text{opt}_\text{neural}}
\newcommand{\eoptnn}{\mathcal{\varepsilon}^\text{opt}_\text{non-neural}}

\newcommand{\wa}{{\bf{W}^{21}}}
\newcommand{\wb}{{\bf{W}^{32}}}
\newcommand{\ddt}{\frac{d}{dt}}
\newcommand{\ovn}{\overline{N}}

\setlength{\parskip}{0.5em}


% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\title{An analytic theory of generalization dynamics and transfer learning in deep linear networks}
\author{Andrew K. Lampinen$^1$ \& Surya Ganguli$^2$}
\institute{$^1$Department of Psychology, $^2$Department of Applied Physics, Stanford University}

\begin{document}
% \nipsfinalcopy is no longer used

\begin{frame}[t]{}
\vspace{-1.5em}
\begin{columns}

\begin{column}[t]{0.248\textwidth}
\begin{block}{\large Introduction \& background}
\vspace{-0.5em}
There are many puzzles about generalization in deep learning. How can large neural networks generalize well when they have the capacity to just memorize the labels \citep{Zhang2016}? Why is it that neural networks learn real structure faster than they memorize randomized labels \citep{Arpit2017}? Under what circumstances does gradient descent fail to find the parameters which yield optimal generalization performance? Multi-task learning can help generalization \citep[e.g.]{Dong2015, Luong2016}, but how and why does it do so? We address all these questions within the simplified framework of linear neural networks.\\[0.5em] 
We build off prior theory in linear neural networks \citep{Saxe2013}, which shows that linear networks have \emph{nonlinear} learning dynamics which capture many of the interesting features of learning in nonlinear neural networks. A recent related paper \citep{Advani2017} studied generalization in shallow and deep linear networks, but that work was limited to networks with a single output, thereby precluding the possibility of addressing multi-task learning or the many interesting modern tasks that require higher dimensional outputs, for example in language \cite{Dong2015}, image generation \cite{Goodfellow2014}, reinforcement learning \cite{Mnih2015, Silver2016}, etc. 
\end{block}

\begin{block}{\large Theoretical framework}
We consider a noisy teacher network that generates training data for a potentially more complex student network. \\[0.5em] 
\textbf{Teacher:} We consider a 3-layer linear teacher with \(\overline{N}_i\) units in layer \(i\), and weight matrices \({\bf\overline{W}}^{21} \in \R^{\overline{N_2} \times \overline{N}_1}\) and \({\bf\overline{W}}^{32} \in \R^{\overline{N_3} \times \overline{N_2}}\) between the input to hidden, and hidden to output layers, respectively. 
The teacher network thus computes the composite map \(\bb{y} = \bb{W} \bf{x}\), where \( \bb{W} \equiv \bb{W}^{32}\bb{W}^{21} \). Of critical importance is the singular value decomposition (SVD) of \(\bb{W}\):
\begin{equation}
\bb{W} = \bb{U}\,\bb{S}\,\bb{V}^T = \sum_{\alpha=1}^{\overline{N_2}} \overline{s}^\alpha \bb{u}^\alpha \bb{v}^\alpha{}^T,
\label{eq:teachersvd}
\end{equation}
We work in the limit $\overline{N_1}, \overline{N_3} \rightarrow \infty$ with an $O(1)$ aspect ratio $\mathcal{A}=\overline{N_3}/\overline{N_1} \in (0,1]$ so that the teacher has fewer outputs than inputs. We study generalization performance as a function of the \(\overline{N}_2\) teacher singular values. \\[10pt] 
We assume the teacher generates noisy outputs from a set of \(\overline{N}_1\) orthonormal inputs:
\begin{equation}
\bh{y}^{\mu} = \bb{W}\bh{x}^\mu + \bf{z} ^ \mu \qquad \text{for} \quad \mu = 1, \dots, \overline{N}_1.
\label{eq:traindata}
\end{equation}
This training set yields important second-order training statistics that will guide student learning:
\begin{equation}
{\bf\Sigma}^{11} \equiv \sum_{\mu=1}^{\overline{N}_1} {\bh x}^\mu {\bh x}^\mu{}^T = {\bf I},
\qquad
{\bf \Sigma}^{31} \equiv \sum_{\mu=1}^{\overline{N_1}} \bh{y}^\mu {\bh x}^\mu{}^T = \bb{W} + {\bf Z}{\bh X}^T.
\label{eq:secondorder}
\end{equation}
Here the input covariance \({\bf\Sigma}^{11}\) is assumed to be white, and \({\bf Z} \in \R^{\overline{N}_3 \times \overline{N}_1}\) is the noise matrix, whose $\mu$'th column is $\bf{z}^\mu$. Its matrix elements \(z^\mu_i\) are drawn iid. from a Gaussian with zero mean and variance \(\sigma_z^2 / {\overline{N}_1}\). As generalization performance will depend on the {\it ratio} of teacher singular values to the noise variance parameter $\sigma^2_z$, we simply set $\sigma_z=1$ in the following. Thus we can think of teacher singular values as signal to noise ratios (SNRs). \\[0.5em]
\textbf{Student:} Consider a student network with \(N_i\) units in each layer. We assume the first and last layers match the teacher (i.e. \(N_1 = \overline{N_1}\) and \(N_3 = \overline{N_3}\)) but \(N_2 \geq \overline{N_2}\), allowing the student to have more hidden units than the teacher. (The student may also be deeper.) Now consider any student whose input-output map is given by \({\bf y} = {\bf W}^{32} {\bf W}^{21} \equiv {\bf Wx}\). Its training error on the teacher dataset in \eqref{eq:traindata} and its test error on any set of {\it new} \(\overline{N}_1 \) white inputs with \({\bb{x}}^\mu\) obeying \(\sum_{\mu} {\bb{x}}^\mu {\bb{x}}^\mu{}^T = {\bf I}\) can be expressed in terms of the student, training data and teacher SVDs (see the paper for details), which we denote by: 
\({\bf W} = {\bf U}{\bf S}{\bf V}^T \),  
\({\bf \Sigma}^{31} = {\bh U}{\bh S}{\bh V}^T \), and 
\(\bb{W} = \bb{U}\, \bb{S}\,\bb{V}^T\) respectively. Specifically,
\begin{align}
\trainerr &= \left[\sum_{\beta=1}^{\overline{N}_3} \hat{s}_{\beta}^2\right]^{-1} 
\left[ \sum_{\alpha=1}^{N_2} s_{\alpha}^2 +  \sum_{\beta=1}^{\overline{N}_3} \hat{s}_{\beta}^2
- 2 \sum_{\alpha=1}^{N_2} \sum_{\beta=1}^{\overline{N}_3}  s_{\alpha} \hat{s}_{\beta} \left({\bf u}^{\alpha} \cdot \bf{\hat u}^{\beta} \right) \left({\bf v}^{\alpha} \cdot \bf{\hat v}^{\beta} \right)\right] \label{eq:trainerr}\\
\generr &= \left[\sum_{\beta=1}^{\overline{N}_2} \overline{s}_{\beta}^2\right]^{-1} 
\left[ \sum_{\alpha=1}^{N_2} s_{\alpha}^2 +  \sum_{\beta=1}^{\overline{N}_2} \overline{s}_{\beta}^2
- 2 \sum_{\alpha=1}^{N_2} \sum_{\beta=1}^{\overline{N}_2}  s_{\alpha} \overline{s}_{\beta} \left({\bf u}^{\alpha} \cdot \bb{u}^{\beta} \right) \left({\bf v}^{\alpha} \cdot \bb{v}^{\beta} \right)\right] \label{eq:generr}
\end{align}
\end{block}
\end{column}
\begin{column}[t]{0.248\textwidth}
\begin{block}{\large Student training dynamics}
\begin{figure}[H]
\vspace{-0.5em}
\centering
\begin{subfigure}[t]{0.35\textwidth}
\includegraphics[width=\textwidth]{../../plots/paper/fig_1a.png}
\label{s_of_t_a}
\end{subfigure}~
\begin{subfigure}[t]{0.35\textwidth}
\includegraphics[width=\textwidth]{../../plots/paper/fig_1b.png}
\label{s_of_t_b}
\end{subfigure}
\vspace{-1em}
\caption{Learning dynamics as a function of singular dimension strength. (a) shows how modes of different singular value are learned, (b) shows that there is a wave of learning that picks up singular dimensions with smaller and smaller singular values as \(t \rightarrow \infty\).}
\vspace{-0.5em}
\end{figure}
We assume batch gradient descent with learning rate $\lambda$ on the squared error $\sum_{\mu}||\bh{y}^\mu - \wb\wa \bh{x}^\mu||_2^2$. We consider two classes of student initializations.  The first is a {\it random student} initialized with random orthogonal matrices and all singular values equal to $\epsilon$. However, because of complex coupling between the student and teacher modes, the exact learning dynamics are difficult to obtain in this case. \\[0.5em]
Thus we also consider a {\it training aligned} (TA) initialization in which the initial weights are chosen so that the TA has the same singular vectors as the training data ${\bf \Sigma}^{31}$, but has all singular values equal to $\epsilon$.  As shown in \citep{Saxe2013}, as the TA learns its singular vectors remain unchanged, while the singular values evolve as $s^\alpha(t) = s(t,\hat s^\alpha)$, where the learning curve function $s(t,\hat s)$ as well as its functional inverse $t(s,\hat s)$ is given by 
\begin{equation}
s(t,\hat s)=\frac{\hat s e^{2\hat st/\tau}}{e^{2\hat st/\tau}-1+\hat s/\epsilon}, \qquad
t(s,\hat s) = \frac{\tau}{2\hat s} 
   \ln{\frac{{\hat s}/\epsilon -1}{{\hat s}/s -1}}.
\label{s_soln}
\end{equation}
Which describe how each TA mode learns the corresponding data mode. Each mode undergoes a sigmoidal learning curve with a sharp transition around time $t/\tau = \frac{1}{2\hat s}  \ln{({\hat s}/\epsilon -1)}$. \\[0.5em] 
\citet{Saxe2013} showed that that TA networks provide a good approximation to the train-error dynamics of randomly initialized networks, below we show that they also provide a good description of the \emph{generalization} dynamics. The results in this section assume a single hidden layer, but there are also solutions for deeper networks, see the paper.
\end{block}
%% Col 2 block 2
\begin{block}{\large How the signal is buried in the training data}
\begin{figure}
\vspace{-0.5em}
\centering
\begin{subfigure}[t]{0.44\textwidth}
\includegraphics[height=2.6in]{../../plots/paper/fig_2a.png}
\label{fig2b}
\end{subfigure}%
\begin{subfigure}[t]{0.30\textwidth}
\includegraphics[height=2.6in]{../../plots/paper/fig_2c.png}
\label{fig2c}
\end{subfigure}%
\begin{subfigure}[t]{0.21\textwidth}
\includegraphics[height=2.6in]{../../plots/paper/fig_2d.png}
\label{fig2d}
\end{subfigure}
\vspace{-1em}
\caption{The signal through the noise. Theoretical vs. empirical (a) histogram of singular values of noisy teacher \(\hat{s}\). (b) \(\hat{s}\) as a function of \(\overline{s}\). (c) alignment of noisy teacher and noiseless teacher singular vectors as a function of \(\overline{s}\). ($\overline{N_1}= \overline{N_3} = 100$.)}
\label{fig2}
\vspace{-0.5em}
\end{figure}
How do the true singular dimensions get distorted by the noisy teacher? The top $\overline{N}_2$ singular values and vectors of $\bf{\Sigma}^{31}$ converge to ${\hat s}(\overline{s}_\alpha)$, where:
\begin{equation}
\hat{s}(\overline{s}) = \begin{cases}
{(\overline{s})^{-1}}{\sqrt{(1+\overline{s}^2)(\mathcal{A}+\overline{s}^2)}}\ & \text{if } \overline{s} > \mathcal{A}^{1/4} \\
1+\sqrt{\mathcal{A}} & \text{otherwise}.
\end{cases}
\label{eq:shatsbar}
\end{equation}
The associated singular vectors of the teacher can also acquire a nontrivial overlap with the $\overline{N}_2$ modes of the teacher through the relation 
$\left\lvert {\bh u}^{\alpha} \cdot \bb{u}^{\beta} \right\rvert   \left\lvert{\bh v}^{\alpha} \cdot \bb{v}^{\beta} \right\rvert = \mathcal{O}(\overline{s}_\alpha)$, where 
\begin{equation}
\mathcal{O}(\overline{s}) = 
\begin{cases}
\left[1-
        \frac{\mathcal{A}(1+\overline{s}^2)}          
            {\overline{s}^2(\mathcal{A}+\overline{s}^2)}
\right]^{1/2} 
\left[1-
        \frac{(\mathcal{A}+\overline{s}^2)}          
            {\overline{s}^2(1+\overline{s}^2)}
\right]^{1/2}

& \text{if } \overline{s} > \mathcal{A}^{1/4} \\
0 & \text{otherwise}
\end{cases}
\label{eq:ovlap}
\end{equation}
The remaining $N_3 - \overline{N_2}$ modes are orthogonal to the top ones, and their singular values obey the Marchenko-Pastur (MP) distribution: 
\begin{equation}
P(\hat{s}) = \begin{cases}
\frac{\sqrt{4\mathcal{A}-(\hat{s}^2 - (1+\mathcal{A}))^2}}{\pi \mathcal{A}\hat{s}} & \hat{s} \in [1-\sqrt{\mathcal{A}}, 1+\sqrt{\mathcal{A}}] \\
0 & \text{otherwise}.
\end{cases}
\label{eq:mp}
\end{equation}

Overall, these equations describe a singular vector {\it phase transition} in the training data, where weak modes are lost in the noise, but strong modes ($\overline{s} > \mathcal{A}^{1/4}$) are imprinted on the noisy signal, but with \emph{inflated} singular values, and an overlap with the true singular dimensions that approaches unity as $\overline{s}$ increases.
\end{block}
\end{column}


%%% Column 3
\begin{column}[t]{0.248\textwidth}
\begin{block}{\large Putting it together: a theory of generalization dynamics}
\vspace{-0.25em}
Using the results of the two prior sections, we obtain a complete theory of the evolution of train and generalization error over learning time:
{\small
\begin{equation}
\trainerr(t) = 
\left[\sum_{\alpha=1}^{\overline{N}_3} \hat{s}_{\alpha}^2\right]^{\!-1}\!\!
\left[
(N_3 - N_2) \langle \hat s^2 \rangle_{\mathcal{R}_{out}} \!+ \! (N_2 - \overline{N}_2) \langle (s(\hat{s},t) -\hat{s})^2 \rangle_{\mathcal{R}_{in}} +  
\sum_{\alpha=1}^{\overline{N}_2} 
    \left[
       s_\alpha(t) -  
       \hat{s}_{\alpha} \right]^2
\right]
\label{eq:trainerrth}
\end{equation}
}%
The first term is the normalization, the second is an integral over the MP distribution for the error due to noise modes the network doesn't have the capacity to fit, the third is another integral for the noise modes the network does fit, and the last is the learning of the (appropriately distorted) signal modes. Similarly, for the test/generalization error:
{\small
\begin{equation}
\generr(t) = 
\left[\sum_{\alpha=1}^{\overline{N}_2} \overline{s}_{\alpha}^2\right]^{-1}
\left[
(N_2 - \overline{N}_2) \langle s(\hat{s},t)^2 \rangle_{\mathcal{R}_{in}} + 
\sum_{\alpha=1}^{\overline{N}_2} 
    \left[
       (s_\alpha(t) - \overline{s}_{\alpha})^2  
       + 2 s_\alpha(t) \overline{s}_{\alpha}(1-\mathcal{O}(\overline{s}_\alpha))
       \right]      
\right]
\label{eq:generrth}
\end{equation}
}%
The first term is the normalization, the second is an integral over the MP distribution for the noise the network overfits, and the last is the learning of the distorted signal modes. Over training, the test error first improves as the signal modes are learned, then decreases as the noise is overfit. 
\end{block}

\begin{block}{\large Numerical tests of the theory}
\vspace{-0.5em}
For both shallow and deep students, and varying teacher ranks, our theory matches empirical results quite well. In the paper, we show that it also qualitatively matches nonlinear networks. 
\begin{figure}
\centering
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig3_redux_A.png}
\label{fig_3a}
\end{subfigure}%
\begin{subfigure}[t]{0.285\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig3_redux_B.png}
\label{fig_3b}
\end{subfigure}%
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig3_redux_C.png}
\label{fig_3c}
\end{subfigure}%
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig3_redux_D.png}
\label{fig_3d}
\end{subfigure}\\[-1em]
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig3_redux_E.png}
\label{fig_3e}
\end{subfigure}%
\begin{subfigure}[t]{0.285\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig3_redux_F.png}
\label{fig_3f}
\end{subfigure}%
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig3_redux_G.png}
\label{fig_3g}
\end{subfigure}%
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig3_redux_H.png}
\label{fig_3h}
\end{subfigure}
\vspace{-1em}
\caption{Theory and simulation for rank 1 (row 1, a-d) and rank 3 (row 2, e-h) teachers with single-hidden-layer students: (a-b, e-f) log train and test error, showing excellent match for TA, and close for random student. (c,g) comparing TA and random student's optimal stopping errors, showing almost perfect match. (d,h)  comparing optimal stopping times, showing alignment lag.}
\vspace{-0.5em}
\label{gen_results_fig}
\end{figure}

\begin{figure}
\vspace{-0.5em}
\centering
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/deep_fig_redux_A.png}
\label{deep_fig_a}
\end{subfigure}%
\begin{subfigure}[t]{0.285\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/deep_fig_redux_B.png}
\label{deep_fig_b}
\end{subfigure}%
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/deep_fig_redux_C.png}
\label{deep_fig_c}
\end{subfigure}%
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/deep_fig_redux_D.png}
\label{deep_fig_d}
\end{subfigure}\\[-1em]
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/deep_fig_redux_E.png}
\label{deep_fig_e}
\end{subfigure}%
\begin{subfigure}[t]{0.285\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/deep_fig_redux_F.png}
\label{deep_fig_f}
\end{subfigure}%
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/deep_fig_redux_G.png}
\label{deep_fig_g}
\end{subfigure}%
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/deep_fig_redux_H.png}
\label{deep_fig_h}
\end{subfigure}
\vspace{-1em}
\caption{Our theory applies to deeper networks: theory and simulation for rank 1 (row 1, a-d) and rank 3 (row 2, e-h) teachers with deep students: (a-b, e-f) log train and test error, respectively, showing excellent match for TA. (c,g) excellent match of TA and random optimal stopping errors. (d,h) TA and randomly initialized students optimal stopping times, showing large alignment lag.}
\label{deeper_results_fig}
\vspace{-1em}
\end{figure}
\end{block}
\begin{block}{\large Randomized vs. real data: a learning time puzzle}
\vspace{-0.5em}
Deep networks can memorize data with randomized targets \citep{Zhang2016}, yet they learn real data faster \citep{Arpit2017}. Our theory explains this -- randomizing the targets spreads the variance of the data modes over many noise modes, diluting the top singular values (although increasing the lower ones).
\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[height=2.5in]{../../plots/paper/randomized_fig_A.png}
\label{rand_fig_a}
\end{subfigure}~
\begin{subfigure}[t]{0.4\textwidth}
\includegraphics[height=2.5in]{../../plots/paper/randomized_fig_B.png}
\label{rand_fig_b}
\end{subfigure}
\caption{Learning randomized data: Comparing (a) singular value distributions and (b) learning curves for data with a signal vs. random data that preserves basic statistics (mean, variance).}
\label{randomizing_fig}
\end{figure}
\vspace{-1em}
\end{block}
\end{column}




%% Col 4
\begin{column}[t]{0.248\textwidth}
\begin{block}{\large Varying the number of training examples}
\vspace{-0.5em}
In the prior sections we assumed that the number of training examples $P$ equaled the number of inputs $N_1$ so the data were full rank, but what happens when $P \neq N_1$? If $P > N_1$, the noise averages out and the SNR improves, and learning is faster. If $P < N_1$, the SNR is lower, but the network also observes only a projection of the corrupted data modes onto a subspace of the inputs. This results in more complex behavior. Interestingly, although the optimal stopping error is monotonic in $P$, the asypmtotic test error is not, because when $P < N_1$ there is a \emph{frozen subspace} where overfitting doesn't occur \citep{Advani2017}. 
\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/changing_p_A.png}
\label{supp_P_fig_A}
\end{subfigure}%
\begin{subfigure}[t]{0.265\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/changing_p_B.png}
\label{supp_P_fig_B}
\end{subfigure}%
\begin{subfigure}[t]{0.265\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/changing_p_C.png}
\label{supp_P_fig_C}
\end{subfigure}%
\vspace{-1em}
\caption{Varying the number of training examples. (a,b) Train and test error with different numbers of inputs. (c) Minimum generalization error plotted against $\text{SNR}\sqrt{P/N_1}$.} \label{supp_P_fig}
\vspace{-1em}
\end{figure}
\end{block}

\begin{block}{\large Multi-task learning \& transfer}
\vspace{-0.5em}
How does learning multiple related tasks affect generalization? We show that the change in the optimal stopping error on task A from learning it alongside task B, which we denote by $\mathcal{T}^{A\leftarrow B}$ depends only on the SNRs of the tasks and the alignment of their input modes, not their output modes. We describe how multi-task learning highlights shared structure, while obscuring idiosyncratic structure, and identify the regimes where transfer is beneficial or detrimental. 
\begin{figure}
\centering
\begin{tikzpicture}[auto, background rectangle/.style={fill=white}, show background rectangle, scale=0.5, every node/.style={scale=0.5}]
%%%% teacher 1
\begin{scope}[shift={(-16, 0)}] 
% input layer

\node [netnode] at (-2,0) (i1) {};
\node [netnode] at (-1,0) (i2) {};
\node at (0, 0) (id) {\normalsize $\cdots$};
\node [netnode] at (1,0) (i3) {};
\node [netnode] at (2,0) (i4) {};
\draw[decoration={brace,mirror, raise=3pt},decorate, thick] (i1.south) -- node[below = 0.2] {\small $N_1$ inputs} (i4.south);


% hidden layer
\node [netnode] at (-1,2) (h1) {};

% output layer
\node [netnode] at (-3,4) (o0) {};
\node at (-2,4) (o1) {\normalsize $\cdots$};
\node [netnode] at (-1,4) (o2) {};
\draw[decoration={brace,raise=3pt},decorate, thick] (o0.north) -- node[above = 0.25, text width=2.5cm, align=center] (t1out) {\small Task 1 outputs} (o2.north);

\node at (0, -2) (teach) {\small \textbf{Task 1 teacher}};

% input -> hidden
\path [draw, thick] (i1) to (h1);
\path [draw, thick] (i2) to (h1);
\path [draw, thick] (i3) to (h1);
\path [draw, thick] (i4) to  node [xshift=2em, yshift=-0.1em] (W1) {} (h1);

% hidden -> output
\path [draw, thick] (h1) to (o0);
\path [draw, thick] (h1) to (o2);

\end{scope}
%%%% teacher 2
\begin{scope}[shift={(-9, 0)}] 
% input layer

\node [netnode] at (-2,0) (i1) {};
\node [netnode] at (-1,0) (i2) {};
\node at (0, 0) (id) {\normalsize $\cdots$};
\node [netnode] at (1,0) (i3) {};
\node [netnode] at (2,0) (i4) {};
\draw[decoration={brace,mirror, raise=3pt},decorate, thick] (i1.south) -- node[below = 0.2] {\small $N_1$ inputs} (i4.south);


% hidden layer
\node [netnode] at (1,2) (h1) {};

% output layer
\node [netnode] at (1,4) (o0) {};
\node at (2,4) (o1) {\normalsize $\cdots$};
\node [netnode] at (3,4) (o2) {};
\draw[decoration={brace,raise=3pt},decorate, thick] (o0.north) -- node[above = 0.25, text width=2.5cm, align=center] (t2out) {\small Task 2 outputs} (o2.north);

\node at (0, -2) (teach) {\small \textbf{Task 2 teacher}};

% input -> hidden
\path [draw, thick] (i1) to node (W2) {} (h1);
\path [draw, thick] (i2) to (h1);
\path [draw, thick] (i3) to (h1);
\path [draw, thick] (i4) to (h1);

% hidden -> output
\path [draw, thick] (h1) to (o0);
\path [draw, thick] (h1) to (o2);

\end{scope}

%%%% teacher equation
\node at (-12.5, 2) (plus) {\Huge $\displaystyle\boldsymbol{+}$};
\node at (-4.5, 2) (equals) {\Huge $\displaystyle\boldsymbol{=}$};

%%%%% alignment
\path [draw, dashed, thick, <->, black!50!white] (W1) to node [yshift=-2.2em] {\small Aligned?} ([xshift=-1.5em,yshift=-0.75em]W2);
\path [draw, dashed, thick, <->, black!50!white] (t1out.east) to node [text width=3cm] {\small Relative signal strength?} (t2out.west);

%%%% multi-teacher
% input layer

\node [netnode] at (-2,0) (i1) {};
\node [netnode] at (-1,0) (i2) {};
\node at (0, 0) (id) {\normalsize $\cdots$};
\node [netnode] at (1,0) (i3) {};
\node [netnode] at (2,0) (i4) {};
\draw[decoration={brace,mirror, raise=3pt},decorate, thick] (i1.south) -- node[below = 0.2] {\small $N_1$ inputs} (i4.south);


% hidden layer
\node [netnode] at (-1,2) (h1) {};
\node [netnode] at (1,2) (h2) {};

% output layer
\node [netnode] at (-3,4) (o0) {};
\node at (-2,4) (o1) {\normalsize $\cdots$};
\node [netnode] at (-1,4) (o2) {};
\node [netnode] at (1,4) (o3) {};
\node at (2,4) (o4) {\normalsize $\cdots$};
\node [netnode] at (3,4) (o5) {};
\draw[decoration={brace,raise=3pt},decorate, thick] (o0.north) -- node[above = 0.2] {\small Multi-task outputs} (o5.north);

\node at (0, -2) (teach) {\small \textbf{Multi-task teacher}};

% input -> hidden
\path [draw, thick] (i1) to (h1);
\path [draw, thick] (i1) to (h2);
\path [draw, thick] (i2) to (h1);
\path [draw, thick] (i2) to (h2);
\path [draw, thick] (i3) to (h1);
\path [draw, thick] (i3) to (h2);
\path [draw, thick] (i4) to (h1);
\path [draw, thick] (i4) to (h2);

% hidden -> output
\path [draw, thick] (h1) to (o0);
\path [draw, thick] (h1) to (o2);

\path [draw, thick] (h2) to (o3);
\path [draw, thick] (h2) to (o5);

%%%%% student
\node at (10, -2) (stud) {\small \textbf{Student}};
% input layer

\node [netnode] at (8,0) (si1) {};
\node [netnode] at (9,0) (si2) {};
\node at (10, 0) (sid) {\normalsize $\cdots$};
\node [netnode] at (11,0) (si3) {};
\node [netnode] at (12,0) (si4) {};

% hidden layer
\node [netnode] at (8,2) (sh1) {};
\node [netnode] at (9,2) (sh2) {};
\node at (10,2) (shd) {\normalsize $\cdots$};
\node [netnode] at (11,2) (sh3) {};
\node [netnode] at (12,2) (sh4) {};

% output layer
\node [netnode] at (7,4) (so0) {};
\node  at (8,4) (so1) {\normalsize $\cdots$};
\node [netnode] at (9,4) (so2) {};
\node [netnode] at (11,4) (so3) {};
\node  at (12,4) (so4) {\normalsize $\cdots$};
\node [netnode] at (13,4) (so5) {};

% input -> hidden
\path [draw, thick] (si1) to (sh1);
\path [draw, thick] (si1) to (sh2);
\path [draw, thick] (si1) to (sh3);
\path [draw, thick] (si1) to (sh4);
\path [draw, thick] (si2) to (sh1);
\path [draw, thick] (si2) to (sh2);
\path [draw, thick] (si2) to (sh3);
\path [draw, thick] (si2) to (sh4);
\path [draw, thick] (si3) to (sh1);
\path [draw, thick] (si3) to (sh2);
\path [draw, thick] (si3) to (sh3);
\path [draw, thick] (si3) to (sh4);
\path [draw, thick] (si4) to (sh1);
\path [draw, thick] (si4) to (sh2);
\path [draw, thick] (si4) to (sh3);
\path [draw, thick] (si4) to (sh4);

% hidden -> output
\path [draw, thick] (sh1) to (so2);
\path [draw, thick] (sh1) to (so3);
\path [draw, thick] (sh2) to (so2);
\path [draw, thick] (sh2) to (so3);
\path [draw, thick] (sh3) to (so2);
\path [draw, thick] (sh3) to (so3);
\path [draw, thick] (sh4) to (so2);
\path [draw, thick] (sh4) to (so3);

\path [draw, thick] (sh1) to (so0);
\path [draw, thick] (sh2) to (so0);
\path [draw, thick] (sh3) to (so0);
\path [draw, thick] (sh4) to (so0);
\path [draw, thick] (sh1) to (so5);
\path [draw, thick] (sh2) to (so5);
\path [draw, thick] (sh3) to (so5);
\path [draw, thick] (sh4) to (so5);


%%%%% connections
\node at (5, -2) (learn) {\small \textbf{Training}};

\path [draw, thick, ->, bend right] ([xshift=0.5em,yshift=-0.5em]i4.315) to node {\small inputs}([xshift=-0.5em,yshift=-0.5em]si1.225);
\node [scale=1.5] at (5, 5) (sum) {\(\bigoplus\)};
\path [draw, thick, ->, bend left] ([xshift=0.5em,yshift=0.5em]o5.45) to ([xshift=0.25em]sum.180);
\path [draw, thick, ->, bend left] ([xshift=-0.25em]sum.0) to ([xshift=-0.5em,yshift=0.5em]so0.135);
\node [text width=8 cm, align=center] at (5, 7.5) (noise) {\small IID Gaussian noise (fixed across training)};
\path [draw, thick, ->] (noise.270) to ([yshift=-0.25em]sum.90);
\end{tikzpicture}
\caption{Transfer setting-- If two different tasks are combined, how well students of the combined teacher perform on each task depends on the alignment and SNRs of the teachers.}
\label{transfer_conceptual_fig}
\vspace{-1em}
\end{figure}
\begin{figure}[H]
\centering
\colorbox{white}{
\includegraphics[width=0.3\textwidth]{transfer_pros_cons.png}
}
\caption{Conceptual cartoon of how transfer depends on task alignment.}
\label{transfer_cartoon_fig}
\end{figure}
\begin{figure}
\vspace{-1em}
\centering
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig_5a.png}
\label{fig2b}
\end{subfigure}%
\begin{subfigure}[t]{0.2\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig_5b.png}
\label{fig2c}
\end{subfigure}%
\begin{subfigure}[t]{0.3\textwidth}
\includegraphics[height=2.25in]{../../plots/paper/fig_5c.png}
\label{fig2d}
\end{subfigure}%
\vspace{-1em}
\caption{Transfer benefit $\mathcal{T}^{A\leftarrow B}(\overline{s}_A, \overline{s}_B, q)$ plotted at different values of $\overline{s}_A$. (a) $\overline{s}_A = \sqrt[4]{\mathcal{A}}$. This task is impossible to learn alone, but with support from another aligned task learning can occur. (b) $\overline{s}_A = 3$. Modest signals face interference from poorly aligned tasks, but benefits from well aligned tasks. (c) $\overline{s}_A = 100$. Tasks with very strong signals are hardly affected (note y-axis).}
\label{fig5}
\vspace{-1em}
\end{figure}
\end{block}
\begin{block}{\large Summary}
\vspace{-0.5em}
Our theory addresses diverse issues in generalization, including how task structure and dataset size influence generalization, and when multi-task learning is beneficial or detrimental. In the paper, we provide more theoretical details and other exciting applications, such as a non-gradient learning algorithm that provably outpeforms gradient descent. 
\end{block}

\begin{block}{\large References}
\vspace{-0.5em}
{
\tiny
\bibliographystyle{plainnat}
\bibliography{generalization}
}
\end{block}
\end{column}
\end{columns}
\end{frame}
\end{document}
